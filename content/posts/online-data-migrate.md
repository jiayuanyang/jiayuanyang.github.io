+++
title = "在线数据迁移"
date = "2024-06-11T08:00:00+08:00"
tags = ["数据迁移", "双写", "一致性", "时序"]
description = "在线数据迁移 数据一致性 双写"

+++

# 在线数据迁移

很多场景都有数据迁移的需求：

- 单表改分表
- 增加分表数，分32张表，扩到128张表
- 原来使用数据库A，后续迁移到数据库B
- 系统进行重构，新老数据库字段变化



如果允许停机迁移，数据的准确性一致性容易得到保证

但是大部分场景不允许停机迁移，系统中断几分钟，就会带来不少损失。

大部分应用选择不停机迁移，如何保证数据一致性是个问题



# 双写

1. 打开双写，读老库
2. 迁移存量数据
3. 一致性校验
4. 双写，灰度读新库
5. 双写，全量读新库
6. 下线双写、移除迁移代码



打开双写，目的是新修改数据新老数据库达到一致

迁移存量数据后，新老数据库完全一致

然后维持双写，灰度读新库持续观察，没有问题后读写新库

有问题可以随时切回读写老数据库



实际实施起来，比较复杂

1. 如何双写

- 先写新库还是先写老库？
- update时，如果新数据库没有数据，affect rows不一致，是否需要先从老数据库迁移
- 老数据库用事务更新多张表，新库是否需要使用事务
- 双写失败如何处理，接口返回成功还是失败
  - 双insert，失败可能是超时导致，实际已插入，即使超时时增加反查，反查本身仍然可能超时
  - 接口逻辑需多次update，前2个update更新成功，最后一个update新库失败，产生不一致

2. 时序问题

- delete与迁移的时序：存量迁移逻辑捞出数据A，准备迁移到新库，此时delete数据A，新老库双delete。数据A执行迁移，新库多数据
- 异步写新库的时序：如果异步写新库，多个更新的时序会错乱
- 云控开关时序：期望打开开关的一瞬间，所有实例都感知到变化。但是实际上还会有已经在处理中的请求。因此，处理存量数据时，一般会选择比打开双写早一点的时间，多处理一些数据

3. 怎么保证数据一致

- 双写无法保证同时成功，有时间差
- 数据校验
  - 记录update_time，做增量校验
  - update时，同时更新行的hash值，校验时可以select sum(hash) where xx（适用于结构不变的迁移）
  - 切换期间也可能会引起不一致
    - 一种可行的方式是切换前停止几秒的写请求



# 存储自身的数据迁移

双写方案在一致性上并不是很完美，可以先看看存储自身是如何处理的，能够提供参考

## MySQL

- 全量同步
  - 可以使用mysqldump导出，导出时记录binlog位置
- 增量同步
  - 应用binlog



### 在线结构变更

MySQL早期版本DDL锁表，对应用影响较大，有pt-osc，gh-ost方案可以选择

- pt-osc
  - 使用触发器处理新增的修改
    - 原表的insert，在新表replace into
    - 原表的delete，在新表delete ignore
    - 原本的update，在新表replace into
  - 批量导入存量数据
    - 使用insert ignore，避免已经被触发器插入
    - insert时需要锁，避免delete引起的时序问题
      - select读到数据A，请求delete A，触发器在新库空操作，然后insert到新库
      - 实际上pt-osc使用insert ignore select  xxx from old_table lock in share mode
        - insert  select 时持有锁，不会发生这种时序问题
- gh-ost
  - 应用binlog处理新增的修改
    - insert，转为replace into
    - delete，转为delete
    - update，不变
      - 未导入时，update空操作，后续会将新数据导入
  - 导入存量数据，与pt-osc类似
    - insert ignore select  xxx from old_table lock in share mode



## Redis Cluster

Redis对一致性要求并不高，方案可以作为参考

- 全量同步
  - master导出RDB文件，slave加载
  - 导出RDB期间的读写记录在缓冲区

- 增量同步
  - 维持长连接，命令传播
  - 使用的是环形缓冲区，速度不匹配会丢数据导致不一致



### slot迁移

新老节点配合

从老节点迁移数据到新节点

请求key时，如果该key已经从老节点迁移到新节点，老节点会返回ASK命令

客户端请求新节点

迁移完成后，再次请求原本在老节点的key，会返回MOVE命令，客户端可以更新路由



# 数据一致性

一般情况下，双写失败的概率很小，做了数据校验后进行切换，数据不一致概率很低，迁移后还可以再次校验

但双写无法保证新老存储的事务性，总归是有不一致的概率



下面讨论两种数据一致性高且比较通用的方案



## 方案一：全量同步+增量迁移

导出快照，并且记录导出快照时位点，同步存量数据

利用binlog追增量数据，主从差异降为0或者很小时，禁止主库写入，然后进行切换



停写可以保证数据一致性，由于大部分都有分库分表，停写只会影响一部分用户

注意在切换后，避免遗留/新增的请求仍然访问老库 



## 方案二：细粒度迁移，写操作与迁移互斥

方案一利用binlog有序，导入全量存量数后追增量数据

还可以分uid迁移，可以理解成对uid的操作加分布式读写锁

写操作与迁移操作竞争同一把分布式锁

- 用户在线的读写请求
  - 读时判断路由，读新库还是老库 (已经切到新库，读了老库不算问题)
  - 写时加锁，与用户其余写请求、离线迁移互斥

- worker离线的迁移请求
  - 分uid迁移，控制速率



更具体一点的方案参考：

新建一张迁移状态表，字段 id,uid,status,ctime,mtime,finish等

- 用户读请求，读迁移状态表，如果finish=1,读新库，否则读老库

- 用户写请求，先判断是否迁移完成，可以使用缓存等策略加速
  - 状态为待迁移时，更新状态占用锁，update set status = '迁移中' where status  = '待迁移' and uid = xxx，然后迁移数据到新库，迁移失败释放锁，下次迁移时删除新库数据；迁移成功后更新status = '迁移成功'，更新失败释放锁，返回处理失败



与分布式锁类似，异常情况下可能会无法释放锁。可以增加一些逻辑，添加heartbeat字段，超时自动剥夺锁，worker定时扫描人工处理等等



该方案将写与迁移互斥，保证数据一致性

写持有锁时，其他写无法进行， 迁移无法进行

迁移持有锁时，写无法进行



实际操作时，可以进行迁移操作，迁移后更新status，但是并不读写新库

观察服务与新老存储负载情况，再决定是否可采取该方案

---

当然，还是要看具体场景

有些场景会有更简单的解决方案