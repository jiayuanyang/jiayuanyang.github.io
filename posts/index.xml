<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on jiayuan&#39;s Blog</title>
    <link>/posts/</link>
    <description>Recent content in Posts on jiayuan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2024 09:00:00 +0800</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>InnoDB索引占用空间分析</title>
      <link>/posts/innodb-space/</link>
      <pubDate>Mon, 27 May 2024 09:00:00 +0800</pubDate>
      
      <guid>/posts/innodb-space/</guid>
      <description>背景 一两年前在申请Redis资源时，预估了N GB，上线后查看监控， 数据占用大于N GB（大很多）
因此阅读Redis源码，了解到内部一些数据结构的额外开销：dictEntry，robj，sds header，malloc碎片等等
对于MySQL，一直不确定空间在上有哪些开销
问题 新建如下表，插入100W行记录，占用多大空间？
create table test ( id bigint not null auto_increment, uid bigint not null, status int not null, ctime bigint not null, mtime bigint not null, primary key (id) ) engine = innodb; uid、status分别新增索引，会各自增加多大空间？
主键是bigint，一个page非叶子节点最多多少条记录？ 主键是int，又是多少
-- 存储过程插入100W行 DROP PROCEDURE IF EXISTS insert_batch; DELIMITER $$ CREATE PROCEDURE insert_batch(max_num INT) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT INSERT INTO test (uid, status, ctime, mtime) VALUES (1, 2, 3, 4); SET i = i + 1; UNTIL i = max_num END REPEAT; COMMIT; END$$ DELIMITER ; call insert_batch(1000000); 分析工具 innodb_ruby 作者博客</description>
      <content>&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;
&lt;p&gt;一两年前在申请Redis资源时，预估了N GB，上线后查看监控， 数据占用大于N GB（大很多）&lt;/p&gt;
&lt;p&gt;因此阅读Redis源码，了解到内部一些数据结构的额外开销：dictEntry，robj，sds header，malloc碎片等等&lt;/p&gt;
&lt;p&gt;对于MySQL，一直不确定空间在上有哪些开销&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;新建如下表，插入100W行记录，占用多大空间？&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;create table test (
    id bigint not null auto_increment,
    uid bigint not null,
    status int not null,
    ctime bigint not null,
    mtime bigint not null,
    primary key (id)
) engine = innodb;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;uid、status分别新增索引，会各自增加多大空间？&lt;/p&gt;
&lt;p&gt;主键是bigint，一个page非叶子节点最多多少条记录？ 主键是int，又是多少&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-- 存储过程插入100W行
DROP PROCEDURE IF EXISTS insert_batch;
DELIMITER $$
CREATE PROCEDURE insert_batch(max_num INT)
BEGIN
    DECLARE i INT DEFAULT 0;
    SET autocommit = 0;
    REPEAT
        INSERT INTO test (uid, status, ctime, mtime) VALUES (1, 2, 3, 4);
        SET i = i + 1;
        UNTIL i = max_num
    END REPEAT;
    COMMIT;
END$$
DELIMITER ;

call insert_batch(1000000);
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;分析工具&#34;&gt;分析工具&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jeremycole/innodb_ruby&#34;&gt;innodb_ruby&lt;/a&gt;      &lt;a href=&#34;https://blog.jcole.us/innodb/&#34;&gt;作者博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ibd文件分析&lt;/p&gt;
&lt;h1 id=&#34;一索引空间占用&#34;&gt;一、索引空间占用&lt;/h1&gt;
&lt;p&gt;不同版本可能会有不同，仅供参考。下面测试使用MySQL 5.7版本&lt;/p&gt;
&lt;h2 id=&#34;10-结论&#34;&gt;1.0 结论&lt;/h2&gt;
&lt;h3 id=&#34;聚簇索引&#34;&gt;聚簇索引&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;叶子节点
&lt;ul&gt;
&lt;li&gt;各字段大小 +  18B（record header=5B + roll_ptr=7B + trx_id=6B）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非叶子节点
&lt;ul&gt;
&lt;li&gt;主键大小 + 5B(record header) + 4B（指针）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;二级索引&#34;&gt;二级索引&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引字段大小 + 主键大小 + 5B(record header)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引字段大小 + 主键大小  + 5B(record header) + 4B（指针）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他影响因素&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_fill_factor&#34;&gt;innodb_fill_factor&lt;/a&gt; page内默认预留1/16空间&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;测试验证时，叶子节点有预留，非叶子节点不预留&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;page directory：平均4条record一个slot，每个slot 2B，即1个记录0.5B&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个page有128B的元数据开销&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;页内碎片&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.jcole.us/innodb/&#34;&gt;作者博客&lt;/a&gt;里可以找到详细说明&lt;/p&gt;
&lt;h3 id=&#34;计算&#34;&gt;计算&lt;/h3&gt;
&lt;h4 id=&#34;聚簇索引-1&#34;&gt;聚簇索引&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;记录大小 54B：各个字段占用 8(id)+8(uid)+8(ctime)+8(ctime)+4(status) = 36B + 额外开销18B=54B&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一页最多记录数：(16384 -128)*15/16 / 54.5 = 279.6330&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;记录大小： 8+5+4 = 17B&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128) / 17.5 = 928.9143&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;二级索引-idx_status为例&#34;&gt;二级索引 idx_status为例&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;叶子节点
&lt;ul&gt;
&lt;li&gt;记录大小：4+8+5 = 17&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128)*15/16 / 17.5 = 870.8571&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非叶子节点
&lt;ul&gt;
&lt;li&gt;记录大小：4+8+5+4 = 21&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128) / 21.5 = 756.0930&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面使用innodb_space分析验证&lt;/p&gt;
&lt;h2 id=&#34;11-聚簇索引-叶子节点&#34;&gt;1.1 聚簇索引 叶子节点&lt;/h2&gt;
&lt;h3 id=&#34;叶子节点使用page数量--3585&#34;&gt;叶子节点使用page数量 = 3585&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522232345900.png&#34; alt=&#34;image-20240522232345900&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里只分析INDEX页&lt;/p&gt;
&lt;p&gt;ALLOCATED也会占用ibd空间，具体规则未深入研究&lt;/p&gt;
&lt;p&gt;space-indexes显示 ibd文件中，叶子节点使用了3585个page (每个page 16KB)&lt;/p&gt;
&lt;h3 id=&#34;叶子节点的记录数是1392042793583&#34;&gt;叶子节点的记录数是139+204+279*3583&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523001012100.png&#34; alt=&#34;image-20240523001012100&#34;&gt;&lt;/p&gt;
&lt;p&gt;index-level-summary   -l 表示第几层 -l 0为叶子节点&lt;/p&gt;
&lt;p&gt;page4 139条记录，page3616 204条记录，剩余3583个page都是279条记录，139+204+279 * 3583 = 100W&lt;/p&gt;
&lt;p&gt;一页最多279个记录，符合计算&lt;/p&gt;
&lt;p&gt;page5， data=15066, records=279 15066/279=54，符合计算&lt;/p&gt;
&lt;p&gt;free=1050 表示每页有1050剩余空间&lt;/p&gt;
&lt;p&gt;1050/16384 = 0.0641 与 innodb_fill_factor 1/16=0.0625接近&lt;/p&gt;
&lt;h3 id=&#34;每页的占用&#34;&gt;每页的占用&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522234350316.png&#34; alt=&#34;image-20240522234350316&#34;&gt;&lt;/p&gt;
&lt;p&gt;128B(页的结构信息) + 54 * 279（每条记录大小） * 140 (page directory开销) + 1050 (空闲) = 16384&lt;/p&gt;
&lt;p&gt;一字节不差&lt;/p&gt;
&lt;h3 id=&#34;page-directory-为什么是140字节&#34;&gt;page directory 为什么是140字节&lt;/h3&gt;
&lt;p&gt;每个slot 2B&lt;/p&gt;
&lt;p&gt;279 / 4 = 69.75，是向上进到70的吗？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522235107637.png&#34; alt=&#34;image-20240522235107637&#34;&gt;&lt;/p&gt;
&lt;p&gt;分析page5的page directory&lt;/p&gt;
&lt;p&gt;可见并不是进上去的，确实有70个slot，其中infimum的owned固定为1 只包含自身，supremum为8，除自身外还有7个   4*68 + 7 = 279 刚好&lt;/p&gt;
&lt;h2 id=&#34;12-聚簇索引-非叶子节点&#34;&gt;1.2 聚簇索引 非叶子节点&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523001846807.png&#34; alt=&#34;image-20240523001846807&#34;&gt;&lt;/p&gt;
&lt;p&gt;-l 1可以看到，每页最多928个记录，符合计算&lt;/p&gt;
&lt;p&gt;15776/928 = 17，记录大小符合&lt;/p&gt;
&lt;h2 id=&#34;13-二级索引-叶子节点&#34;&gt;1.3 二级索引 叶子节点&lt;/h2&gt;
&lt;p&gt;添加索引 alter table test add index idx_uid (uid);&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002302195.png&#34; alt=&#34;image-20240523002302195&#34;&gt;&lt;/p&gt;
&lt;p&gt;再添加一个索引  alter table test add index idx_status (status);&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002320724.png&#34; alt=&#34;image-20240523002320724&#34;&gt;&lt;/p&gt;
&lt;p&gt;idx_uid叶子大小应该为 8+8+5 = 21B&lt;/p&gt;
&lt;p&gt;idx_status叶子节点大小应该为 4+8+5=17B&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002706499.png&#34; alt=&#34;image-20240523002706499&#34;&gt;&lt;/p&gt;
&lt;p&gt;idx_status举例&lt;/p&gt;
&lt;p&gt;一页最多928个记录，符合计算&lt;/p&gt;
&lt;p&gt;15776/928 = 17，符合计算&lt;/p&gt;
&lt;p&gt;用叶子节点used进行校验&lt;/p&gt;
&lt;p&gt;idx_uid.used/idx_status.used = 1325 / 1078 = 1.2291&lt;/p&gt;
&lt;p&gt;21/17 = 1.2353 接近&lt;/p&gt;
&lt;h2 id=&#34;14-二级索引-非叶子节点&#34;&gt;1.4 二级索引 非叶子节点&lt;/h2&gt;
&lt;p&gt;idx_status非叶子节点大小  4+8+5+4 = 21&lt;/p&gt;
&lt;p&gt;用-l 1的数据计算&lt;/p&gt;
&lt;p&gt;上面计算出最大记录数是756.0930，这里是755&lt;/p&gt;
&lt;p&gt;15855/755=21 符合计算&lt;/p&gt;
&lt;h2 id=&#34;15-总结&#34;&gt;1.5 总结&lt;/h2&gt;
&lt;h3 id=&#34;聚簇索引-2&#34;&gt;聚簇索引&lt;/h3&gt;
&lt;p&gt;叶子节点除了各字段外，还有roll_ptr，trx_id&lt;/p&gt;
&lt;h3 id=&#34;二级索引-1&#34;&gt;二级索引&lt;/h3&gt;
&lt;p&gt;叶子节点没有roll_ptr，没有trx_id&lt;/p&gt;
&lt;p&gt;叶子节点有主键id&lt;/p&gt;
&lt;p&gt;非叶子节点也有主键id&lt;/p&gt;
&lt;p&gt;ps：二级索引上没有roll_ptr、trx_id，MVCC可见性如何判断？&lt;/p&gt;
&lt;h3 id=&#34;不同主键大小非叶子节点一页最多存多少&#34;&gt;不同主键大小，非叶子节点一页最多存多少？&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bigint
&lt;ul&gt;
&lt;li&gt;非叶子节点最多928.8 = (16384-128) /(8+5+4+0.5)&lt;/li&gt;
&lt;li&gt;实际验证是928&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;int
&lt;ul&gt;
&lt;li&gt;非叶子节点最多1204.0 = (16384-128) /(8+5+4+0.5)&lt;/li&gt;
&lt;li&gt;实际验证不是1204， 是1203
&lt;ul&gt;
&lt;li&gt;与上面 计算得到756.0930，实际755类似， 实际上是能够再存一个记录，但是剩下一个空位&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;二一些常见问题&#34;&gt;二、一些常见问题&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;为什么表不要超过千万行
&lt;ul&gt;
&lt;li&gt;bigint主键， 假设记录1k，叶子节点大概存15个
&lt;ul&gt;
&lt;li&gt;两层非叶子节点928^2 = 861184 = 86.1W ，三层记录数1300W&lt;/li&gt;
&lt;li&gt;三层非叶子节点928^3=799178752  7.99亿 四层记录数 120亿&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;三层时，前2层非叶子节点基本可以全缓存在内存，占用(928+1)*16K = 232.25MB
&lt;ul&gt;
&lt;li&gt;增删改查仅需要访问一次磁盘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;四层时，不可能完全缓存前3层非叶子节点，需要访问2次磁盘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;为什么索引快
&lt;ul&gt;
&lt;li&gt;主键上字段很多，数据量大&lt;/li&gt;
&lt;li&gt;二级索引数据量小&lt;/li&gt;
&lt;li&gt;回表成本很高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分表分多少合适，以uid mod分表数为例
&lt;ul&gt;
&lt;li&gt;首先，一定不是越多越好，分65536个，16384个表显然不合适&lt;/li&gt;
&lt;li&gt;bigint主键， 完全缓存前两层非叶子节点占用14.5MB&lt;/li&gt;
&lt;li&gt;如果所有uid都是均匀访问
&lt;ul&gt;
&lt;li&gt;分1024个表，前两层全在内存，占用14.5G，基本能够容纳&lt;/li&gt;
&lt;li&gt;分2048个表，想要全缓存前两层，需要29G内存， 比较大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;压测、实践验证为准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;三遇到的慢查询分析&#34;&gt;三、遇到的慢查询分析&lt;/h1&gt;
&lt;p&gt;explain&lt;/p&gt;
&lt;h2 id=&#34;31-varchar字段-like-xxx-不走索引&#34;&gt;3.1 varchar字段 like %xxx% 不走索引&lt;/h2&gt;
&lt;p&gt;select * from t where country like&amp;rsquo;%US%&amp;rsquo; ; 用不了country索引&lt;/p&gt;
&lt;p&gt;只能全表扫描&lt;/p&gt;
&lt;p&gt;全表扫描一定比用索引快吗？&lt;/p&gt;
&lt;p&gt;如果满足where的行比较少，只有少数回表，走索引会更快&lt;/p&gt;
&lt;p&gt;force index (idx_country)， 是无法用上country索引的&lt;/p&gt;
&lt;p&gt;需要使用子查询才能走country索引&lt;/p&gt;
&lt;p&gt;select * from t where id in(select id from t where country like &amp;lsquo;%US%&amp;rsquo;)&lt;/p&gt;
&lt;p&gt;子查询里可以用到idx_country索引，只查id，不会走全表扫描&lt;/p&gt;
&lt;p&gt;（这个场景后续改用了ES做查询）&lt;/p&gt;
&lt;h2 id=&#34;32-覆盖索引&#34;&gt;3.2 覆盖索引&lt;/h2&gt;
&lt;p&gt;select count(*) from t where uid = 123 and status =  1; (uid有索引，(uid, status)没有索引)&lt;/p&gt;
&lt;p&gt;大部分情况会用上uid索引，再回表判断status&lt;/p&gt;
&lt;p&gt;有些uid数据较多，会走全表扫描&lt;/p&gt;
&lt;p&gt;创建(uid, status)覆盖索引， 避免回表，就会选择索引了&lt;/p&gt;
&lt;p&gt;覆盖索引，有时，仅仅为了覆盖，将原来多字段的聚簇索引抽出部分字段，组成一个“窄“表，加快查询&lt;/p&gt;
&lt;p&gt;实际上就是空间换时间的体现&lt;/p&gt;
&lt;h2 id=&#34;34-避免选择错误索引&#34;&gt;3.4 避免选择错误索引&lt;/h2&gt;
&lt;p&gt;select where uid = 123 order by ctime limit 10;&lt;/p&gt;
&lt;p&gt;uid有索引，ctime有索引&lt;/p&gt;
&lt;p&gt;我们更希望选择uid索引，但由于order by ctime，优化器有时会选择ctime&lt;/p&gt;
&lt;p&gt;除了force index，还可以修改sql，order by (ctime + 0) 或者 order by ctime asc,  id desc (ctime asc, id asc 可以用索引)&lt;/p&gt;
&lt;h2 id=&#34;35-深翻页&#34;&gt;3.5 深翻页&lt;/h2&gt;
&lt;p&gt;翻页时offset过大， 查询会变慢&lt;/p&gt;
&lt;p&gt;因为被翻页的记录也都要回表&lt;/p&gt;
&lt;p&gt;一种方法是不用offset，每次带上上次的id，where id &amp;gt; lastid 来查询&lt;/p&gt;
&lt;p&gt;另外也可以用子查询，先查出id，再用id来查&lt;/p&gt;
&lt;h2 id=&#34;36-离线worker扫表&#34;&gt;3.6 离线worker扫表&lt;/h2&gt;
&lt;p&gt;需求是扫描ctime再(111, 999)范围，status是unpay的订单&lt;/p&gt;
&lt;p&gt;select * from order_info where ctime &amp;gt; 111 and ctime &amp;lt; 999 and status = &amp;lsquo;unpay&amp;rsquo; and id &amp;gt; $lastID order by id asc limit 1;&lt;/p&gt;
&lt;p&gt;走了id索引，但是用主键扫描，首次定位第一个id，耗时较长&lt;/p&gt;
&lt;p&gt;方法1：先用ctime索引，查出最大最小id  select min(id), max(id) from order_info where xxx&lt;/p&gt;
&lt;p&gt;方法2：直接使用ctime索引，再回表，select * from order_info where (ctime = $lastTime and id &amp;gt; $lastID) or ctime &amp;gt; lastTime&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Golang Mutex</title>
      <link>/posts/golang-mutex/</link>
      <pubDate>Mon, 20 May 2024 23:32:13 +0800</pubDate>
      
      <guid>/posts/golang-mutex/</guid>
      <description>问题引入 上一篇博客提到IO阻塞，打印log时，时间可能是乱序的
由此来分析Go中sync.Mutex在不同场景的表现
测试代码 go1.19，在每分钟0s Write时阻塞3s
package main import ( &amp;#34;log&amp;#34; &amp;#34;math/rand&amp;#34; &amp;#34;os&amp;#34; &amp;#34;sync/atomic&amp;#34; &amp;#34;time&amp;#34; ) type DelayWriter struct { *os.File } var timeslot [60]int64 func (w *DelayWriter) Write(b []byte) (int, error) { now := time.Now() if now.Second() == 0 &amp;amp;&amp;amp; atomic.AddInt64(&amp;amp;timeslot[now.Minute()], 1) == 1 { time.Sleep(time.Millisecond * 3333) // 每分钟 0s，模拟写日志阻塞 } return w.File.Write(b) } func NewDelayWriter(filename string) *DelayWriter { f, err := os.OpenFile(filename, os.O_CREATE|os.O_RDWR|os.O_APPEND|os.O_TRUNC, 0666) if err != nil { panic(err) } return &amp;amp;DelayWriter{f} } var logger = log.</description>
      <content>&lt;h1 id=&#34;问题引入&#34;&gt;问题引入&lt;/h1&gt;
&lt;p&gt;上一篇博客提到IO阻塞，打印log时，时间可能是乱序的&lt;/p&gt;
&lt;p&gt;由此来分析Go中sync.Mutex在不同场景的表现&lt;/p&gt;
&lt;p&gt;测试代码 go1.19，在每分钟0s Write时阻塞3s&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package main

import (
	&amp;#34;log&amp;#34;
	&amp;#34;math/rand&amp;#34;
	&amp;#34;os&amp;#34;
	&amp;#34;sync/atomic&amp;#34;
	&amp;#34;time&amp;#34;
)

type DelayWriter struct {
	*os.File
}

var timeslot [60]int64

func (w *DelayWriter) Write(b []byte) (int, error) {
	now := time.Now()
	if now.Second() == 0 &amp;amp;&amp;amp; atomic.AddInt64(&amp;amp;timeslot[now.Minute()], 1) == 1 {
		time.Sleep(time.Millisecond * 3333) // 每分钟 0s，模拟写日志阻塞
	}
	return w.File.Write(b)
}

func NewDelayWriter(filename string) *DelayWriter {
	f, err := os.OpenFile(filename, os.O_CREATE|os.O_RDWR|os.O_APPEND|os.O_TRUNC, 0666)
	if err != nil {
		panic(err)
	}
	return &amp;amp;DelayWriter{f}
}

var logger = log.New(NewDelayWriter(&amp;#34;test.log&amp;#34;), &amp;#34;&amp;#34;, log.LstdFlags|log.Lmicroseconds|log.Lshortfile)

func main() {
	for i := 0; i &amp;lt; 100; i++ {
		go func(index int) {
			var seq int
			for {
				seq++

				r := rand.Int31n(1000) + 100
				time.Sleep(time.Duration(r) * time.Millisecond)

				go serve(index, seq) // 处理请求
			}
		}(i)
	}

	time.Sleep(time.Minute * 2)
	logger.Println(&amp;#34;end&amp;#34;)
}

func serve(index int, seq int) {
	logger.Println(&amp;#34;recv request&amp;#34;, index, seq)
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果，可以看到在0s附近，时间有递增、递减、反复变化&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/golang-mutex-log.jpg&#34; alt=&#34;log&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;syncmutex实现分析&#34;&gt;sync.Mutex实现分析&lt;/h1&gt;
&lt;p&gt;当前sync.Mutex的实现比较复杂，引入了普通（normal）模式，饥饿（starvation）模式&lt;/p&gt;
&lt;p&gt;sync.Mutex代码经过了4个版本变化，直接看最新代码较难理解，接下来按照4个版本变化依次介绍&lt;/p&gt;
&lt;h2 id=&#34;信号量原语&#34;&gt;信号量原语&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Semacquire waits until *s &amp;gt; 0 and then atomically decrements it.
// It is intended as a simple sleep primitive for use by the synchronization
// library and should not be used directly.
func runtime_Semacquire(s *uint32)

// Semrelease atomically increments *s and notifies a waiting goroutine
// if one is blocked in Semacquire.
// It is intended as a simple wakeup primitive for use by the synchronization
// library and should not be used directly.
func runtime_Semrelease(s *uint32)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;v1simple-implementation&#34;&gt;V1：Simple implementation.&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Mutex struct {
    key  int32 // Indication of whether the lock is held
    sema int32 // Semaphore dedicated to block/wake up goroutine
}

func (m *Mutex) Lock() {
    if atomic.AddInt32(&amp;amp;m.key, 1) == 1 {
        return
    }
    semacquire(&amp;amp;m.sema)
}

func (m *Mutex) Unlock() {
    if atomic.AddInt32(&amp;amp;m.key, -1) == 0 {
        return
    }
    semrelease(&amp;amp;m.sema)
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock&#34;&gt;Lock&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;key原子增加
&lt;ul&gt;
&lt;li&gt;新值为1获取到锁；否则：&lt;/li&gt;
&lt;li&gt;信号量semacquire&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unlock&#34;&gt;Unlock&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;key原子增加-1
&lt;ul&gt;
&lt;li&gt;新值为0，说明没有其他在等待的，直接返回；否则：&lt;/li&gt;
&lt;li&gt;信号量semrelease，唤醒等待的goroutine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举例&#34;&gt;举例&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;goroutine1（简称g1） Lock&lt;/li&gt;
&lt;li&gt;g2请求Lock，semacquire&lt;/li&gt;
&lt;li&gt;g1 Unlock，semrelease，唤醒等待者g2&lt;/li&gt;
&lt;li&gt;g2进入临界区操作，操作完 再Unlock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果g2 原子incr key，还未调用 semacquire(&amp;amp;m.sema)&lt;/p&gt;
&lt;p&gt;g1先semrelease(&amp;amp;m.sema)，还能不能唤醒g2？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g1 Lock 成功&lt;/li&gt;
&lt;li&gt;g2 Lock，进程暂停&lt;/li&gt;
&lt;li&gt;g1 Unlock，semrelease(&amp;amp;m.sema)，此时g1还未休眠，无法唤醒，信号量自增&lt;/li&gt;
&lt;li&gt;g2 进程继续，semacquire(&amp;amp;m.sema)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;g2执行semacquire(&amp;amp;m.sema)  后&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Semacquire waits until *s &amp;gt; 0 and then atomically decrements it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;还是能够正常执行&lt;/p&gt;
&lt;h2 id=&#34;v2new-goroutine-participates-in-lock-competition&#34;&gt;V2：New Goroutine participates in lock competition.&lt;/h2&gt;
&lt;p&gt;信号量先进先出，V1版本如果有goroutine在等待了，新来的&lt;strong&gt;正在&lt;/strong&gt;运行的goroutine也必须等待&lt;/p&gt;
&lt;p&gt;唤醒老的在睡眠的goroutine，显然开销大于正在运行的goroutine直接获取锁&lt;/p&gt;
&lt;p&gt;V2版本，允许已经有goroutine睡眠等待时，正在运行的goroutine先获取到锁&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Mutex struct {
   state int32
   sema  uint32
}

const (
   mutexLocked = 1 &amp;lt;&amp;lt; iota // mutex is locked
   mutexWoken
   mutexWaiterShift = iota
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock-1&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
   if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
      return
   }

   awoke := false
   for {
      old := m.state
      var new int32
      if old&amp;amp;mutexLocked == 0 {
         new = old | mutexLocked
      } else {
      	 new = old + 1&amp;lt;&amp;lt;mutexWaiterShift
      }
      if awoke {
         new &amp;amp;^= mutexWoken
      }
      if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
         if old&amp;amp;mutexLocked == 0 {
            break
         }
         runtime.Semacquire(&amp;amp;m.sema)
         awoke = true
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;state cas从0到1成功，直接得到锁； 否则
&lt;ul&gt;
&lt;li&gt;for 循环，进行cas操作
&lt;ul&gt;
&lt;li&gt;如果当前没有锁标记，加上锁标记&lt;/li&gt;
&lt;li&gt;有锁标记，增加一个等待计数&lt;/li&gt;
&lt;li&gt;另外，awoke为true时，清除mutexWoken比特位&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cas进行更新，更新成功时
&lt;ul&gt;
&lt;li&gt;旧值没有锁标记，表示本次cas拿到锁，直接退出；否则&lt;/li&gt;
&lt;li&gt;Semacquire，进入睡眠等待，等待结束后，awoke置为true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unlock-1&#34;&gt;Unlock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Unlock() {
   new := atomic.AddInt32(&amp;amp;m.state, -mutexLocked)

   old := new
   for {
      if old&amp;gt;&amp;gt;mutexWaiterShift == 0 || old&amp;amp;(mutexLocked|mutexWoken) != 0 {
         return
      }

      new = (old - 1&amp;lt;&amp;lt;mutexWaiterShift) | mutexWoken
      if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
         runtime.Semrelease(&amp;amp;m.sema)
         return
      }
      old = m.state
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;原子减-1&lt;/li&gt;
&lt;li&gt;for循环
&lt;ul&gt;
&lt;li&gt;如果没有等待者， 或者 有lock/woken标记， 退出；否则&lt;/li&gt;
&lt;li&gt;cas更新，新值为减去1个等待计数|woken标记
&lt;ul&gt;
&lt;li&gt;cas成功，Semrelease唤醒一个等待者&lt;/li&gt;
&lt;li&gt;cas失败， 继续判断&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举例-1&#34;&gt;举例&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;g1 Lock&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;g2, g3 请求Lock，进入休眠等待&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;g1 Unlock 和 g4 Lock， g5 Lock几乎同时发生&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;第一种情况 g1 Unlock 先完成了原子-1&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g4, g5看到 old&amp;amp;mutexLocked == 0，都增加locked比特，进行cas更新&lt;/li&gt;
&lt;li&gt;g4 g5其中一个更新成功， 获取到锁&lt;/li&gt;
&lt;li&gt;另一个更新失败，再次进入for循环，
&lt;ol&gt;
&lt;li&gt;看到  old&amp;amp;mutexLocked != 0， 增加等待计数，cas更新成功后休眠等待&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;此时g1 进入for循环， 看到已经&lt;strong&gt;有了mutex标记&lt;/strong&gt;，结束Unlock，&lt;strong&gt;不需要再进行唤醒&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二种情况 g1 Unlock 先完成了原子-1，并且cas新增woken标记成功&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g4其中一个获取锁成功，g5获取锁失败，再次进入for循环，&lt;strong&gt;还未进行第二次cas更新&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;g4 很快完成操作，调用Unlock&lt;/li&gt;
&lt;li&gt;此时，g4看到有&lt;strong&gt;woken标记&lt;/strong&gt;，结束Unlock，不需要再进行唤醒
&lt;ol&gt;
&lt;li&gt;如果g4第二次cas更新成功，去掉woken标记，此时g4还会进行唤醒&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;问题&#34;&gt;问题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为什么要加woken标记&lt;/li&gt;
&lt;li&gt;CAS会不会有ABA问题？&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;问题1为什么要加woken标记&#34;&gt;问题1：为什么要加woken标记&lt;/h4&gt;
&lt;p&gt;防止惊群，只需要唤醒一个&lt;/p&gt;
&lt;p&gt;g1 Lock成功&lt;/p&gt;
&lt;p&gt;g2, g3, g4 请求Lock， 进入睡眠等待&lt;/p&gt;
&lt;p&gt;g1 Unlock，准备semrelease（还未调用）&lt;/p&gt;
&lt;p&gt;此时 新来g5 可以Lock成功&lt;/p&gt;
&lt;p&gt;g5 Unlock，准备semrelease（还未调用）&lt;/p&gt;
&lt;p&gt;然后 g1, g5 同时semrelease，会唤醒多个在睡眠等待的goroutine&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;加了woken标记，就只会唤醒一个&lt;/p&gt;
&lt;p&gt;woken的goroutine和新来的（也可能没有新来的）竞争， 失败则再次睡眠&lt;/p&gt;
&lt;p&gt;被woken的会去掉woken标记进行cas， cas更新成功，都会去掉woken标记，未抢到锁时还会进入睡眠&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;问题2会不会有aba问题&#34;&gt;问题2：会不会有ABA问题&lt;/h4&gt;
&lt;p&gt;不会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlock时， cas什么时候失败？&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Unlock 原子-1后，没有lock标记，新来的g先于Unlock的cas加上了lock，cas失败
&lt;ol&gt;
&lt;li&gt;此时，在进入判断，会认为有lock标记，退出&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;和上面类似，Unlock 原子-1后，没有lock标记，新来的g先于Unlock的cas加上了lock，cas失败。但是在Unlock再次判断之前，新来的g调用了Unlock。
&lt;ol&gt;
&lt;li&gt;此时有两个g在Unlock&lt;/li&gt;
&lt;li&gt;两个都判断没有lock标记，假设还有一个等待者， 这是二者都会尝试加上woken标记，并减去1个等待者&lt;/li&gt;
&lt;li&gt;只有一个添加woken成功，然后唤醒等待者，另一个cas失败，再次判断，由于woken存在或者等待者为0退出&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;cas判断，会不会有N次semacquire，但是semrelease小于N?
&lt;ul&gt;
&lt;li&gt;不会，有多少次 semacquire， 就有多少次 semrelease&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v3give-new-goroutines-some-more-chances&#34;&gt;V3：Give new goroutines some more chances.&lt;/h2&gt;
&lt;p&gt;增加自旋&lt;/p&gt;
&lt;p&gt;有mutexLocked时，进行有限次数自旋，并尝试cas增加mutexWoken标记&lt;/p&gt;
&lt;p&gt;整体逻辑和V2类似&lt;/p&gt;
&lt;h3 id=&#34;lock-2&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
    // Fast path: grab unlocked mutex.
    if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
        return
    }

    awoke := false
    iter := 0
    for {
        old := m.state
        new := old | mutexLocked
        if old&amp;amp;mutexLocked != 0 {
            if runtime_canSpin(iter) {
                if !awoke &amp;amp;&amp;amp; old&amp;amp;mutexWoken == 0 &amp;amp;&amp;amp; old&amp;gt;&amp;gt;mutexWaiterShift != 0 &amp;amp;&amp;amp;
                    atomic.CompareAndSwapInt32(&amp;amp;m.state, old, old|mutexWoken) {
                    awoke = true
                }
                runtime_doSpin()
                iter++
                continue
            }
            new = old + 1&amp;lt;&amp;lt;mutexWaiterShift
        }
        if awoke {
            new &amp;amp;^= mutexWoken
        }
        if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
            if old&amp;amp;mutexLocked == 0 {
                break
            }
            runtime_Semacquire(&amp;amp;m.sema)
            awoke = true
            iter = 0
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;unlock-2&#34;&gt;Unlock&lt;/h3&gt;
&lt;p&gt;逻辑同V2&lt;/p&gt;
&lt;h2 id=&#34;v4solve-the-old-goroutine-starvation-problem&#34;&gt;V4：Solve the old goroutine starvation problem.&lt;/h2&gt;
&lt;p&gt;V3版本新来的goroutine更容易竞争到锁，老的竞争者可能一直得不到锁&lt;/p&gt;
&lt;p&gt;V4版本解决饥饿问题&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;const (
    mutexLocked = 1 &amp;lt;&amp;lt; iota // mutex is locked
    mutexWoken
    mutexStarving // separate out a starvation token from the state field
    mutexWaiterShift = iota
    starvationThresholdNs = 1e6    
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock-3&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
	// Fast path: grab unlocked mutex.
	if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
		return
	}
	// Slow path (outlined so that the fast path can be inlined)
	m.lockSlow()
}

func (m *Mutex) lockSlow() {
	var waitStartTime int64
	starving := false
	awoke := false
	iter := 0
	old := m.state
	for {
		// Don&amp;#39;t spin in starvation mode, ownership is handed off to waiters
		// so we won&amp;#39;t be able to acquire the mutex anyway.
		if old&amp;amp;(mutexLocked|mutexStarving) == mutexLocked &amp;amp;&amp;amp; runtime_canSpin(iter) {
			// Active spinning makes sense.
			// Try to set mutexWoken flag to inform Unlock
			// to not wake other blocked goroutines.
			if !awoke &amp;amp;&amp;amp; old&amp;amp;mutexWoken == 0 &amp;amp;&amp;amp; old&amp;gt;&amp;gt;mutexWaiterShift != 0 &amp;amp;&amp;amp;
				atomic.CompareAndSwapInt32(&amp;amp;m.state, old, old|mutexWoken) {
				awoke = true
			}
			runtime_doSpin()
			iter++
			old = m.state
			continue
		}
		new := old
		// Don&amp;#39;t try to acquire starving mutex, new arriving goroutines must queue.
		if old&amp;amp;mutexStarving == 0 {
			new |= mutexLocked
		}
		if old&amp;amp;(mutexLocked|mutexStarving) != 0 {
			new += 1 &amp;lt;&amp;lt; mutexWaiterShift
		}
		// The current goroutine switches mutex to starvation mode.
		// But if the mutex is currently unlocked, don&amp;#39;t do the switch.
		// Unlock expects that starving mutex has waiters, which will not
		// be true in this case.
		if starving &amp;amp;&amp;amp; old&amp;amp;mutexLocked != 0 {
			new |= mutexStarving
		}
		if awoke {
			// The goroutine has been woken from sleep,
			// so we need to reset the flag in either case.
			if new&amp;amp;mutexWoken == 0 {
				throw(&amp;#34;sync: inconsistent mutex state&amp;#34;)
			}
			new &amp;amp;^= mutexWoken
		}
		if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
			if old&amp;amp;(mutexLocked|mutexStarving) == 0 {
				break // locked the mutex with CAS
			}
			// If we were already waiting before, queue at the front of the queue.
			queueLifo := waitStartTime != 0
			if waitStartTime == 0 {
				waitStartTime = runtime_nanotime()
			}
			runtime_SemacquireMutex(&amp;amp;m.sema, queueLifo, 1)
			starving = starving || runtime_nanotime()-waitStartTime &amp;gt; starvationThresholdNs
			old = m.state
			if old&amp;amp;mutexStarving != 0 {
				// If this goroutine was woken and mutex is in starvation mode,
				// ownership was handed off to us but mutex is in somewhat
				// inconsistent state: mutexLocked is not set and we are still
				// accounted as waiter. Fix that.
				if old&amp;amp;(mutexLocked|mutexWoken) != 0 || old&amp;gt;&amp;gt;mutexWaiterShift == 0 {
					throw(&amp;#34;sync: inconsistent mutex state&amp;#34;)
				}
				delta := int32(mutexLocked - 1&amp;lt;&amp;lt;mutexWaiterShift)
				if !starving || old&amp;gt;&amp;gt;mutexWaiterShift == 1 {
					// Exit starvation mode.
					// Critical to do it here and consider wait time.
					// Starvation mode is so inefficient, that two goroutines
					// can go lock-step infinitely once they switch mutex
					// to starvation mode.
					delta -= mutexStarving
				}
				atomic.AddInt32(&amp;amp;m.state, delta)
				break
			}
			awoke = true
			iter = 0
		} else {
			old = m.state
		}
	}

}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;unlock-3&#34;&gt;Unlock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Unlock() {
	// Fast path: drop lock bit.
	new := atomic.AddInt32(&amp;amp;m.state, -mutexLocked)
	if new != 0 {
		// Outlined slow path to allow inlining the fast path.
		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
		m.unlockSlow(new)
	}
}

func (m *Mutex) unlockSlow(new int32) {
	if new&amp;amp;mutexStarving == 0 {
		old := new
		for {
			// If there are no waiters or a goroutine has already
			// been woken or grabbed the lock, no need to wake anyone.
			// In starvation mode ownership is directly handed off from unlocking
			// goroutine to the next waiter. We are not part of this chain,
			// since we did not observe mutexStarving when we unlocked the mutex above.
			// So get off the way.
			if old&amp;gt;&amp;gt;mutexWaiterShift == 0 || old&amp;amp;(mutexLocked|mutexWoken|mutexStarving) != 0 {
				return
			}
			// Grab the right to wake someone.
			new = (old - 1&amp;lt;&amp;lt;mutexWaiterShift) | mutexWoken
			if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
				runtime_Semrelease(&amp;amp;m.sema, false, 1)
				return
			}
			old = m.state
		}
	} else {
		// Starving mode: handoff mutex ownership to the next waiter.
		// Note: mutexLocked is not set, the waiter will set it after wakeup.
		// But mutex is still considered locked if mutexStarving is set,
		// so new coming goroutines won&amp;#39;t acquire it.
		runtime_Semrelease(&amp;amp;m.sema, true, 1)
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;V1版本：先进先出，新来的goroutine只能排队&lt;/li&gt;
&lt;li&gt;V2版本：新来的g和被唤醒的g竞争，可能直接拿到锁
&lt;ul&gt;
&lt;li&gt;g被唤醒不是直接拿到锁， 与新来的g竞争&lt;/li&gt;
&lt;li&gt;最多只有一个g被唤醒，被唤醒的g竞争失败，清空woken标记，再次睡眠&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V3版本：增加自旋逻辑，临界区较少时，自旋一会就能拿到锁，无需进入睡眠
&lt;ul&gt;
&lt;li&gt;自旋时会尝试设置woken标记， 通知Unlock无需唤醒其他等待者&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V4版本：解决饥饿，新增mutexStarving标记位，引入普通/饥饿模式
&lt;ul&gt;
&lt;li&gt;普通模式，同上述&lt;/li&gt;
&lt;li&gt;饥饿模式：
&lt;ul&gt;
&lt;li&gt;新来的g，不竞争锁， 在队尾等待&lt;/li&gt;
&lt;li&gt;被唤醒的g，无需再次竞争，直接得到锁&lt;/li&gt;
&lt;li&gt;Unlock，不设置woken标记， 直接唤醒&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;回到最开始的问题&#34;&gt;回到最开始的问题&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;为什么日志时间乱序？ mutex是先进先出的吗&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;线上go版本是1.16，是上面V4版本的实现，以测试代码为例&lt;/p&gt;
&lt;p&gt;在每分钟0s Write时阻塞，此时后续的g，会先进行自旋，然后得不到锁，进入睡眠&lt;/p&gt;
&lt;p&gt;到这里还都是按照先后顺序的，先进入，先打印，不应该出现时间乱序&lt;/p&gt;
&lt;p&gt;第一个g，阻塞3s后，Write，然后Unlock&lt;/p&gt;
&lt;p&gt;如果此时没有新来的g，交给sema第一个等待者，wake后没有竞争直接拿到锁，是不会进入饥饿模式的&lt;/p&gt;
&lt;p&gt;但是如果有新来的g，Unlock时处于普通模式，新来的g与被唤醒的g竞争&lt;/p&gt;
&lt;p&gt;1）如果新来的g（g1）竞争到，被唤醒的g0等待了3s，会cas进入饥饿模式。不过，这个cas不一定成功。g1 Unlock时刚好新来g2，仍然可以得到锁。g0会再次cas更新，直到CAS成功，一定能够加上mutexStarving标记&lt;/p&gt;
&lt;p&gt;如果在CAS中直接获取到锁，会加上starving标记&lt;/p&gt;
&lt;p&gt;如果在CAS中没有获取到锁，也会加上starving标记，并且加入到sema的队头，仍然符合先进先出的顺序&lt;/p&gt;
&lt;p&gt;2）如果被唤醒的g竞争到，不会进入饥饿模式，新来的g加到信号量队尾&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上述分析说明mutex整体上是先进先出的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;普通模式，新来的g与被唤醒的g竞争，新来的g优势大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;即使实际已经饥饿，等待时间过长，但是依次唤醒，依次都得到所有权，并不会转变到饥饿模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;但是一但没有获取到锁，就会加上starving标记，后续进入饥饿模式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;饥饿模式时，新来的g直接加入队尾&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;那么， 上述日志的时间乱序，另有原因&lt;/p&gt;
&lt;p&gt;标准库log方法&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (l *Logger) Output(calldepth int, s string) error {
	now := time.Now() // get this early.
	var file string
	var line int
	l.mu.Lock()
	defer l.mu.Unlock()
	if l.flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		// Release lock while getting caller info - it&amp;#39;s expensive.
		l.mu.Unlock()
		var ok bool
		_, file, line, ok = runtime.Caller(calldepth)
		if !ok {
			file = &amp;#34;???&amp;#34;
			line = 0
		}
		l.mu.Lock()
	}
	l.buf = l.buf[:0]
	l.formatHeader(&amp;amp;l.buf, now, file, line)
	l.buf = append(l.buf, s...)
	if len(s) == 0 || s[len(s)-1] != &amp;#39;\n&amp;#39; {
		l.buf = append(l.buf, &amp;#39;\n&amp;#39;)
	}
	_, err := l.out.Write(l.buf)
	return err
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;解释现象-个人理解&#34;&gt;解释现象 个人理解&lt;/h2&gt;
&lt;p&gt;时间是在Lock之前获取，为了获取代码行信息，会先Unlock，得到代码行数后，再Lock&lt;/p&gt;
&lt;p&gt;日志中时间乱序是因为 Unlock后，runtime.Caller耗时有差异，再次Lock的顺序与获取time.Now顺序不一致&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图片里第一个03s是因为新来的g先获取到了锁&lt;/li&gt;
&lt;li&gt;后续进入饥饿模式，时间是按顺序的&lt;/li&gt;
&lt;li&gt;饥饿模式退出后，有新来的g获取到了锁&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;去掉 Lshortfile|Llongfile， 再次运行，只会有零星的几个时间乱序，符合上述描述，还未实际进入饥饿模式时，新来的g也能够获取到锁&lt;/li&gt;
&lt;li&gt;将标准库内 runtime.Caller 注释掉，Lock， 仍然是Unlock， Lock, Unlock的顺序， 有阻塞3s，日志时间是有序的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;标准库log的优化&#34;&gt;标准库log的优化&lt;/h2&gt;
&lt;p&gt;go1.21 &lt;a href=&#34;https://github.com/golang/go/commit/c3b4c27fd31b51226274a0c038e9c10a65f11657&#34;&gt;代码&lt;/a&gt; 修改了log实现&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Performance:
	name           old time/op  new time/op  delta
	Concurrent-24  19.9µs ± 2%   8.3µs ± 1%  -58.37%  (p=0.000 n=10+10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不再是每个Log对象用一个buf，使用了[]byte池&lt;/p&gt;
&lt;p&gt;mutex只锁Write方法，runtime.Caller提前计算&lt;/p&gt;
&lt;p&gt;用新版本测试，不会再出现时间乱序的问题&lt;/p&gt;
&lt;h2 id=&#34;futex&#34;&gt;futex&lt;/h2&gt;
&lt;p&gt;linux用futex实现pthread_mutex_t&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/futex.2.html&#34;&gt;futex(2) — Linux manual page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;与go的实现有何不同&lt;/p&gt;
&lt;p&gt;后续分析&lt;/p&gt;
&lt;h1 id=&#34;参考文章&#34;&gt;参考文章&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://levelup.gitconnected.com/deep-understanding-of-golang-mutex-9964b02c56e9&#34;&gt;Deep Understanding of Golang Mutex&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Go异步日志实现</title>
      <link>/posts/golang-async-log/</link>
      <pubDate>Sat, 18 May 2024 12:47:51 +0800</pubDate>
      
      <guid>/posts/golang-async-log/</guid>
      <description>一、问题背景 线上某接口偶现pvlost，问题实例物理机所在磁盘都有大量磁盘IO，ioutil持续100%
二、分析 为什么pvlost？
header中透传deadline，判断time.Now() &amp;gt; deadline后，直接丢弃请求 框架在收到请求时立即打印一条请求日志 日志库实现类似go标准库的log，使用sync.Mutex保护buf，打印日志被串行化 磁盘IO打满时，log.Info(&amp;ldquo;msg&amp;rdquo;)在Write时可能会阻塞N秒，后续的请求也被阻塞，打印日志后进入 time.Now() &amp;gt; deadline 的判断时，已经超过了deadline，丢弃请求。对外表现为pvlost Write写文件会阻塞？
文件Write只需要写到内核的cache中，由操作系统负责flush，IO压力大时，cache不足，打印日志会阻塞 线上遇到的case，一般阻塞3秒以下 本次问题是物理机上部署的其他实例大量占用IO 写日志阻塞有以下解决方式
硬件层面 更换SSD：SSD有着更高的读写性能、更高的IOPS 独立部署：不与其他占用IO大的实例混部 服务层面 打印log不立即Write，将日志写到ctx上下文中（例如ctx提供Info、Error日志方法），回包后再Write，此时阻塞不影响请求 异步日志：日志写到进程缓冲区，异步Write 其实，Write行为可以看作是异步的，内核有page cache，并且会定期flush
但是内核cache不足时，也是会阻塞
因此，异步日志本质是在进程内增加cache，不依赖内核的cache
PS：起初并未怀疑是磁盘IO的问题，物理机有多磁盘，监控上需要选中对应磁盘才能看到相应监控
定位问题时最直接的表现是日志中时间乱序，故障时间前后几秒，日志中的时间是乱的，时间变化不合规律
正常的日志是下面这样，时间递增
2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:05.100 msg... 2024-01-01 12:34:05.200 msg... 故障时，时间从5s到6s到7s，又会回到5s
2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:06.100 msg... 2024-01-01 12:34:07.200 msg... 2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:06.100 msg... 2024-01-01 12:34:05.000 msg... 有怀疑是时钟波动的问题，但是时钟波动概率还是很小，多次出现故障，并且时钟短时间频繁波动，排除
三、实现异步日志 几种解决方式中，异步日志可行性最高。
当前日志库实现整体逻辑类似标准库log，额外增加了日志滚动功能，增加Info、Error等日志等级
// 标准库log // A Logger represents an active logging object that generates lines of // output to an io.</description>
      <content>&lt;h1 id=&#34;一问题背景&#34;&gt;一、问题背景&lt;/h1&gt;
&lt;p&gt;线上某接口偶现pvlost，问题实例物理机所在磁盘都有大量&lt;strong&gt;磁盘IO&lt;/strong&gt;，ioutil持续100%&lt;/p&gt;
&lt;h1 id=&#34;二分析&#34;&gt;二、分析&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;为什么pvlost？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;header中透传deadline，判断time.Now() &amp;gt; deadline后，直接丢弃请求&lt;/li&gt;
&lt;li&gt;框架在收到请求时立即打印一条请求日志&lt;/li&gt;
&lt;li&gt;日志库实现类似go标准库的log，使用sync.Mutex保护buf，打印日志被&lt;strong&gt;串行化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;磁盘IO打满时，log.Info(&amp;ldquo;msg&amp;rdquo;)在Write时可能会&lt;strong&gt;阻塞N秒&lt;/strong&gt;，后续的请求也被阻塞，打印日志后进入 time.Now() &amp;gt; deadline 的判断时，已经超过了deadline，丢弃请求。对外表现为pvlost&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write写文件会阻塞？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文件Write只需要写到内核的cache中，由操作系统负责flush，IO压力大时，cache不足，打印日志会阻塞&lt;/li&gt;
&lt;li&gt;线上遇到的case，一般&lt;strong&gt;阻塞3秒以下&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;本次问题是物理机上部署的其他实例大量占用IO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写日志阻塞有以下解决方式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;硬件层面
&lt;ol&gt;
&lt;li&gt;更换SSD：SSD有着更高的读写性能、更高的IOPS&lt;/li&gt;
&lt;li&gt;独立部署：不与其他占用IO大的实例混部&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;服务层面
&lt;ol&gt;
&lt;li&gt;打印log不立即Write，将日志写到ctx上下文中（例如ctx提供Info、Error日志方法），回包后再Write，此时阻塞不影响请求&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;异步日志&lt;/strong&gt;：日志写到进程缓冲区，异步Write&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其实，Write行为可以看作是异步的，内核有page cache，并且会定期flush&lt;/p&gt;
&lt;p&gt;但是内核cache不足时，也是会阻塞&lt;/p&gt;
&lt;p&gt;因此，异步日志本质是在进程内增加cache，不依赖内核的cache&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;PS：起初并未怀疑是磁盘IO的问题，物理机有多磁盘，监控上需要选中对应磁盘才能看到相应监控&lt;/p&gt;
&lt;p&gt;定位问题时最直接的表现是日志中时间乱序，故障时间前后几秒，日志中的时间是乱的，时间变化不合规律&lt;/p&gt;
&lt;p&gt;正常的日志是下面这样，时间递增&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:05.100 msg...
2024-01-01 12:34:05.200 msg...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;故障时，时间从5s到6s到7s，又会回到5s&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:06.100 msg...
2024-01-01 12:34:07.200 msg...
2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:06.100 msg...
2024-01-01 12:34:05.000 msg...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有怀疑是时钟波动的问题，但是时钟波动概率还是很小，多次出现故障，并且时钟短时间频繁波动，排除&lt;/p&gt;
&lt;h1 id=&#34;三实现异步日志&#34;&gt;三、实现异步日志&lt;/h1&gt;
&lt;p&gt;几种解决方式中，异步日志可行性最高。&lt;/p&gt;
&lt;p&gt;当前日志库实现整体逻辑类似标准库log，额外增加了&lt;strong&gt;日志滚动&lt;/strong&gt;功能，增加Info、Error等日志等级&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 标准库log
// A Logger represents an active logging object that generates lines of
// output to an io.Writer. Each logging operation makes a single call to
// the Writer&amp;#39;s Write method. A Logger can be used simultaneously from
// multiple goroutines; it guarantees to serialize access to the Writer.
type Logger struct {
	mu        sync.Mutex // ensures atomic writes; protects the following fields
	prefix    string     // prefix on each line to identify the logger (but see Lmsgprefix)
	flag      int        // properties
	out       io.Writer  // destination for output
	buf       []byte     // for accumulating text to write
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;二者实现上对比&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标准库log&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out为io.Writer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们的日志库&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out为*os.File，使用到Write写日志、Close方法关闭日志。打开新文件，滚动到新文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然只用到Write、Close，那么可以将out定义为io.WriteCloser&lt;/p&gt;
&lt;p&gt;提供fileWrapper方法， 将*os.File转为io.WriteCloser&lt;/p&gt;
&lt;p&gt;未开启异步log时，out = file，开启异步log时，将file包装为异步WriteCloser&lt;/p&gt;
&lt;h3 id=&#34;31-简单实现利用chan-byte&#34;&gt;3.1 简单实现：利用chan []byte&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
type AsyncWriteCloser struct {
	*os.File
	ch    chan []byte
	close chan error
}

func NewAsyncWriteCloser(f *os.File) *AsyncWriteCloser {
	res := &amp;amp;AsyncWriteCloser{
		File:  f,
		ch:    make(chan []byte, 1024),
		close: make(chan error),
	}
	go res.consume()
	return res
}

func (wc *AsyncWriteCloser) consume() {
	for b := range wc.ch {
		wc.File.Write(b)
	}
	wc.close &amp;lt;- wc.File.Close()
}

func (wc *AsyncWriteCloser) Write(b []byte) (int, error) {
	copy := append([]byte(nil), b...)
	wc.ch &amp;lt;- copy
	return len(b), nil
}

func (wc *AsyncWriteCloser) Close(b []byte) error {
	close(wc.ch)
	return &amp;lt;-wc.close
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意：Write参数b要进行&lt;strong&gt;深拷贝&lt;/strong&gt;。函数返回后，调用函数可以修改b的内容&lt;/p&gt;
&lt;p&gt;这个实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write次数不变，Write在chan未满时立即返回，阻塞只会在consume中&lt;/li&gt;
&lt;li&gt;多了一次内存拷贝， Write方法中的拷贝&lt;/li&gt;
&lt;li&gt;[]byte对象多&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-优化实现积攒数据每秒写一次&#34;&gt;3.2 优化实现：积攒数据，每秒写一次&lt;/h3&gt;
&lt;p&gt;先写到cur []byte, 写满cap后再写入chan，减少Write次数，避免产生大量[]byte对象&lt;/p&gt;
&lt;h4 id=&#34;write逻辑&#34;&gt;Write逻辑&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type AsyncWriteCloser2 struct {
	*os.File
	frozen chan []byte

	mu  sync.Mutex
	cur []byte

	closed  bool

	wg sync.WaitGroup
}

func (wc *AsyncWriteCloser2) Write(b []byte) (int, error) {
	wc.mu.Lock()
	defer wc.mu.Unlock()

	if wc.closed {
		return 0, errors.New(&amp;#34;write into closed file&amp;#34;)
	}

	l := len(b)
	if l+len(wc.cur) &amp;lt;= cap(wc.cur) {
		wc.cur = append(wc.cur, b...)
		return l, nil
	}

	if len(wc.cur) &amp;gt; 0 {
		// 阻塞send
		wc.frozen &amp;lt;- wc.cur
		wc.cur = nil // TODO 池化
	}

	wc.cur = append(wc.cur, b...)
	return l, nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外， 当日志量过少时，写满cap需要一定时间，日志更新慢，需要增加&lt;strong&gt;每秒写一次&lt;/strong&gt;的逻辑&lt;/p&gt;
&lt;h4 id=&#34;消费逻辑&#34;&gt;消费逻辑&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (wc *AsyncWriteCloser2) consume() {
	defer wc.wg.Done()

	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()

	var exit bool
	for !exit {
		select {
		case b, ok := &amp;lt;-wc.frozen:
			if !ok { // closed
				exit = true
				break
			}
			wc.File.Write(b)

		case &amp;lt;-ticker.C:
			if len(wc.frozen) &amp;gt; 0 {
				continue
			}

			wc.mu.Lock()
			// 此时 frozen可能满了
			if len(wc.cur) &amp;gt; 0 {
				select {
				case wc.frozen &amp;lt;- wc.cur: // 要写到frozen 顺序消费
					wc.cur = nil
				default:
					// 已经满了 or closed
				}
			}
			wc.mu.Unlock()
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;close等待数据写完&#34;&gt;Close等待数据写完&lt;/h4&gt;
&lt;p&gt;Close时，需要将已有数据写完，因此，在Close方法等待consume完成后调用文件Close&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (wc *AsyncWriteCloser2) waitLocked() {
	// cur还有数据， 写到frozen
	if len(wc.cur) &amp;gt; 0 {
		wc.frozen &amp;lt;- wc.cur
		wc.cur = nil
	}

	close(wc.frozen)
	wc.wg.Wait()
}

func (wc *AsyncWriteCloser2) Close(b []byte) error {
	wc.mu.Lock()
	defer wc.mu.Unlock()

	if wc.closed {
		return errors.New(&amp;#34;closed&amp;#34;)
	}
	wc.closed = true

	wc.waitLocked()

	return wc.File.Close()
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;乍一看没有问题，但是，这样可能会&lt;strong&gt;死锁&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;waitLocked时持有mutex，当前chan已满，此时写入cur阻塞，等待consume消费fronze&lt;/p&gt;
&lt;p&gt;如果consume的select触发ticker分支，ticker会加锁，保护cur，不会消费frozen，也就是waitLocked中无法写入chan&lt;/p&gt;
&lt;p&gt;二者互相等待，死锁&lt;/p&gt;
&lt;p&gt;问题发生在哪？cur写入chan阻塞？写cur移到File.Close前也会死锁&lt;/p&gt;
&lt;p&gt;问题在于sync.Mutex不可重入， Close等待consume退出，consume内会尝试获取Close持有的锁&lt;/p&gt;
&lt;p&gt;解决方式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;consume内使用TryLock，go1.18及以上可使用&lt;/li&gt;
&lt;li&gt;waitLocked写入chan前，先Unlock
&lt;ol&gt;
&lt;li&gt;这个场景可以解决问题，但是不建议&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;consume中select有两个分支，拆成&lt;strong&gt;2个goroutine&lt;/strong&gt;，一个负责消费frozen，一个负责ticker加锁将cur写入frozen，Close还是等待consume退出，consume不加锁，不会有问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;拆成2个goroutine，这种方法通用性很强&lt;/p&gt;
&lt;h3 id=&#34;33-避免重启丢日志&#34;&gt;3.3 避免重启丢日志&lt;/h3&gt;
&lt;p&gt;异步io.WriteCloser只要满足在Close时写完数据即可。&lt;/p&gt;
&lt;p&gt;这样，服务重启时，就不可避免丢数据&lt;/p&gt;
&lt;p&gt;一种选择提供Flush方法，收到信号Flush，但Flush后还会有日志，不能完全避免日志不丢失&lt;/p&gt;
&lt;p&gt;为避免日志丢失，可以提供一个Stop方法（并不只是实现io.WriteCloser了），停止异步写，转变为同步写文件&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;05-21补充&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上面说的关闭异步log的方法，是需要log库与自定义的异步io.WriteCloser相配合，二者有耦合&lt;/p&gt;
&lt;p&gt;思考了下，应该在log内实现Stop方法，加锁，调用异步io.WriteCloser的Flush方法，此时可以将log的out切换为原本的file。 解除了二者的耦合！&lt;/p&gt;
&lt;h3 id=&#34;34-还能做些什么&#34;&gt;3.4 还能做些什么&lt;/h3&gt;
&lt;p&gt;增加统计信息，例如Write最大耗时，chan最大长度等&lt;/p&gt;
&lt;h3 id=&#34;35-过程中的一些问题&#34;&gt;3.5 过程中的一些问题&lt;/h3&gt;
&lt;p&gt;起初尝试用context通知consume退出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;consume的select中可以增加ctx.Done，用来做退出通知
&lt;ul&gt;
&lt;li&gt;select的多个case同时触发时，会随机选择一个&lt;/li&gt;
&lt;li&gt;在退出后还要再消费完chan，代码偏复杂&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;close(chan) 或者 写入nil 标识关闭，这样退出for循环后一定已经消费完chan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;标准库log实现&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go1.18&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
// Output writes the output for a logging event. The string s contains
// the text to print after the prefix specified by the flags of the
// Logger. A newline is appended if the last character of s is not
// already a newline. Calldepth is used to recover the PC and is
// provided for generality, although at the moment on all pre-defined
// paths it will be 2.
func (l *Logger) Output(calldepth int, s string) error {
	now := time.Now() // get this early.
	var file string
	var line int
	l.mu.Lock()
	defer l.mu.Unlock()
	if l.flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		// Release lock while getting caller info - it&amp;#39;s expensive.
		l.mu.Unlock()
		var ok bool
		_, file, line, ok = runtime.Caller(calldepth)
		if !ok {
			file = &amp;#34;???&amp;#34;
			line = 0
		}
		l.mu.Lock()
	}
	l.buf = l.buf[:0]
	l.formatHeader(&amp;amp;l.buf, now, file, line)
	l.buf = append(l.buf, s...)
	if len(s) == 0 || s[len(s)-1] != &amp;#39;\n&amp;#39; {
		l.buf = append(l.buf, &amp;#39;\n&amp;#39;)
	}
	_, err := l.out.Write(l.buf)
	return err
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;mutex临界区较大&lt;/p&gt;
&lt;p&gt;注释写到 Release lock while getting caller info - it&amp;rsquo;s expensive.&lt;/p&gt;
&lt;p&gt;测试了下runtime.Caller的耗时&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func main() {
	for i := 0; i &amp;lt; 10; i++ {
		start := time.Now()
		runtime.Caller(2)
		fmt.Println(&amp;#34;cost&amp;#34;, time.Since(start))
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Caller第一次调用耗时15us，后续在500ns-1us左右。 开销确实大。&lt;/p&gt;
&lt;p&gt;同样功能C++可以用宏定义在编译期得到结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go1.21&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;mutex加锁范围变化，临界区变小&lt;/p&gt;
&lt;p&gt;增加了[]byte池，不再是每个Logger对象使用一个buf []byte&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (l *Logger) output(pc uintptr, calldepth int, appendOutput func([]byte) []byte) error {
	now := time.Now() // get this early.

	// Load prefix and flag once so that their value is consistent within
	// this call regardless of any concurrent changes to their value.
	prefix := l.Prefix()
	flag := l.Flags()

	var file string
	var line int
	if flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		if pc == 0 {
			var ok bool
			_, file, line, ok = runtime.Caller(calldepth)
			if !ok {
				file = &amp;#34;???&amp;#34;
				line = 0
			}
		}
	}

	buf := getBuffer()
	defer putBuffer(buf)
	formatHeader(buf, now, prefix, flag, file, line)
	*buf = appendOutput(*buf)
	if len(*buf) == 0 || (*buf)[len(*buf)-1] != &amp;#39;\n&amp;#39; {
		*buf = append(*buf, &amp;#39;\n&amp;#39;)
	}

	l.outMu.Lock()
	defer l.outMu.Unlock()
	_, err := l.out.Write(*buf)
	return err
}

var bufferPool = sync.Pool{New: func() any { return new([]byte) }}

func getBuffer() *[]byte {
	p := bufferPool.Get().(*[]byte)
	*p = (*p)[:0]
	return p
}
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;四总结&#34;&gt;四、总结&lt;/h1&gt;
&lt;p&gt;使用异步io.WriteCloser，避免打印日志时发生阻塞，避免了接口请求失败&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;遗留问题：sync.Mutex&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;发生问题时日志可以看出，mutex的等待队列不是先进先出，有着一些随机性&lt;/p&gt;
&lt;p&gt;sync.Mutex有饥饿模式 ，饥饿模式下也不是先进先出吗？&lt;/p&gt;
&lt;p&gt;怎样复现日志乱序的现象？&lt;/p&gt;
&lt;p&gt;后续有时间再写篇博客分析&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
