<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on jiayuan&#39;s Blog</title>
    <link>/posts/</link>
    <description>Recent content in Posts on jiayuan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 22:00:00 +0800</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于第三方异步通知 &amp; 超时控制</title>
      <link>/posts/developer-notification/</link>
      <pubDate>Tue, 18 Jun 2024 22:00:00 +0800</pubDate>
      
      <guid>/posts/developer-notification/</guid>
      <description>在接入某支付方时， 对接了实时通知。用户支付后，支付方会实时推送支付结果
如果收到通知后，业务处理失败，应如何返回响应？
我们的系统直接返回失败，有如下考虑：
异步通知处理失败，返回失败响应，第三方会逐渐增加间隔进行重试，到达最大重试次数后放弃 自身系统此时可能确有故障，恢复后第三方重试即可成功 用户侧支付成功后会主动上报支付结果 离线worker会定期轮询未支付订单，主动查询订单结果，在上述2种方式均失效时做兜底 T+１的对账再一次监控是否有掉单 以上4种方式，最大程度避免了用户支付后，权益未到账
因此，系统不强依赖第三方的异步通知，可以直接返回失败
最近遇到一个场景，靠第三方的重试是没用的
通知的数据有时效性，当我们返回失败后，5分钟后第三方再次发起通知时，参数已过期
收到第三方通知 业务逻辑处理耗时较长，无法优化 第三方超时断开了连接，conext被cancel，返回失败响应 同时，业务逻辑中断，没有继续进行 对于这个场景，不应该使用第三方请求的context，应该使用新ccontext，自主管理超时时间
但是，这样仍然会超时，返回失败，第三方再次重试时参数已过期，怎么办?
在接受到通知时，记录下通知详情后，即可返回第三方成功响应
然后执行业务逻辑，处理失败内部可按一定策略重试，避免参数过期</description>
      <content>&lt;p&gt;在接入某支付方时， 对接了实时通知。用户支付后，支付方会实时推送支付结果&lt;/p&gt;
&lt;p&gt;如果收到通知后，业务处理失败，应如何返回响应？&lt;/p&gt;
&lt;p&gt;我们的系统直接返回失败，有如下考虑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;异步通知处理失败，返回失败响应，第三方会逐渐增加间隔进行重试，到达最大重试次数后放弃
&lt;ol&gt;
&lt;li&gt;自身系统此时可能确有故障，恢复后第三方重试即可成功&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;用户侧支付成功后会主动上报支付结果&lt;/li&gt;
&lt;li&gt;离线worker会定期轮询未支付订单，主动查询订单结果，在上述2种方式均失效时做兜底&lt;/li&gt;
&lt;li&gt;T+１的对账再一次监控是否有掉单&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上4种方式，最大程度避免了用户支付后，权益未到账&lt;/p&gt;
&lt;p&gt;因此，系统不强依赖第三方的异步通知，可以直接返回失败&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;最近遇到一个场景，靠第三方的重试是没用的&lt;/p&gt;
&lt;p&gt;通知的数据有&lt;strong&gt;时效性&lt;/strong&gt;，当我们返回失败后，5分钟后第三方再次发起通知时，参数&lt;strong&gt;已过期&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收到第三方通知&lt;/li&gt;
&lt;li&gt;业务逻辑处理耗时较长，无法优化&lt;/li&gt;
&lt;li&gt;第三方超时断开了连接，conext被cancel，返回失败响应&lt;/li&gt;
&lt;li&gt;同时，业务逻辑中断，&lt;strong&gt;没有继续进行&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于这个场景，不应该使用第三方请求的context，应该使用新ccontext，自主管理超时时间&lt;/p&gt;
&lt;p&gt;但是，这样仍然会超时，返回失败，第三方再次重试时参数已过期，怎么办?&lt;/p&gt;
&lt;p&gt;在接受到通知时，记录下通知详情后，即可返回第三方成功响应&lt;/p&gt;
&lt;p&gt;然后执行业务逻辑，处理失败内部可按一定策略重试，避免参数过期&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Go sync包分析</title>
      <link>/posts/golang-sync-package/</link>
      <pubDate>Thu, 13 Jun 2024 23:00:00 +0800</pubDate>
      
      <guid>/posts/golang-sync-package/</guid>
      <description>sync.RWMutex 读者写者是经典的进程同步问题
按照实现的优先级可以分为读者优先、读写公平、写者优先
读者优先 如下伪码实现
type RWMutex struct { rmutex sync.Mutex wmutex sync.Mutex readerCnt int } func RLock() { rmutex.Lock r := ++readerCnt if r == 1 {wmutex.Lock} // 应该在rmutex内调用 rmutex.Unlock } func RUnlock() { rmutex.Lock r := --readerCnt if r == 0 {wmutex.Unlock} // 应该在rmutex内调用 rmutex.Unlock } func Lock() { wmutex.Lock() } func Unlock() { wmutex.Unlock() } 注意，RLock中 if r == 1 RUnlock中if r == 0 的逻辑要在持有rmutex时进行
如果RLock中 if r==1在rmutex.</description>
      <content>&lt;h1 id=&#34;syncrwmutex&#34;&gt;sync.RWMutex&lt;/h1&gt;
&lt;p&gt;读者写者是经典的进程同步问题&lt;/p&gt;
&lt;p&gt;按照实现的优先级可以分为读者优先、读写公平、写者优先&lt;/p&gt;
&lt;h3 id=&#34;读者优先&#34;&gt;读者优先&lt;/h3&gt;
&lt;p&gt;如下伪码实现&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type RWMutex struct {
	rmutex sync.Mutex
	wmutex sync.Mutex
	readerCnt int
}

func RLock() {
	rmutex.Lock
	r := ++readerCnt

	if r == 1  {wmutex.Lock} // 应该在rmutex内调用

	rmutex.Unlock
}

func RUnlock() {
	rmutex.Lock
	r := --readerCnt

	if r == 0 {wmutex.Unlock} // 应该在rmutex内调用

	rmutex.Unlock
}

func Lock() {
	wmutex.Lock()
}

func Unlock() {
	wmutex.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意，RLock中 if r == 1   RUnlock中if r == 0 的逻辑要在持有rmutex时进行&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果RLock中 if r==1在rmutex.Unlock之后&amp;hellip;&lt;/li&gt;
&lt;li&gt;如果RUnlock中if r==0在rmutex.Unlock之后&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rlock 先锁rmutex，读者数量自增， 如果是第一个读者，锁住wmutex&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果在RLock前有写者持有写锁，第一个读者阻塞在wmutex.Lock，后续的RLock请求会阻塞在rmutex.Lock&lt;/li&gt;
&lt;li&gt;如果第一个RLock时没有写者，第一个RLock加wmutex成功， 后续的写会等待，读者可以进入&lt;/li&gt;
&lt;li&gt;持续有读请求，写者可能会饥饿&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;写者优先&#34;&gt;写者优先&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type RWMutex struct {
	rcntmu sync.Mutex
	wcntmu sync.Mutex
	wmu sync.Mutex // 写者互斥
	mu sync.mutex  // 读者加锁时需要锁，第一个写进入时锁，写者最后一个退出时解锁
	readerCnt int
	writerCnt int
}

func RLock() {
	mu.Lock  // 第一步获取mu
	rcntmu.Lock
	r := ++readerCnt

	if r == 1  {wmutex.Lock} // 应该在rcntmu内调用

	rcntmu.Unlock

	mu.Unlock
}

func RUnlock() {
	rcntmu.Lock
	r := --readerCnt

	if r == 0 {wmutex.Unlock} // 应该在rcntmu内调用

	rcntmu.Unlock
}

func Lock() {
	wcntmu.Lock
	w := ++writerCnt
	if w == 1 { mu.Lock } // 第一个写者，Lock mu，阻塞后来的读
	wcntmu.Unlock

	wmu.Lock // 写互斥
}

func Unlock() {
	wcntmu.Lock
	w := --writerCnt
	if w == 0 { mu.UnLock }  // 最后一个写退出，解锁
	wcntmu.Unlock

	wmu.Unlock
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;读RLock时需要获取mu，&lt;/p&gt;
&lt;p&gt;写Lock时，第一个写者竞争mu，持有后等待最后一个写者Unlock时Unlock mu&lt;/p&gt;
&lt;p&gt;写者优先级高于读者&lt;/p&gt;
&lt;h2 id=&#34;读写公平&#34;&gt;读写公平&lt;/h2&gt;
&lt;p&gt;省略&lt;/p&gt;
&lt;h2 id=&#34;go实现&#34;&gt;Go实现&lt;/h2&gt;
&lt;p&gt;利用原子变量&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type RWMutex struct {
	w           Mutex        // held if there are pending writers
	writerSem   uint32       // semaphore for writers to wait for completing readers
	readerSem   uint32       // semaphore for readers to wait for completing writers
	readerCount atomic.Int32 // number of pending readers
	readerWait  atomic.Int32 // number of departing readers
}

const rwmutexMaxReaders = 1 &amp;lt;&amp;lt; 30

func (rw *RWMutex) RLock() {
	if rw.readerCount.Add(1) &amp;lt; 0 {
		// A writer is pending, wait for it.
		runtime_SemacquireRWMutexR(&amp;amp;rw.readerSem, false, 0)
	}
}

func (rw *RWMutex) RUnlock() {
	if r := rw.readerCount.Add(-1); r &amp;lt; 0 {
		// Outlined slow-path to allow the fast-path to be inlined
		rw.rUnlockSlow(r)
	}
}

func (rw *RWMutex) rUnlockSlow(r int32) {
	// A writer is pending.
	if rw.readerWait.Add(-1) == 0 {
		// The last reader unblocks the writer.
		runtime_Semrelease(&amp;amp;rw.writerSem, false, 1)
	}
}

func (rw *RWMutex) Lock() {
	// First, resolve competition with other writers.
	rw.w.Lock()
	// Announce to readers there is a pending writer.
	r := rw.readerCount.Add(-rwmutexMaxReaders) + rwmutexMaxReaders
	// Wait for active readers.
	if r != 0 &amp;amp;&amp;amp; rw.readerWait.Add(r) != 0 {
		runtime_SemacquireRWMutex(&amp;amp;rw.writerSem, false, 0)
	}
}

func (rw *RWMutex) Unlock() {
	// Announce to readers there is no active writer.
	r := rw.readerCount.Add(rwmutexMaxReaders)

	// Unblock blocked readers, if any.
	for i := 0; i &amp;lt; int(r); i++ {
		runtime_Semrelease(&amp;amp;rw.readerSem, false, 0)
	}
	// Allow other writers to proceed.
	rw.w.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;RLock增加readerCount，&amp;lt;0表示有写者申请写锁或持有写锁，等待readerSem&lt;/p&gt;
&lt;p&gt;RUnlock减少readerCount，&amp;lt;0表示有写者申请写锁或持有写锁，进入慢路径rUnlockSlow&lt;/p&gt;
&lt;p&gt;rUnlockSlow内减少readerWait，新值=0时，唤醒writerSem&lt;/p&gt;
&lt;p&gt;写者Lock时串行的，写者之间用rw.w互斥&lt;/p&gt;
&lt;p&gt;获取rw.w的写者将readerCount减小rwmutexMaxReaders(10.7亿)，并取到减小前的readerCount&lt;/p&gt;
&lt;p&gt;如果原来的readerCount&amp;gt;0，表示有读者已获取读锁，将readerWait增加读者数量，Add后不为0睡眠等待writerSem&lt;/p&gt;
&lt;p&gt;写者Lock与读者RUnlock有关联&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;写者加锁请求将readerCount减少10.7亿后，新来的读者会等待readerSem&lt;/li&gt;
&lt;li&gt;现有的读者会进入rUnlockSlow，减少readerWait，=0唤醒writerSem
&lt;ul&gt;
&lt;li&gt;一种情况是在写者readerWait之前，全部现有读者RUnlock完毕，写者Add后为0，不需要等待writerSem；同时读者rUnlockSlow也不需要唤醒writerSem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写者Unlock时（一定没有正在进行的读者），将readerCount增加10.7亿，得到等待的读者数量&lt;/p&gt;
&lt;p&gt;依次唤醒阻塞在readerSem上的读者&lt;/p&gt;
&lt;p&gt;Go的实现不是&amp;quot;写者优先&amp;quot;，一个写者持有锁，新的写者与其余读者同时等待时，新的写者并不是高优处理&lt;/p&gt;
&lt;h1 id=&#34;syncmutex&#34;&gt;sync.Mutex&lt;/h1&gt;
&lt;p&gt;异步Log文章中有简单分析&lt;/p&gt;
&lt;h1 id=&#34;synconce&#34;&gt;sync.Once&lt;/h1&gt;
&lt;p&gt;double check&lt;/p&gt;
&lt;h1 id=&#34;syncmap&#34;&gt;sync.Map&lt;/h1&gt;
&lt;h1 id=&#34;atomicvalue&#34;&gt;atomic.Value&lt;/h1&gt;
</content>
    </item>
    
    <item>
      <title>InnoDB加锁分析</title>
      <link>/posts/innodb-lock/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0800</pubDate>
      
      <guid>/posts/innodb-lock/</guid>
      <description>加锁规则 MySQL实战45讲总结的规则(5.x版本，规则可能会变化) 两个“原则”、两个“优化”和一个“bug”：
原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。
原则 2：查找过程中访问到的对象才会加锁。
优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。
优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
其他参考 不同语句加锁
MySQL8 performance_schema.data_locks;
InnoDB 等值查询加锁分析 仅分析等值查询，范围查询比较复杂
索引：唯一索引/非唯一索引 隔离级别：RR/RC 是否有符合条件的行：是/否 唯一索引 表结构
create table t ( id int not null, uid int not null, val int not null, primary key (id), unique key uk_uid (uid) ) engine = innodb; insert into t values (5, 5, 5), (10, 10, 10), (15, 15, 15), (20, 20, 20); RR隔离级别 命中行 begin; select * from t where uid = 5 for update;</description>
      <content>&lt;h1 id=&#34;加锁规则&#34;&gt;加锁规则&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MySQL实战45讲总结的规则(5.x版本，规则可能会变化)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两个“原则”、两个“优化”和一个“bug”：&lt;/p&gt;
&lt;p&gt;原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。&lt;/p&gt;
&lt;p&gt;原则 2：查找过程中访问到的对象才会加锁。&lt;/p&gt;
&lt;p&gt;优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。&lt;/p&gt;
&lt;p&gt;优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。&lt;/p&gt;
&lt;p&gt;一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其他参考&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/innodb-locks-set.html&#34;&gt;不同语句加锁&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MySQL8 performance_schema.data_locks;&lt;/p&gt;
&lt;h1 id=&#34;innodb-等值查询加锁分析&#34;&gt;InnoDB 等值查询加锁分析&lt;/h1&gt;
&lt;p&gt;仅分析等值查询，范围查询比较复杂&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引：唯一索引/非唯一索引&lt;/li&gt;
&lt;li&gt;隔离级别：RR/RC&lt;/li&gt;
&lt;li&gt;是否有符合条件的行：是/否&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;唯一索引&#34;&gt;唯一索引&lt;/h2&gt;
&lt;p&gt;表结构&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;create table t (
 id int not null,
 uid int not null,
 val int not null,
 primary key (id),
 unique key uk_uid (uid)
) engine = innodb;

insert into t values (5, 5, 5), (10, 10, 10), (15, 15, 15), (20, 20, 20);
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;rr隔离级别&#34;&gt;RR隔离级别&lt;/h3&gt;
&lt;h4 id=&#34;命中行&#34;&gt;命中行&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;begin; select * from t where uid = 5 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;仅对记录5加锁&lt;/p&gt;
&lt;h4 id=&#34;未命中行&#34;&gt;未命中行&lt;/h4&gt;
&lt;p&gt;session1 &lt;code&gt;begin; select * from t where uid = 13 for update;&lt;/code&gt;
锁(10, 15) 的gap，没有行锁&lt;/p&gt;
&lt;h3 id=&#34;rc隔离级别&#34;&gt;RC隔离级别&lt;/h3&gt;
&lt;h4 id=&#34;命中行-1&#34;&gt;命中行&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;begin; select * from t where uid = 5 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;仅对记录5加锁&lt;/p&gt;
&lt;h4 id=&#34;未命中行-1&#34;&gt;未命中行&lt;/h4&gt;
&lt;p&gt;session1 &lt;code&gt;begin; select * from t where uid = 13 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;session1 没有gap lock，没有行锁&lt;/p&gt;
&lt;p&gt;session2 可以插入任意间隙，可以更新已有记录&lt;/p&gt;
&lt;h2 id=&#34;非唯一索引&#34;&gt;非唯一索引&lt;/h2&gt;
&lt;p&gt;表结构&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;create table t (
 id int not null,
 uid int not null,
 val int not null,
 primary key (id),
 key idx_uid (uid)
) engine = innodb;

insert into t values (5, 5, 5), (10, 10, 10), (15, 15, 15), (20, 20, 20), (24, 20, 20), (28, 20, 20), (40, 40, 40);
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;rr隔离级别-1&#34;&gt;RR隔离级别&lt;/h3&gt;
&lt;h4 id=&#34;命中行-2&#34;&gt;命中行&lt;/h4&gt;
&lt;p&gt;session 1 &lt;code&gt;begin; select * from t where uid = 20 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;锁(20, 40) 的gap&lt;/p&gt;
&lt;p&gt;3个uid=20的行锁&lt;/p&gt;
&lt;h4 id=&#34;未命中行-2&#34;&gt;未命中行&lt;/h4&gt;
&lt;p&gt;session 1 &lt;code&gt;begin; select * from t where uid = 18 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;锁(15, 20(id=20))的 gap，可以插入(21, 20, 20)，不能插入(19, 20, 20)，注意非唯一索引，同一个索引值之间的间隙与主键id相关&lt;/p&gt;
&lt;p&gt;uid=20(id=20)不需要加行锁&lt;/p&gt;
&lt;h3 id=&#34;rc隔离级别-1&#34;&gt;RC隔离级别&lt;/h3&gt;
&lt;h4 id=&#34;命中行-3&#34;&gt;命中行&lt;/h4&gt;
&lt;p&gt;session 1 &lt;code&gt;begin; select * from t where uid = 20 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;只加uid=20行锁&lt;/p&gt;
&lt;h4 id=&#34;未命中行-3&#34;&gt;未命中行&lt;/h4&gt;
&lt;p&gt;session 1 &lt;code&gt;begin; select * from t where uid = 18 for update;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;不加锁&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;RR级别需要加gap lock，gap lock加record lock称作next-key lock&lt;/p&gt;
&lt;p&gt;RC不需要加gap lock&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RR隔离级别（等值查询）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;唯一索引
&lt;ul&gt;
&lt;li&gt;命中时，锁命中的一条记录&lt;/li&gt;
&lt;li&gt;未命中，锁间隙，下一条记录无需加行锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非唯一索引
&lt;ul&gt;
&lt;li&gt;命中时，锁间隙，锁符合条件的记录行锁&lt;/li&gt;
&lt;li&gt;未命中时，锁间隙，下一条无需加行锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RC隔离级别（等值查询）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;唯一索引
&lt;ul&gt;
&lt;li&gt;命中时，锁命中的一条记录&lt;/li&gt;
&lt;li&gt;未命中时，无需加锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非唯一索引
&lt;ul&gt;
&lt;li&gt;命中时，锁命中的记录&lt;/li&gt;
&lt;li&gt;未命中时，无需加锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;如何验证锁相关问题&#34;&gt;如何验证锁相关问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;InnoDB update未命中索引时，RR级别是否锁全表？
&lt;ul&gt;
&lt;li&gt;实际上是扫描主键，逐行加锁&lt;/li&gt;
&lt;li&gt;验证：session 1，先锁住最后一个主键，session 2 进行更新，再对最后一个主键加锁时，需要等待，session 1更新第一个主键，会报死锁错误
&lt;ul&gt;
&lt;li&gt;说明未命中索引的更新并不是先加表锁，而是先加了行锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RC级别，会提前释放不满足条件的行，是在语句执行完释放，还是在判断不满足条件后理解释放？
&lt;ul&gt;
&lt;li&gt;验证：同样可以session1锁住最后一行，session 2进行更新，where条件有不符合条件的，当阻塞在最后一行时，session 3更新被session 2释放的行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;怎么用锁实现各隔离级别&#34;&gt;怎么用锁实现各隔离级别&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;读未提交
&lt;ol&gt;
&lt;li&gt;读不加锁，写加写锁
&lt;ol&gt;
&lt;li&gt;读不加锁，但是也能保证数据的正确。例如varchar(20)，不加锁也不会读错&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;读已提交
&lt;ol&gt;
&lt;li&gt;读加读锁，读完释放，写加写锁&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;可重复读
&lt;ol&gt;
&lt;li&gt;读加读锁，事务提交释放，写加写锁&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;串行化
&lt;ol&gt;
&lt;li&gt;读写都加锁&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;举例，有并发事务t1, t2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;读未提交，读不加锁，t2更新了一行， 还未提交，t1就可以读到，t2回滚，t1脏读&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读已提交，读加读锁，t2更新一行，未提交时，t2持有写锁，t1读不到，t2提交后，t1加上读锁，可以读到&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可重复读，t1读锁提交时释放，期间t2无法修改t1读到的记录，自然实现可重复读，但是仍然会有幻读，t2可以插入新记录&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;串行化，都加锁，不会出现幻读&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mvcc实现&#34;&gt;MVCC实现&lt;/h2&gt;
&lt;p&gt;只用锁也能实现各个隔离级别，但是并发较低&lt;/p&gt;
&lt;p&gt;写加写锁，大部分无法进行读&lt;/p&gt;
&lt;p&gt;MVCC支持写时读快照版本，增加并发&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>在线数据迁移</title>
      <link>/posts/online-data-migrate/</link>
      <pubDate>Tue, 11 Jun 2024 08:00:00 +0800</pubDate>
      
      <guid>/posts/online-data-migrate/</guid>
      <description>在线数据迁移 很多场景都有数据迁移的需求：
单表改分表 增加分表数，分32张表，扩到128张表 原来使用数据库A，后续迁移到数据库B 系统进行重构，新老数据库字段变化 如果允许停机迁移，数据的准确性一致性容易得到保证
但是大部分场景不允许停机迁移，系统中断几分钟，就会带来不少损失。
大部分应用选择不停机迁移，如何保证数据一致性是个问题
双写 打开双写，读老库 迁移存量数据 一致性校验 双写，灰度读新库 双写，全量读新库 下线双写、移除迁移代码 打开双写，目的是新修改数据新老数据库达到一致
迁移存量数据后，新老数据库完全一致
然后维持双写，灰度读新库持续观察，没有问题后读写新库
有问题可以随时切回读写老数据库
实际实施起来，比较复杂
如何双写 先写新库还是先写老库？ update时，如果新数据库没有数据，affect rows不一致，是否需要先从老数据库迁移 老数据库用事务更新多张表，新库是否需要使用事务 双写失败如何处理，接口返回成功还是失败 双insert，失败可能是超时导致，实际已插入，即使超时时增加反查，反查本身仍然可能超时 接口逻辑需多次update，前2个update更新成功，最后一个update新库失败，产生不一致 时序问题 delete与迁移的时序：存量迁移逻辑捞出数据A，准备迁移到新库，此时delete数据A，新老库双delete。数据A执行迁移，新库多数据 异步写新库的时序：如果异步写新库，多个更新的时序会错乱 云控开关时序：期望打开开关的一瞬间，所有实例都感知到变化。但是实际上还会有已经在处理中的请求。因此，处理存量数据时，一般会选择比打开双写早一点的时间，多处理一些数据 怎么保证数据一致 双写无法保证同时成功，有时间差 数据校验 记录update_time，做增量校验 update时，同时更新行的hash值，校验时可以select sum(hash) where xx（适用于结构不变的迁移） 切换期间也可能会引起不一致 一种可行的方式是切换前停止几秒的写请求 存储自身的数据迁移 双写方案在一致性上并不是很完美，可以先看看存储自身是如何处理的，能够提供参考
MySQL 全量同步 可以使用mysqldump导出，导出时记录binlog位置 增量同步 应用binlog 在线结构变更 MySQL早期版本DDL锁表，对应用影响较大，有pt-osc，gh-ost方案可以选择
pt-osc 使用触发器处理新增的修改 原表的insert，在新表replace into 原表的delete，在新表delete ignore 原本的update，在新表replace into 批量导入存量数据 使用insert ignore，避免已经被触发器插入 insert时需要锁，避免delete引起的时序问题 select读到数据A，请求delete A，触发器在新库空操作，然后insert到新库 实际上pt-osc使用insert ignore select xxx from old_table lock in share mode insert select 时持有锁，不会发生这种时序问题 gh-ost 应用binlog处理新增的修改 insert，转为replace into delete，转为delete update，不变 未导入时，update空操作，后续会将新数据导入 导入存量数据，与pt-osc类似 insert ignore select xxx from old_table lock in share mode Redis Cluster Redis对一致性要求并不高，方案可以作为参考</description>
      <content>&lt;h1 id=&#34;在线数据迁移&#34;&gt;在线数据迁移&lt;/h1&gt;
&lt;p&gt;很多场景都有数据迁移的需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单表改分表&lt;/li&gt;
&lt;li&gt;增加分表数，分32张表，扩到128张表&lt;/li&gt;
&lt;li&gt;原来使用数据库A，后续迁移到数据库B&lt;/li&gt;
&lt;li&gt;系统进行重构，新老数据库字段变化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果允许停机迁移，数据的准确性一致性容易得到保证&lt;/p&gt;
&lt;p&gt;但是大部分场景不允许停机迁移，系统中断几分钟，就会带来不少损失。&lt;/p&gt;
&lt;p&gt;大部分应用选择不停机迁移，如何保证数据一致性是个问题&lt;/p&gt;
&lt;h1 id=&#34;双写&#34;&gt;双写&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;打开双写，读老库&lt;/li&gt;
&lt;li&gt;迁移存量数据&lt;/li&gt;
&lt;li&gt;一致性校验&lt;/li&gt;
&lt;li&gt;双写，灰度读新库&lt;/li&gt;
&lt;li&gt;双写，全量读新库&lt;/li&gt;
&lt;li&gt;下线双写、移除迁移代码&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;打开双写，目的是新修改数据新老数据库达到一致&lt;/p&gt;
&lt;p&gt;迁移存量数据后，新老数据库完全一致&lt;/p&gt;
&lt;p&gt;然后维持双写，灰度读新库持续观察，没有问题后读写新库&lt;/p&gt;
&lt;p&gt;有问题可以随时切回读写老数据库&lt;/p&gt;
&lt;p&gt;实际实施起来，比较复杂&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何双写&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;先写新库还是先写老库？&lt;/li&gt;
&lt;li&gt;update时，如果新数据库没有数据，affect rows不一致，是否需要先从老数据库迁移&lt;/li&gt;
&lt;li&gt;老数据库用事务更新多张表，新库是否需要使用事务&lt;/li&gt;
&lt;li&gt;双写失败如何处理，接口返回成功还是失败
&lt;ul&gt;
&lt;li&gt;双insert，失败可能是超时导致，实际已插入，即使超时时增加反查，反查本身仍然可能超时&lt;/li&gt;
&lt;li&gt;接口逻辑需多次update，前2个update更新成功，最后一个update新库失败，产生不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;时序问题&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;delete与迁移的时序：存量迁移逻辑捞出数据A，准备迁移到新库，此时delete数据A，新老库双delete。数据A执行迁移，新库多数据&lt;/li&gt;
&lt;li&gt;异步写新库的时序：如果异步写新库，多个更新的时序会错乱&lt;/li&gt;
&lt;li&gt;云控开关时序：期望打开开关的一瞬间，所有实例都感知到变化。但是实际上还会有已经在处理中的请求。因此，处理存量数据时，一般会选择比打开双写早一点的时间，多处理一些数据&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;怎么保证数据一致&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;双写无法保证同时成功，有时间差&lt;/li&gt;
&lt;li&gt;数据校验
&lt;ul&gt;
&lt;li&gt;记录update_time，做增量校验&lt;/li&gt;
&lt;li&gt;update时，同时更新行的hash值，校验时可以select sum(hash) where xx（适用于结构不变的迁移）&lt;/li&gt;
&lt;li&gt;切换期间也可能会引起不一致
&lt;ul&gt;
&lt;li&gt;一种可行的方式是切换前停止几秒的写请求&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;存储自身的数据迁移&#34;&gt;存储自身的数据迁移&lt;/h1&gt;
&lt;p&gt;双写方案在一致性上并不是很完美，可以先看看存储自身是如何处理的，能够提供参考&lt;/p&gt;
&lt;h2 id=&#34;mysql&#34;&gt;MySQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;全量同步
&lt;ul&gt;
&lt;li&gt;可以使用mysqldump导出，导出时记录binlog位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;增量同步
&lt;ul&gt;
&lt;li&gt;应用binlog&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;在线结构变更&#34;&gt;在线结构变更&lt;/h3&gt;
&lt;p&gt;MySQL早期版本DDL锁表，对应用影响较大，有pt-osc，gh-ost方案可以选择&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pt-osc
&lt;ul&gt;
&lt;li&gt;使用触发器处理新增的修改
&lt;ul&gt;
&lt;li&gt;原表的insert，在新表replace into&lt;/li&gt;
&lt;li&gt;原表的delete，在新表delete ignore&lt;/li&gt;
&lt;li&gt;原本的update，在新表replace into&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;批量导入存量数据
&lt;ul&gt;
&lt;li&gt;使用insert ignore，避免已经被触发器插入&lt;/li&gt;
&lt;li&gt;insert时需要锁，避免delete引起的时序问题
&lt;ul&gt;
&lt;li&gt;select读到数据A，请求delete A，触发器在新库空操作，然后insert到新库&lt;/li&gt;
&lt;li&gt;实际上pt-osc使用insert ignore select  xxx from old_table lock in share mode
&lt;ul&gt;
&lt;li&gt;insert  select 时持有锁，不会发生这种时序问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gh-ost
&lt;ul&gt;
&lt;li&gt;应用binlog处理新增的修改
&lt;ul&gt;
&lt;li&gt;insert，转为replace into&lt;/li&gt;
&lt;li&gt;delete，转为delete&lt;/li&gt;
&lt;li&gt;update，不变
&lt;ul&gt;
&lt;li&gt;未导入时，update空操作，后续会将新数据导入&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;导入存量数据，与pt-osc类似
&lt;ul&gt;
&lt;li&gt;insert ignore select  xxx from old_table lock in share mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;redis-cluster&#34;&gt;Redis Cluster&lt;/h2&gt;
&lt;p&gt;Redis对一致性要求并不高，方案可以作为参考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;全量同步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master导出RDB文件，slave加载&lt;/li&gt;
&lt;li&gt;导出RDB期间的读写记录在缓冲区&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;增量同步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维持长连接，命令传播&lt;/li&gt;
&lt;li&gt;使用的是环形缓冲区，速度不匹配会丢数据导致不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;slot迁移&#34;&gt;slot迁移&lt;/h3&gt;
&lt;p&gt;新老节点配合&lt;/p&gt;
&lt;p&gt;从老节点迁移数据到新节点&lt;/p&gt;
&lt;p&gt;请求key时，如果该key已经从老节点迁移到新节点，老节点会返回ASK命令&lt;/p&gt;
&lt;p&gt;客户端请求新节点&lt;/p&gt;
&lt;p&gt;迁移完成后，再次请求原本在老节点的key，会返回MOVE命令，客户端可以更新路由&lt;/p&gt;
&lt;h1 id=&#34;数据一致性&#34;&gt;数据一致性&lt;/h1&gt;
&lt;p&gt;一般情况下，双写失败的概率很小，做了数据校验后进行切换，数据不一致概率很低，迁移后还可以再次校验&lt;/p&gt;
&lt;p&gt;但双写无法保证新老存储的事务性，总归是有不一致的概率&lt;/p&gt;
&lt;p&gt;下面讨论两种数据一致性高且比较通用的方案&lt;/p&gt;
&lt;h2 id=&#34;方案一全量同步增量迁移&#34;&gt;方案一：全量同步+增量迁移&lt;/h2&gt;
&lt;p&gt;导出快照，并且记录导出快照时位点，同步存量数据&lt;/p&gt;
&lt;p&gt;利用binlog追增量数据，主从差异降为0或者很小时，禁止主库写入，然后进行切换&lt;/p&gt;
&lt;p&gt;停写可以保证数据一致性，由于大部分都有分库分表，停写只会影响一部分用户&lt;/p&gt;
&lt;p&gt;注意在切换后，避免遗留/新增的请求仍然访问老库&lt;/p&gt;
&lt;h2 id=&#34;方案二细粒度迁移写操作与迁移互斥&#34;&gt;方案二：细粒度迁移，写操作与迁移互斥&lt;/h2&gt;
&lt;p&gt;方案一利用binlog有序，导入全量存量数后追增量数据&lt;/p&gt;
&lt;p&gt;还可以分uid迁移，可以理解成对uid的操作加分布式读写锁&lt;/p&gt;
&lt;p&gt;写操作与迁移操作竞争同一把分布式锁&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用户在线的读写请求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读时判断路由，读新库还是老库 (已经切到新库，读了老库不算问题)&lt;/li&gt;
&lt;li&gt;写时加锁，与用户其余写请求、离线迁移互斥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;worker离线的迁移请求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分uid迁移，控制速率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更具体一点的方案参考：&lt;/p&gt;
&lt;p&gt;新建一张迁移状态表，字段 id,uid,status,ctime,mtime,finish等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用户读请求，读迁移状态表，如果finish=1,读新库，否则读老库&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户写请求，先判断是否迁移完成，可以使用缓存等策略加速&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状态为待迁移时，更新状态占用锁，update set status = &amp;lsquo;迁移中&amp;rsquo; where status  = &amp;lsquo;待迁移&amp;rsquo; and uid = xxx，然后迁移数据到新库，迁移失败释放锁，下次迁移时删除新库数据；迁移成功后更新status = &amp;lsquo;迁移成功&amp;rsquo;，更新失败释放锁，返回处理失败&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与分布式锁类似，异常情况下可能会无法释放锁。可以增加一些逻辑，添加heartbeat字段，超时自动剥夺锁，worker定时扫描人工处理等等&lt;/p&gt;
&lt;p&gt;该方案将写与迁移互斥，保证数据一致性&lt;/p&gt;
&lt;p&gt;写持有锁时，其他写无法进行， 迁移无法进行&lt;/p&gt;
&lt;p&gt;迁移持有锁时，写无法进行&lt;/p&gt;
&lt;p&gt;实际操作时，可以进行迁移操作，迁移后更新status，但是并不读写新库&lt;/p&gt;
&lt;p&gt;观察服务与新老存储负载情况，再决定是否可采取该方案&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;当然，还是要看具体场景&lt;/p&gt;
&lt;p&gt;有些场景会有更简单的解决方案&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>缓存使用经验</title>
      <link>/posts/cache/</link>
      <pubDate>Mon, 27 May 2024 22:00:00 +0800</pubDate>
      
      <guid>/posts/cache/</guid>
      <description>一、Redis 1.1 大key、热key问题 大key 压缩：使用JSON序列化时，可以进行压缩，一般压缩率能在50%+ 拆分：假设使用string存浏览记录列表 记录过多时，数据过大，需要拆分 方式1：构造一个总key，history-summary:$uid，总key里保存分key信息 方式2：history:$uid直接存第一页信息，额外存总数量，其余全部页的key。数量少时，访问一次即可 下一页的key为$history_p1:$uid, hisotry_p2:$uid时，如果更新部分key失败，会有不一致问题 分页可以使用随机key，在第一页里标明，给前端page token，下一次可以用token作为key取数据 删除时，需要额外一个get操作， 先get到分页随机key，再删除 分页的如果用hashtag，可以使用redis事务保证同时成功，但是会集中在一个节点 热key 拆分：例如计数，拆成_0, _1，读取时合并 复制：只读的key，可以复制到_0, _1, _2，读取时随机读一个， 更新时一起更新 情况允许时，redis也可以读从库，但一般不会用 本地缓存：热key可以在本地进行缓存 实例较少时，本地热key发现即可 实例较多时，分布式统计 写热点：MQ削峰聚合 缓存穿透/雪崩 缓存空哨兵 随机过期时间 布隆过滤器提前拦截 1.2 缓存/DB数据一致性 常见的缓存更新方式
先更新DB，后更新缓存 更新缓存失败， 取到老数据 如果是更新DB后异步更新缓存，多次缓存写的顺序不确定 同步写，失败了还是老数据，还是要做重试，可能会有多个缓存更新顺序问题 更新缓存不建议放到DB事务中（见到过线上这么处理的） 开启事务，更新DB，更新缓存，更新缓存成功后提交事务，更新缓存失败，回滚事务 首先，更新缓存失败，可能是超时实际缓存已更新， 此时回滚DB造成数据不一致 更新缓存超时失败，再次查缓存查到了最新数据，此时commit，commit也可能失败，数据还是不一致 先更新缓存，后更新DB 排除 更新DB失败，缓存数据是错误的 先更新DB，后删除缓存 删除缓存失败， 会导致取到老数据，可以进行重试删除。或者什么都不做，等待缓存过期 即使删除成功，由于DB主从延迟，还是有可能读从库读到旧数据，写入缓存旧数据 缓存也有主从延迟的问题， 但一般缓存不读从库，不必考虑缓存的主从延迟 可以引入延迟删除，避免主从同步延迟 先删除缓存，后更新DB 删除缓存和更新DB期间，会读到旧数据， 更新旧数据到缓存 和上面类似 先更新缓存，后更新DB逻辑有问题
其余3种方式都大同小异，都没有100%完美
缓存操作不要放到DB事务 为避免主从延迟，可以采用延迟删除 高并发时，删除缓存会导致大量请求到DB，尽量采用更新缓存方式 更新缓存比删除缓存更复杂，有时要再拉取不同数据，组合起来 更新缓存有2个结果：新的数据 或者旧的数据 成功，新缓存 失败/超时，旧缓存或新缓存 (不考虑缓存过期) 而删除缓存的2个结果：旧的数据 或者无缓存， 这个结果更确定一些，重试无副作用， 而更新缓存的重试， 要考虑有没有更新的数据更新 成功，缓存被删除 失败/超时，旧缓存 或者 缓存被删除 更新缓存的时序会导致写入缓存顺序不一致，可以通过订阅binlog，串行更新 很多时候， 不必考虑太细， 我们认为时间差足够用应付上述的极端问题</description>
      <content>&lt;h1 id=&#34;一redis&#34;&gt;一、Redis&lt;/h1&gt;
&lt;h2 id=&#34;11-大key热key问题&#34;&gt;1.1 大key、热key问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;大key
&lt;ul&gt;
&lt;li&gt;压缩：使用JSON序列化时，可以进行压缩，一般压缩率能在50%+&lt;/li&gt;
&lt;li&gt;拆分：假设使用string存浏览记录列表
&lt;ul&gt;
&lt;li&gt;记录过多时，数据过大，需要拆分&lt;/li&gt;
&lt;li&gt;方式1：构造一个总key，history-summary:$uid，总key里保存分key信息&lt;/li&gt;
&lt;li&gt;方式2：history:$uid直接存第一页信息，额外存总数量，其余全部页的key。数量少时，访问一次即可
&lt;ul&gt;
&lt;li&gt;下一页的key为$history_p1:$uid, hisotry_p2:$uid时，如果更新部分key失败，会有不一致问题
&lt;ul&gt;
&lt;li&gt;分页可以使用随机key，在第一页里标明，给前端page token，下一次可以用token作为key取数据&lt;/li&gt;
&lt;li&gt;删除时，需要额外一个get操作， 先get到分页随机key，再删除&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分页的如果用hashtag，可以使用redis事务保证同时成功，但是会集中在一个节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;热key
&lt;ul&gt;
&lt;li&gt;拆分：例如计数，拆成_0, _1，读取时合并&lt;/li&gt;
&lt;li&gt;复制：只读的key，可以复制到_0, _1, _2，读取时随机读一个， 更新时一起更新
&lt;ul&gt;
&lt;li&gt;情况允许时，redis也可以读从库，但一般不会用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;本地缓存：热key可以在本地进行缓存
&lt;ul&gt;
&lt;li&gt;实例较少时，本地热key发现即可&lt;/li&gt;
&lt;li&gt;实例较多时，分布式统计&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;写热点：MQ削峰聚合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缓存穿透/雪崩
&lt;ul&gt;
&lt;li&gt;缓存空哨兵&lt;/li&gt;
&lt;li&gt;随机过期时间&lt;/li&gt;
&lt;li&gt;布隆过滤器提前拦截&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;12-缓存db数据一致性&#34;&gt;1.2 缓存/DB数据一致性&lt;/h2&gt;
&lt;p&gt;常见的缓存更新方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先更新DB，后更新缓存
&lt;ul&gt;
&lt;li&gt;更新缓存失败， 取到老数据
&lt;ul&gt;
&lt;li&gt;如果是更新DB后异步更新缓存，多次缓存写的顺序不确定&lt;/li&gt;
&lt;li&gt;同步写，失败了还是老数据，还是要做重试，可能会有多个缓存更新顺序问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;更新缓存不建议放到DB事务中（见到过线上这么处理的）
&lt;ul&gt;
&lt;li&gt;开启事务，更新DB，更新缓存，更新缓存成功后提交事务，更新缓存失败，回滚事务
&lt;ul&gt;
&lt;li&gt;首先，更新缓存失败，可能是超时实际缓存已更新， 此时回滚DB造成数据不一致&lt;/li&gt;
&lt;li&gt;更新缓存超时失败，再次查缓存查到了最新数据，此时commit，commit也可能失败，数据还是不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;del&gt;先更新缓存，后更新DB&lt;/del&gt; 排除
&lt;ul&gt;
&lt;li&gt;更新DB失败，缓存数据是错误的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先更新DB，后删除缓存
&lt;ul&gt;
&lt;li&gt;删除缓存失败， 会导致取到老数据，可以进行重试删除。或者什么都不做，等待缓存过期
&lt;ul&gt;
&lt;li&gt;即使删除成功，由于&lt;strong&gt;DB主从延迟&lt;/strong&gt;，还是有可能读从库读到旧数据，写入缓存旧数据
&lt;ul&gt;
&lt;li&gt;缓存也有主从延迟的问题， 但一般缓存不读从库，不必考虑缓存的主从延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可以引入延迟删除，避免主从同步延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先删除缓存，后更新DB
&lt;ul&gt;
&lt;li&gt;删除缓存和更新DB期间，会读到旧数据， 更新旧数据到缓存
&lt;ul&gt;
&lt;li&gt;和上面类似&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先更新缓存，后更新DB逻辑有问题&lt;/p&gt;
&lt;p&gt;其余3种方式都大同小异，都没有100%完美&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存操作不要放到DB事务&lt;/li&gt;
&lt;li&gt;为避免主从延迟，可以采用延迟删除&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高并发&lt;/strong&gt;时，删除缓存会导致大量请求到DB，尽量采用&lt;strong&gt;更新缓存&lt;/strong&gt;方式
&lt;ul&gt;
&lt;li&gt;更新缓存比删除缓存更复杂，有时要再拉取不同数据，组合起来&lt;/li&gt;
&lt;li&gt;更新缓存有2个结果：新的数据 或者旧的数据
&lt;ul&gt;
&lt;li&gt;成功，新缓存&lt;/li&gt;
&lt;li&gt;失败/超时，旧缓存或新缓存 (不考虑缓存过期)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;而删除缓存的2个结果：旧的数据 或者无缓存， 这个结果更确定一些，重试无副作用， 而更新缓存的重试， 要考虑有没有&lt;strong&gt;更新的数据更新&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;成功，缓存被删除&lt;/li&gt;
&lt;li&gt;失败/超时，旧缓存 或者 缓存被删除&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;更新缓存的时序会导致写入缓存顺序不一致，可以通过订阅binlog，串行更新&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很多时候， 不必考虑太细， 我们认为&lt;strong&gt;时间差&lt;/strong&gt;足够用应付上述的极端问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先更新DB，后删除缓存
&lt;ul&gt;
&lt;li&gt;有主从延迟问题， 但主从延迟一般很小，再次查询的时间差足够同步&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先删除缓存，后更新DB
&lt;ul&gt;
&lt;li&gt;删除缓存 与 更新DB的时间差内，可能将旧数据写入缓存，认为这个时间差很小，忽略&lt;/li&gt;
&lt;li&gt;再有，即使期间没有读操作，也仍有主从延迟问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;p&gt;数据库与缓存是两个系统，总会有不一致&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间差&lt;/strong&gt;与&lt;strong&gt;时序&lt;/strong&gt;会有引发各种问题&lt;/p&gt;
&lt;h3 id=&#34;一种更新的方式尽力避免主从延迟避免不一致&#34;&gt;一种更新的方式，尽力避免主从延迟，避免不一致&lt;/h3&gt;
&lt;p&gt;例如uid的账号基本信息， 空哨兵可以存empty特殊字符，也可以存{uid:-1}， 用-1标记为空&lt;/p&gt;
&lt;p&gt;参考这种方式&lt;/p&gt;
&lt;p&gt;更新DB后， 写入缓存{uid: -2} （-2标识最近有更新，读取时需要再次读从库，或者主库）  过期时间 略大于主从延迟&lt;/p&gt;
&lt;p&gt;应用读缓存，读到-2时，时效要求强的，可以读主库  (读了主库也不更新缓存)， 时效要求低的，读从库&lt;/p&gt;
&lt;p&gt;等到-2过期后，缓存无数据，再次读从库，构造缓存&lt;/p&gt;
&lt;p&gt;并发更新时，会一直向缓存写入-2，等到-2过期，认为最近没有更新，可以读从库构建数据&lt;/p&gt;
&lt;p&gt;这个方式假设主从复制在一个固定时间内完成&lt;/p&gt;
&lt;h3 id=&#34;另外一种更新方式&#34;&gt;另外一种更新方式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;更新DB后，更新缓存&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新DB后，更新缓存期间，缓存可能过期&lt;/p&gt;
&lt;p&gt;此时另外一个读请求可能读DB读到旧数据， 再更新DB请求，更新缓存之后， 写入缓存&lt;/p&gt;
&lt;p&gt;更新DB后更新缓存，用SET写入&lt;/p&gt;
&lt;p&gt;读请求构造缓存时，用SETNX写入&lt;/p&gt;
&lt;p&gt;这样避免，读请求的老数据覆盖掉更新后的新数据&lt;/p&gt;
&lt;h2 id=&#34;13-zset使用&#34;&gt;1.3 zset使用&lt;/h2&gt;
&lt;p&gt;用户点赞列表，zset score存时间戳，member存被点赞对象id&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先读缓存，有数据直接使用&lt;/li&gt;
&lt;li&gt;再读DB，写入zset&lt;/li&gt;
&lt;li&gt;写操作时， 需要先判断zset是否存在， 不存在则不更新zset
&lt;ul&gt;
&lt;li&gt;zset存在，则zadd&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单实现就是上述方案，但是上述有时序问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先读缓存，zset不存在&lt;/li&gt;
&lt;li&gt;此时读DB，读了100条数据，还未写入zset&lt;/li&gt;
&lt;li&gt;新来的写请求，新点赞记录，判断zset不存在，未写入zset
&lt;ul&gt;
&lt;li&gt;如何判断zset存在？exist判断，ttl判断，可能恰好只剩下1s过期，不完美&lt;/li&gt;
&lt;li&gt;可以用expire 延迟过期时间， expire $key 60 更新成功返回1，key不存在返回0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;此时100条数据写入zset， zset丢失了最新一条数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;读zset，不存在时，再读DB之前，应该向zset写入一个假数据， 例如score时间戳无限大/无限小， member=-1&lt;/p&gt;
&lt;p&gt;这样，写请求就可以将最新的数据写入&lt;/p&gt;
&lt;p&gt;读缓存时，要过滤掉member=-1&lt;/p&gt;
&lt;h2 id=&#34;14-并发更新缓存的一些通用解法&#34;&gt;1.4 并发更新缓存的一些通用解法&lt;/h2&gt;
&lt;h3 id=&#34;缓存提供版本号set能力--cas更新&#34;&gt;缓存提供版本号set能力  cas更新&lt;/h3&gt;
&lt;p&gt;SET时传入版本号，版本号大于key的版本号时，才能写入&lt;/p&gt;
&lt;h3 id=&#34;监听binlog更新&#34;&gt;监听binlog更新&lt;/h3&gt;
&lt;p&gt;binlog有序，串行操作&lt;/p&gt;
&lt;p&gt;合并：同一个key， update 1, 2, 3 可以合并后只处理最后一个写&lt;/p&gt;
&lt;h3 id=&#34;redis-的watch--multi&#34;&gt;redis 的watch  multi&lt;/h3&gt;
&lt;p&gt;太麻烦&lt;/p&gt;
&lt;h1 id=&#34;二本地缓存&#34;&gt;二、本地缓存&lt;/h1&gt;
&lt;p&gt;redis的性能很高，单节点能够达到10W QPS&lt;/p&gt;
&lt;p&gt;但是有时候还是有瓶颈，可以再本地再做一层缓存&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定期更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;异步更新&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;哪些key要本地缓存？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;预先知道的，可以云控下发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;未知的，热key发现&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地记录
&lt;ul&gt;
&lt;li&gt;可以用lfu+滑动窗口， 可以参考redis的lfu实现&lt;/li&gt;
&lt;li&gt;采样， 1/10 1/20，每10个/20个key 统计一次  100W qps，降到10w qps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读合并&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;golang的singleflight， 多个key合并为一次访问
&lt;ul&gt;
&lt;li&gt;第一个请求的ctx过短，失败后，被合并的请求也立即失败
&lt;ul&gt;
&lt;li&gt;可以加一些策略进行重试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;singleflight也有一些问题：例如单个连接异常，被合并的请求都会被影响&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>InnoDB索引占用空间分析</title>
      <link>/posts/innodb-space/</link>
      <pubDate>Mon, 27 May 2024 09:00:00 +0800</pubDate>
      
      <guid>/posts/innodb-space/</guid>
      <description>背景 一两年前在申请Redis资源时，预估了N GB，上线后查看监控， 数据占用大于N GB（大很多）
因此阅读Redis源码，了解到内部一些数据结构的额外开销：dictEntry，robj，sds header，malloc碎片等等
对于MySQL，一直不确定空间在上有哪些开销
问题 新建如下表，插入100W行记录，占用多大空间？
create table test ( id bigint not null auto_increment, uid bigint not null, status int not null, ctime bigint not null, mtime bigint not null, primary key (id) ) engine = innodb; uid、status分别新增索引，会各自增加多大空间？
主键是bigint，一个page非叶子节点最多多少条记录？ 主键是int，又是多少
-- 存储过程插入100W行 DROP PROCEDURE IF EXISTS insert_batch; DELIMITER $$ CREATE PROCEDURE insert_batch(max_num INT) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT INSERT INTO test (uid, status, ctime, mtime) VALUES (1, 2, 3, 4); SET i = i + 1; UNTIL i = max_num END REPEAT; COMMIT; END$$ DELIMITER ; call insert_batch(1000000); 分析工具 innodb_ruby 作者博客</description>
      <content>&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;
&lt;p&gt;一两年前在申请Redis资源时，预估了N GB，上线后查看监控， 数据占用大于N GB（大很多）&lt;/p&gt;
&lt;p&gt;因此阅读Redis源码，了解到内部一些数据结构的额外开销：dictEntry，robj，sds header，malloc碎片等等&lt;/p&gt;
&lt;p&gt;对于MySQL，一直不确定空间在上有哪些开销&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;新建如下表，插入100W行记录，占用多大空间？&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;create table test (
    id bigint not null auto_increment,
    uid bigint not null,
    status int not null,
    ctime bigint not null,
    mtime bigint not null,
    primary key (id)
) engine = innodb;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;uid、status分别新增索引，会各自增加多大空间？&lt;/p&gt;
&lt;p&gt;主键是bigint，一个page非叶子节点最多多少条记录？ 主键是int，又是多少&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-- 存储过程插入100W行
DROP PROCEDURE IF EXISTS insert_batch;
DELIMITER $$
CREATE PROCEDURE insert_batch(max_num INT)
BEGIN
    DECLARE i INT DEFAULT 0;
    SET autocommit = 0;
    REPEAT
        INSERT INTO test (uid, status, ctime, mtime) VALUES (1, 2, 3, 4);
        SET i = i + 1;
        UNTIL i = max_num
    END REPEAT;
    COMMIT;
END$$
DELIMITER ;

call insert_batch(1000000);
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;分析工具&#34;&gt;分析工具&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jeremycole/innodb_ruby&#34;&gt;innodb_ruby&lt;/a&gt;      &lt;a href=&#34;https://blog.jcole.us/innodb/&#34;&gt;作者博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ibd文件分析&lt;/p&gt;
&lt;h1 id=&#34;一索引空间占用&#34;&gt;一、索引空间占用&lt;/h1&gt;
&lt;p&gt;不同版本可能会有不同，仅供参考。下面测试使用MySQL 5.7版本&lt;/p&gt;
&lt;h2 id=&#34;10-结论&#34;&gt;1.0 结论&lt;/h2&gt;
&lt;h3 id=&#34;聚簇索引&#34;&gt;聚簇索引&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;叶子节点
&lt;ul&gt;
&lt;li&gt;各字段大小 +  18B（record header=5B + roll_ptr=7B + trx_id=6B）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非叶子节点
&lt;ul&gt;
&lt;li&gt;主键大小 + 5B(record header) + 4B（指针）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;二级索引&#34;&gt;二级索引&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引字段大小 + 主键大小 + 5B(record header)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引字段大小 + 主键大小  + 5B(record header) + 4B（指针）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他影响因素&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_fill_factor&#34;&gt;innodb_fill_factor&lt;/a&gt; page内默认预留1/16空间&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;测试验证时，叶子节点有预留，非叶子节点不预留&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;page directory：平均4条record一个slot，每个slot 2B，即1个记录0.5B&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个page有128B的元数据开销&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;页内碎片&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.jcole.us/innodb/&#34;&gt;作者博客&lt;/a&gt;里可以找到详细说明&lt;/p&gt;
&lt;h3 id=&#34;计算&#34;&gt;计算&lt;/h3&gt;
&lt;h4 id=&#34;聚簇索引-1&#34;&gt;聚簇索引&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;记录大小 54B：各个字段占用 8(id)+8(uid)+8(ctime)+8(ctime)+4(status) = 36B + 额外开销18B=54B&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一页最多记录数：(16384 -128)*15/16 / 54.5 = 279.6330&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非叶子节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;记录大小： 8+5+4 = 17B&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128) / 17.5 = 928.9143&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;二级索引-idx_status为例&#34;&gt;二级索引 idx_status为例&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;叶子节点
&lt;ul&gt;
&lt;li&gt;记录大小：4+8+5 = 17&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128)*15/16 / 17.5 = 870.8571&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非叶子节点
&lt;ul&gt;
&lt;li&gt;记录大小：4+8+5+4 = 21&lt;/li&gt;
&lt;li&gt;一页最多记录数：(16384 -128) / 21.5 = 756.0930&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面使用innodb_space分析验证&lt;/p&gt;
&lt;h2 id=&#34;11-聚簇索引-叶子节点&#34;&gt;1.1 聚簇索引 叶子节点&lt;/h2&gt;
&lt;h3 id=&#34;叶子节点使用page数量--3585&#34;&gt;叶子节点使用page数量 = 3585&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522232345900.png&#34; alt=&#34;image-20240522232345900&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里只分析INDEX页&lt;/p&gt;
&lt;p&gt;ALLOCATED也会占用ibd空间，具体规则未深入研究&lt;/p&gt;
&lt;p&gt;space-indexes显示 ibd文件中，叶子节点使用了3585个page (每个page 16KB)&lt;/p&gt;
&lt;h3 id=&#34;叶子节点的记录数是1392042793583&#34;&gt;叶子节点的记录数是139+204+279*3583&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523001012100.png&#34; alt=&#34;image-20240523001012100&#34;&gt;&lt;/p&gt;
&lt;p&gt;index-level-summary   -l 表示第几层 -l 0为叶子节点&lt;/p&gt;
&lt;p&gt;page4 139条记录，page3616 204条记录，剩余3583个page都是279条记录，139+204+279 * 3583 = 100W&lt;/p&gt;
&lt;p&gt;一页最多279个记录，符合计算&lt;/p&gt;
&lt;p&gt;page5， data=15066, records=279 15066/279=54，符合计算&lt;/p&gt;
&lt;p&gt;free=1050 表示每页有1050剩余空间&lt;/p&gt;
&lt;p&gt;1050/16384 = 0.0641 与 innodb_fill_factor 1/16=0.0625接近&lt;/p&gt;
&lt;h3 id=&#34;每页的占用&#34;&gt;每页的占用&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522234350316.png&#34; alt=&#34;image-20240522234350316&#34;&gt;&lt;/p&gt;
&lt;p&gt;128B(页的结构信息) + 54 * 279（每条记录大小） * 140 (page directory开销) + 1050 (空闲) = 16384&lt;/p&gt;
&lt;p&gt;一字节不差&lt;/p&gt;
&lt;h3 id=&#34;page-directory-为什么是140字节&#34;&gt;page directory 为什么是140字节&lt;/h3&gt;
&lt;p&gt;每个slot 2B&lt;/p&gt;
&lt;p&gt;279 / 4 = 69.75，是向上进到70的吗？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240522235107637.png&#34; alt=&#34;image-20240522235107637&#34;&gt;&lt;/p&gt;
&lt;p&gt;分析page5的page directory&lt;/p&gt;
&lt;p&gt;可见并不是进上去的，确实有70个slot，其中infimum的owned固定为1 只包含自身，supremum为8，除自身外还有7个   4*68 + 7 = 279 刚好&lt;/p&gt;
&lt;h2 id=&#34;12-聚簇索引-非叶子节点&#34;&gt;1.2 聚簇索引 非叶子节点&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523001846807.png&#34; alt=&#34;image-20240523001846807&#34;&gt;&lt;/p&gt;
&lt;p&gt;-l 1可以看到，每页最多928个记录，符合计算&lt;/p&gt;
&lt;p&gt;15776/928 = 17，记录大小符合&lt;/p&gt;
&lt;h2 id=&#34;13-二级索引-叶子节点&#34;&gt;1.3 二级索引 叶子节点&lt;/h2&gt;
&lt;p&gt;添加索引 alter table test add index idx_uid (uid);&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002302195.png&#34; alt=&#34;image-20240523002302195&#34;&gt;&lt;/p&gt;
&lt;p&gt;再添加一个索引  alter table test add index idx_status (status);&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002320724.png&#34; alt=&#34;image-20240523002320724&#34;&gt;&lt;/p&gt;
&lt;p&gt;idx_uid叶子大小应该为 8+8+5 = 21B&lt;/p&gt;
&lt;p&gt;idx_status叶子节点大小应该为 4+8+5=17B&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/image-20240523002706499.png&#34; alt=&#34;image-20240523002706499&#34;&gt;&lt;/p&gt;
&lt;p&gt;idx_status举例&lt;/p&gt;
&lt;p&gt;一页最多928个记录，符合计算&lt;/p&gt;
&lt;p&gt;15776/928 = 17，符合计算&lt;/p&gt;
&lt;p&gt;用叶子节点used进行校验&lt;/p&gt;
&lt;p&gt;idx_uid.used/idx_status.used = 1325 / 1078 = 1.2291&lt;/p&gt;
&lt;p&gt;21/17 = 1.2353 接近&lt;/p&gt;
&lt;h2 id=&#34;14-二级索引-非叶子节点&#34;&gt;1.4 二级索引 非叶子节点&lt;/h2&gt;
&lt;p&gt;idx_status非叶子节点大小  4+8+5+4 = 21&lt;/p&gt;
&lt;p&gt;用-l 1的数据计算&lt;/p&gt;
&lt;p&gt;上面计算出最大记录数是756.0930，这里是755&lt;/p&gt;
&lt;p&gt;15855/755=21 符合计算&lt;/p&gt;
&lt;h2 id=&#34;15-总结&#34;&gt;1.5 总结&lt;/h2&gt;
&lt;h3 id=&#34;聚簇索引-2&#34;&gt;聚簇索引&lt;/h3&gt;
&lt;p&gt;叶子节点除了各字段外，还有roll_ptr，trx_id&lt;/p&gt;
&lt;h3 id=&#34;二级索引-1&#34;&gt;二级索引&lt;/h3&gt;
&lt;p&gt;叶子节点没有roll_ptr，没有trx_id&lt;/p&gt;
&lt;p&gt;叶子节点有主键id&lt;/p&gt;
&lt;p&gt;非叶子节点也有主键id&lt;/p&gt;
&lt;p&gt;ps：二级索引上没有roll_ptr、trx_id，MVCC可见性如何判断？&lt;/p&gt;
&lt;h3 id=&#34;不同主键大小非叶子节点一页最多存多少&#34;&gt;不同主键大小，非叶子节点一页最多存多少？&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bigint
&lt;ul&gt;
&lt;li&gt;非叶子节点最多928.8 = (16384-128) /(8+5+4+0.5)&lt;/li&gt;
&lt;li&gt;实际验证是928&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;int
&lt;ul&gt;
&lt;li&gt;非叶子节点最多1204.0 = (16384-128) /(8+5+4+0.5)&lt;/li&gt;
&lt;li&gt;实际验证不是1204， 是1203
&lt;ul&gt;
&lt;li&gt;与上面 计算得到756.0930，实际755类似， 实际上是能够再存一个记录，但是剩下一个空位&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;二一些常见问题&#34;&gt;二、一些常见问题&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;为什么表不要超过千万行
&lt;ul&gt;
&lt;li&gt;bigint主键， 假设记录1k，叶子节点大概存15个
&lt;ul&gt;
&lt;li&gt;两层非叶子节点928^2 = 861184 = 86.1W ，三层记录数1300W&lt;/li&gt;
&lt;li&gt;三层非叶子节点928^3=799178752  7.99亿 四层记录数 120亿&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;三层时，前2层非叶子节点基本可以全缓存在内存，占用(928+1)*16K = 232.25MB
&lt;ul&gt;
&lt;li&gt;增删改查仅需要访问一次磁盘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;四层时，不可能完全缓存前3层非叶子节点，需要访问2次磁盘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;为什么索引快
&lt;ul&gt;
&lt;li&gt;主键上字段很多，数据量大&lt;/li&gt;
&lt;li&gt;二级索引数据量小&lt;/li&gt;
&lt;li&gt;回表成本很高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分表分多少合适，以uid mod分表数为例
&lt;ul&gt;
&lt;li&gt;首先，一定不是越多越好，分65536个，16384个表显然不合适&lt;/li&gt;
&lt;li&gt;bigint主键， 完全缓存前两层非叶子节点占用14.5MB&lt;/li&gt;
&lt;li&gt;如果所有uid都是均匀访问
&lt;ul&gt;
&lt;li&gt;分1024个表，前两层全在内存，占用14.5G，基本能够容纳&lt;/li&gt;
&lt;li&gt;分2048个表，想要全缓存前两层，需要29G内存， 比较大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;压测、实践验证为准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;三遇到的慢查询分析&#34;&gt;三、遇到的慢查询分析&lt;/h1&gt;
&lt;p&gt;explain&lt;/p&gt;
&lt;h2 id=&#34;31-varchar字段-like-xxx-不走索引&#34;&gt;3.1 varchar字段 like %xxx% 不走索引&lt;/h2&gt;
&lt;p&gt;select * from t where country like&amp;rsquo;%US%&amp;rsquo; ; 用不了country索引&lt;/p&gt;
&lt;p&gt;只能全表扫描&lt;/p&gt;
&lt;p&gt;全表扫描一定比用索引快吗？&lt;/p&gt;
&lt;p&gt;如果满足where的行比较少，只有少数回表，走索引会更快&lt;/p&gt;
&lt;p&gt;force index (idx_country)， 是无法用上country索引的&lt;/p&gt;
&lt;p&gt;需要使用子查询才能走country索引&lt;/p&gt;
&lt;p&gt;select * from t where id in(select id from t where country like &amp;lsquo;%US%&amp;rsquo;)&lt;/p&gt;
&lt;p&gt;子查询里可以用到idx_country索引，只查id，不会走全表扫描&lt;/p&gt;
&lt;p&gt;（这个场景后续改用了ES做查询）&lt;/p&gt;
&lt;h2 id=&#34;32-覆盖索引&#34;&gt;3.2 覆盖索引&lt;/h2&gt;
&lt;p&gt;select count(*) from t where uid = 123 and status =  1; (uid有索引，(uid, status)没有索引)&lt;/p&gt;
&lt;p&gt;大部分情况会用上uid索引，再回表判断status&lt;/p&gt;
&lt;p&gt;有些uid数据较多，会走全表扫描&lt;/p&gt;
&lt;p&gt;创建(uid, status)覆盖索引， 避免回表，就会选择索引了&lt;/p&gt;
&lt;p&gt;覆盖索引，有时，仅仅为了覆盖，将原来多字段的聚簇索引抽出部分字段，组成一个“窄“表，加快查询&lt;/p&gt;
&lt;p&gt;实际上就是空间换时间的体现&lt;/p&gt;
&lt;h2 id=&#34;34-避免选择错误索引&#34;&gt;3.4 避免选择错误索引&lt;/h2&gt;
&lt;p&gt;select where uid = 123 order by ctime limit 10;&lt;/p&gt;
&lt;p&gt;uid有索引，ctime有索引&lt;/p&gt;
&lt;p&gt;我们更希望选择uid索引，但由于order by ctime，优化器有时会选择ctime&lt;/p&gt;
&lt;p&gt;除了force index，还可以修改sql，order by (ctime + 0) 或者 order by ctime asc,  id desc (ctime asc, id asc 可以用索引)&lt;/p&gt;
&lt;h2 id=&#34;35-深翻页&#34;&gt;3.5 深翻页&lt;/h2&gt;
&lt;p&gt;翻页时offset过大， 查询会变慢&lt;/p&gt;
&lt;p&gt;因为被翻页的记录也都要回表&lt;/p&gt;
&lt;p&gt;一种方法是不用offset，每次带上上次的id，where id &amp;gt; lastid 来查询&lt;/p&gt;
&lt;p&gt;另外也可以用子查询，先查出id，再用id来查&lt;/p&gt;
&lt;h2 id=&#34;36-离线worker扫表&#34;&gt;3.6 离线worker扫表&lt;/h2&gt;
&lt;p&gt;需求是扫描ctime再(111, 999)范围，status是unpay的订单&lt;/p&gt;
&lt;p&gt;select * from order_info where ctime &amp;gt; 111 and ctime &amp;lt; 999 and status = &amp;lsquo;unpay&amp;rsquo; and id &amp;gt; $lastID order by id asc limit 1;&lt;/p&gt;
&lt;p&gt;走了id索引，但是用主键扫描，首次定位第一个id，耗时较长&lt;/p&gt;
&lt;p&gt;方法1：先用ctime索引，查出最大最小id  select min(id), max(id) from order_info where xxx&lt;/p&gt;
&lt;p&gt;方法2：直接使用ctime索引，再回表，select * from order_info where (ctime = $lastTime and id &amp;gt; $lastID) or ctime &amp;gt; lastTime&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Golang Mutex</title>
      <link>/posts/golang-mutex/</link>
      <pubDate>Mon, 20 May 2024 23:32:13 +0800</pubDate>
      
      <guid>/posts/golang-mutex/</guid>
      <description>问题引入 上一篇博客提到IO阻塞，打印log时，时间可能是乱序的
由此来分析Go中sync.Mutex在不同场景的表现
测试代码 go1.19，在每分钟0s Write时阻塞3s
package main import ( &amp;#34;log&amp;#34; &amp;#34;math/rand&amp;#34; &amp;#34;os&amp;#34; &amp;#34;sync/atomic&amp;#34; &amp;#34;time&amp;#34; ) type DelayWriter struct { *os.File } var timeslot [60]int64 func (w *DelayWriter) Write(b []byte) (int, error) { now := time.Now() if now.Second() == 0 &amp;amp;&amp;amp; atomic.AddInt64(&amp;amp;timeslot[now.Minute()], 1) == 1 { time.Sleep(time.Millisecond * 3333) // 每分钟 0s，模拟写日志阻塞 } return w.File.Write(b) } func NewDelayWriter(filename string) *DelayWriter { f, err := os.OpenFile(filename, os.O_CREATE|os.O_RDWR|os.O_APPEND|os.O_TRUNC, 0666) if err != nil { panic(err) } return &amp;amp;DelayWriter{f} } var logger = log.</description>
      <content>&lt;h1 id=&#34;问题引入&#34;&gt;问题引入&lt;/h1&gt;
&lt;p&gt;上一篇博客提到IO阻塞，打印log时，时间可能是乱序的&lt;/p&gt;
&lt;p&gt;由此来分析Go中sync.Mutex在不同场景的表现&lt;/p&gt;
&lt;p&gt;测试代码 go1.19，在每分钟0s Write时阻塞3s&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package main

import (
	&amp;#34;log&amp;#34;
	&amp;#34;math/rand&amp;#34;
	&amp;#34;os&amp;#34;
	&amp;#34;sync/atomic&amp;#34;
	&amp;#34;time&amp;#34;
)

type DelayWriter struct {
	*os.File
}

var timeslot [60]int64

func (w *DelayWriter) Write(b []byte) (int, error) {
	now := time.Now()
	if now.Second() == 0 &amp;amp;&amp;amp; atomic.AddInt64(&amp;amp;timeslot[now.Minute()], 1) == 1 {
		time.Sleep(time.Millisecond * 3333) // 每分钟 0s，模拟写日志阻塞
	}
	return w.File.Write(b)
}

func NewDelayWriter(filename string) *DelayWriter {
	f, err := os.OpenFile(filename, os.O_CREATE|os.O_RDWR|os.O_APPEND|os.O_TRUNC, 0666)
	if err != nil {
		panic(err)
	}
	return &amp;amp;DelayWriter{f}
}

var logger = log.New(NewDelayWriter(&amp;#34;test.log&amp;#34;), &amp;#34;&amp;#34;, log.LstdFlags|log.Lmicroseconds|log.Lshortfile)

func main() {
	for i := 0; i &amp;lt; 100; i++ {
		go func(index int) {
			var seq int
			for {
				seq++

				r := rand.Int31n(1000) + 100
				time.Sleep(time.Duration(r) * time.Millisecond)

				go serve(index, seq) // 处理请求
			}
		}(i)
	}

	time.Sleep(time.Minute * 2)
	logger.Println(&amp;#34;end&amp;#34;)
}

func serve(index int, seq int) {
	logger.Println(&amp;#34;recv request&amp;#34;, index, seq)
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果，可以看到在0s附近，时间有递增、递减、反复变化&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/golang-mutex-log.jpg&#34; alt=&#34;log&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;syncmutex实现分析&#34;&gt;sync.Mutex实现分析&lt;/h1&gt;
&lt;p&gt;当前sync.Mutex的实现比较复杂，引入了普通（normal）模式，饥饿（starvation）模式&lt;/p&gt;
&lt;p&gt;sync.Mutex代码经过了4个版本变化，直接看最新代码较难理解，接下来按照4个版本变化依次介绍&lt;/p&gt;
&lt;h2 id=&#34;信号量原语&#34;&gt;信号量原语&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Semacquire waits until *s &amp;gt; 0 and then atomically decrements it.
// It is intended as a simple sleep primitive for use by the synchronization
// library and should not be used directly.
func runtime_Semacquire(s *uint32)

// Semrelease atomically increments *s and notifies a waiting goroutine
// if one is blocked in Semacquire.
// It is intended as a simple wakeup primitive for use by the synchronization
// library and should not be used directly.
func runtime_Semrelease(s *uint32)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;v1simple-implementation&#34;&gt;V1：Simple implementation.&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Mutex struct {
    key  int32 // Indication of whether the lock is held
    sema int32 // Semaphore dedicated to block/wake up goroutine
}

func (m *Mutex) Lock() {
    if atomic.AddInt32(&amp;amp;m.key, 1) == 1 {
        return
    }
    semacquire(&amp;amp;m.sema)
}

func (m *Mutex) Unlock() {
    if atomic.AddInt32(&amp;amp;m.key, -1) == 0 {
        return
    }
    semrelease(&amp;amp;m.sema)
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock&#34;&gt;Lock&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;key原子增加
&lt;ul&gt;
&lt;li&gt;新值为1获取到锁；否则：&lt;/li&gt;
&lt;li&gt;信号量semacquire&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unlock&#34;&gt;Unlock&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;key原子增加-1
&lt;ul&gt;
&lt;li&gt;新值为0，说明没有其他在等待的，直接返回；否则：&lt;/li&gt;
&lt;li&gt;信号量semrelease，唤醒等待的goroutine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举例&#34;&gt;举例&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;goroutine1（简称g1） Lock&lt;/li&gt;
&lt;li&gt;g2请求Lock，semacquire&lt;/li&gt;
&lt;li&gt;g1 Unlock，semrelease，唤醒等待者g2&lt;/li&gt;
&lt;li&gt;g2进入临界区操作，操作完 再Unlock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果g2 原子incr key，还未调用 semacquire(&amp;amp;m.sema)&lt;/p&gt;
&lt;p&gt;g1先semrelease(&amp;amp;m.sema)，还能不能唤醒g2？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g1 Lock 成功&lt;/li&gt;
&lt;li&gt;g2 Lock，进程暂停&lt;/li&gt;
&lt;li&gt;g1 Unlock，semrelease(&amp;amp;m.sema)，此时g1还未休眠，无法唤醒，信号量自增&lt;/li&gt;
&lt;li&gt;g2 进程继续，semacquire(&amp;amp;m.sema)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;g2执行semacquire(&amp;amp;m.sema)  后&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Semacquire waits until *s &amp;gt; 0 and then atomically decrements it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;还是能够正常执行&lt;/p&gt;
&lt;h2 id=&#34;v2new-goroutine-participates-in-lock-competition&#34;&gt;V2：New Goroutine participates in lock competition.&lt;/h2&gt;
&lt;p&gt;信号量先进先出，V1版本如果有goroutine在等待了，新来的&lt;strong&gt;正在&lt;/strong&gt;运行的goroutine也必须等待&lt;/p&gt;
&lt;p&gt;唤醒老的在睡眠的goroutine，显然开销大于正在运行的goroutine直接获取锁&lt;/p&gt;
&lt;p&gt;V2版本，允许已经有goroutine睡眠等待时，正在运行的goroutine先获取到锁&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Mutex struct {
   state int32
   sema  uint32
}

const (
   mutexLocked = 1 &amp;lt;&amp;lt; iota // mutex is locked
   mutexWoken
   mutexWaiterShift = iota
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock-1&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
   if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
      return
   }

   awoke := false
   for {
      old := m.state
      var new int32
      if old&amp;amp;mutexLocked == 0 {
         new = old | mutexLocked
      } else {
      	 new = old + 1&amp;lt;&amp;lt;mutexWaiterShift
      }
      if awoke {
         new &amp;amp;^= mutexWoken
      }
      if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
         if old&amp;amp;mutexLocked == 0 {
            break
         }
         runtime.Semacquire(&amp;amp;m.sema)
         awoke = true
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;state cas从0到1成功，直接得到锁； 否则
&lt;ul&gt;
&lt;li&gt;for 循环，进行cas操作
&lt;ul&gt;
&lt;li&gt;如果当前没有锁标记，加上锁标记&lt;/li&gt;
&lt;li&gt;有锁标记，增加一个等待计数&lt;/li&gt;
&lt;li&gt;另外，awoke为true时，清除mutexWoken比特位&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cas进行更新，更新成功时
&lt;ul&gt;
&lt;li&gt;旧值没有锁标记，表示本次cas拿到锁，直接退出；否则&lt;/li&gt;
&lt;li&gt;Semacquire，进入睡眠等待，等待结束后，awoke置为true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unlock-1&#34;&gt;Unlock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Unlock() {
   new := atomic.AddInt32(&amp;amp;m.state, -mutexLocked)

   old := new
   for {
      if old&amp;gt;&amp;gt;mutexWaiterShift == 0 || old&amp;amp;(mutexLocked|mutexWoken) != 0 {
         return
      }

      new = (old - 1&amp;lt;&amp;lt;mutexWaiterShift) | mutexWoken
      if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
         runtime.Semrelease(&amp;amp;m.sema)
         return
      }
      old = m.state
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;原子减-1&lt;/li&gt;
&lt;li&gt;for循环
&lt;ul&gt;
&lt;li&gt;如果没有等待者， 或者 有lock/woken标记， 退出；否则&lt;/li&gt;
&lt;li&gt;cas更新，新值为减去1个等待计数|woken标记
&lt;ul&gt;
&lt;li&gt;cas成功，Semrelease唤醒一个等待者&lt;/li&gt;
&lt;li&gt;cas失败， 继续判断&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举例-1&#34;&gt;举例&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;g1 Lock&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;g2, g3 请求Lock，进入休眠等待&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;g1 Unlock 和 g4 Lock， g5 Lock几乎同时发生&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;第一种情况 g1 Unlock 先完成了原子-1&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g4, g5看到 old&amp;amp;mutexLocked == 0，都增加locked比特，进行cas更新&lt;/li&gt;
&lt;li&gt;g4 g5其中一个更新成功， 获取到锁&lt;/li&gt;
&lt;li&gt;另一个更新失败，再次进入for循环，
&lt;ol&gt;
&lt;li&gt;看到  old&amp;amp;mutexLocked != 0， 增加等待计数，cas更新成功后休眠等待&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;此时g1 进入for循环， 看到已经&lt;strong&gt;有了mutex标记&lt;/strong&gt;，结束Unlock，&lt;strong&gt;不需要再进行唤醒&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二种情况 g1 Unlock 先完成了原子-1，并且cas新增woken标记成功&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;g4其中一个获取锁成功，g5获取锁失败，再次进入for循环，&lt;strong&gt;还未进行第二次cas更新&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;g4 很快完成操作，调用Unlock&lt;/li&gt;
&lt;li&gt;此时，g4看到有&lt;strong&gt;woken标记&lt;/strong&gt;，结束Unlock，不需要再进行唤醒
&lt;ol&gt;
&lt;li&gt;如果g4第二次cas更新成功，去掉woken标记，此时g4还会进行唤醒&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;问题&#34;&gt;问题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为什么要加woken标记&lt;/li&gt;
&lt;li&gt;CAS会不会有ABA问题？&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;问题1为什么要加woken标记&#34;&gt;问题1：为什么要加woken标记&lt;/h4&gt;
&lt;p&gt;防止惊群，只需要唤醒一个&lt;/p&gt;
&lt;p&gt;g1 Lock成功&lt;/p&gt;
&lt;p&gt;g2, g3, g4 请求Lock， 进入睡眠等待&lt;/p&gt;
&lt;p&gt;g1 Unlock，准备semrelease（还未调用）&lt;/p&gt;
&lt;p&gt;此时 新来g5 可以Lock成功&lt;/p&gt;
&lt;p&gt;g5 Unlock，准备semrelease（还未调用）&lt;/p&gt;
&lt;p&gt;然后 g1, g5 同时semrelease，会唤醒多个在睡眠等待的goroutine&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;加了woken标记，就只会唤醒一个&lt;/p&gt;
&lt;p&gt;woken的goroutine和新来的（也可能没有新来的）竞争， 失败则再次睡眠&lt;/p&gt;
&lt;p&gt;被woken的会去掉woken标记进行cas， cas更新成功，都会去掉woken标记，未抢到锁时还会进入睡眠&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;问题2会不会有aba问题&#34;&gt;问题2：会不会有ABA问题&lt;/h4&gt;
&lt;p&gt;不会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlock时， cas什么时候失败？&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Unlock 原子-1后，没有lock标记，新来的g先于Unlock的cas加上了lock，cas失败
&lt;ol&gt;
&lt;li&gt;此时，在进入判断，会认为有lock标记，退出&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;和上面类似，Unlock 原子-1后，没有lock标记，新来的g先于Unlock的cas加上了lock，cas失败。但是在Unlock再次判断之前，新来的g调用了Unlock。
&lt;ol&gt;
&lt;li&gt;此时有两个g在Unlock&lt;/li&gt;
&lt;li&gt;两个都判断没有lock标记，假设还有一个等待者， 这是二者都会尝试加上woken标记，并减去1个等待者&lt;/li&gt;
&lt;li&gt;只有一个添加woken成功，然后唤醒等待者，另一个cas失败，再次判断，由于woken存在或者等待者为0退出&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;cas判断，会不会有N次semacquire，但是semrelease小于N?
&lt;ul&gt;
&lt;li&gt;不会，有多少次 semacquire， 就有多少次 semrelease&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v3give-new-goroutines-some-more-chances&#34;&gt;V3：Give new goroutines some more chances.&lt;/h2&gt;
&lt;p&gt;增加自旋&lt;/p&gt;
&lt;p&gt;有mutexLocked时，进行有限次数自旋，并尝试cas增加mutexWoken标记&lt;/p&gt;
&lt;p&gt;整体逻辑和V2类似&lt;/p&gt;
&lt;h3 id=&#34;lock-2&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
    // Fast path: grab unlocked mutex.
    if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
        return
    }

    awoke := false
    iter := 0
    for {
        old := m.state
        new := old | mutexLocked
        if old&amp;amp;mutexLocked != 0 {
            if runtime_canSpin(iter) {
                if !awoke &amp;amp;&amp;amp; old&amp;amp;mutexWoken == 0 &amp;amp;&amp;amp; old&amp;gt;&amp;gt;mutexWaiterShift != 0 &amp;amp;&amp;amp;
                    atomic.CompareAndSwapInt32(&amp;amp;m.state, old, old|mutexWoken) {
                    awoke = true
                }
                runtime_doSpin()
                iter++
                continue
            }
            new = old + 1&amp;lt;&amp;lt;mutexWaiterShift
        }
        if awoke {
            new &amp;amp;^= mutexWoken
        }
        if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
            if old&amp;amp;mutexLocked == 0 {
                break
            }
            runtime_Semacquire(&amp;amp;m.sema)
            awoke = true
            iter = 0
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;unlock-2&#34;&gt;Unlock&lt;/h3&gt;
&lt;p&gt;逻辑同V2&lt;/p&gt;
&lt;h2 id=&#34;v4solve-the-old-goroutine-starvation-problem&#34;&gt;V4：Solve the old goroutine starvation problem.&lt;/h2&gt;
&lt;p&gt;V3版本新来的goroutine更容易竞争到锁，老的竞争者可能一直得不到锁&lt;/p&gt;
&lt;p&gt;V4版本解决饥饿问题&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;const (
    mutexLocked = 1 &amp;lt;&amp;lt; iota // mutex is locked
    mutexWoken
    mutexStarving // separate out a starvation token from the state field
    mutexWaiterShift = iota
    starvationThresholdNs = 1e6    
)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lock-3&#34;&gt;Lock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Lock() {
	// Fast path: grab unlocked mutex.
	if atomic.CompareAndSwapInt32(&amp;amp;m.state, 0, mutexLocked) {
		return
	}
	// Slow path (outlined so that the fast path can be inlined)
	m.lockSlow()
}

func (m *Mutex) lockSlow() {
	var waitStartTime int64
	starving := false
	awoke := false
	iter := 0
	old := m.state
	for {
		// Don&amp;#39;t spin in starvation mode, ownership is handed off to waiters
		// so we won&amp;#39;t be able to acquire the mutex anyway.
		if old&amp;amp;(mutexLocked|mutexStarving) == mutexLocked &amp;amp;&amp;amp; runtime_canSpin(iter) {
			// Active spinning makes sense.
			// Try to set mutexWoken flag to inform Unlock
			// to not wake other blocked goroutines.
			if !awoke &amp;amp;&amp;amp; old&amp;amp;mutexWoken == 0 &amp;amp;&amp;amp; old&amp;gt;&amp;gt;mutexWaiterShift != 0 &amp;amp;&amp;amp;
				atomic.CompareAndSwapInt32(&amp;amp;m.state, old, old|mutexWoken) {
				awoke = true
			}
			runtime_doSpin()
			iter++
			old = m.state
			continue
		}
		new := old
		// Don&amp;#39;t try to acquire starving mutex, new arriving goroutines must queue.
		if old&amp;amp;mutexStarving == 0 {
			new |= mutexLocked
		}
		if old&amp;amp;(mutexLocked|mutexStarving) != 0 {
			new += 1 &amp;lt;&amp;lt; mutexWaiterShift
		}
		// The current goroutine switches mutex to starvation mode.
		// But if the mutex is currently unlocked, don&amp;#39;t do the switch.
		// Unlock expects that starving mutex has waiters, which will not
		// be true in this case.
		if starving &amp;amp;&amp;amp; old&amp;amp;mutexLocked != 0 {
			new |= mutexStarving
		}
		if awoke {
			// The goroutine has been woken from sleep,
			// so we need to reset the flag in either case.
			if new&amp;amp;mutexWoken == 0 {
				throw(&amp;#34;sync: inconsistent mutex state&amp;#34;)
			}
			new &amp;amp;^= mutexWoken
		}
		if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
			if old&amp;amp;(mutexLocked|mutexStarving) == 0 {
				break // locked the mutex with CAS
			}
			// If we were already waiting before, queue at the front of the queue.
			queueLifo := waitStartTime != 0
			if waitStartTime == 0 {
				waitStartTime = runtime_nanotime()
			}
			runtime_SemacquireMutex(&amp;amp;m.sema, queueLifo, 1)
			starving = starving || runtime_nanotime()-waitStartTime &amp;gt; starvationThresholdNs
			old = m.state
			if old&amp;amp;mutexStarving != 0 {
				// If this goroutine was woken and mutex is in starvation mode,
				// ownership was handed off to us but mutex is in somewhat
				// inconsistent state: mutexLocked is not set and we are still
				// accounted as waiter. Fix that.
				if old&amp;amp;(mutexLocked|mutexWoken) != 0 || old&amp;gt;&amp;gt;mutexWaiterShift == 0 {
					throw(&amp;#34;sync: inconsistent mutex state&amp;#34;)
				}
				delta := int32(mutexLocked - 1&amp;lt;&amp;lt;mutexWaiterShift)
				if !starving || old&amp;gt;&amp;gt;mutexWaiterShift == 1 {
					// Exit starvation mode.
					// Critical to do it here and consider wait time.
					// Starvation mode is so inefficient, that two goroutines
					// can go lock-step infinitely once they switch mutex
					// to starvation mode.
					delta -= mutexStarving
				}
				atomic.AddInt32(&amp;amp;m.state, delta)
				break
			}
			awoke = true
			iter = 0
		} else {
			old = m.state
		}
	}

}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;unlock-3&#34;&gt;Unlock&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (m *Mutex) Unlock() {
	// Fast path: drop lock bit.
	new := atomic.AddInt32(&amp;amp;m.state, -mutexLocked)
	if new != 0 {
		// Outlined slow path to allow inlining the fast path.
		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
		m.unlockSlow(new)
	}
}

func (m *Mutex) unlockSlow(new int32) {
	if new&amp;amp;mutexStarving == 0 {
		old := new
		for {
			// If there are no waiters or a goroutine has already
			// been woken or grabbed the lock, no need to wake anyone.
			// In starvation mode ownership is directly handed off from unlocking
			// goroutine to the next waiter. We are not part of this chain,
			// since we did not observe mutexStarving when we unlocked the mutex above.
			// So get off the way.
			if old&amp;gt;&amp;gt;mutexWaiterShift == 0 || old&amp;amp;(mutexLocked|mutexWoken|mutexStarving) != 0 {
				return
			}
			// Grab the right to wake someone.
			new = (old - 1&amp;lt;&amp;lt;mutexWaiterShift) | mutexWoken
			if atomic.CompareAndSwapInt32(&amp;amp;m.state, old, new) {
				runtime_Semrelease(&amp;amp;m.sema, false, 1)
				return
			}
			old = m.state
		}
	} else {
		// Starving mode: handoff mutex ownership to the next waiter.
		// Note: mutexLocked is not set, the waiter will set it after wakeup.
		// But mutex is still considered locked if mutexStarving is set,
		// so new coming goroutines won&amp;#39;t acquire it.
		runtime_Semrelease(&amp;amp;m.sema, true, 1)
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;V1版本：先进先出，新来的goroutine只能排队&lt;/li&gt;
&lt;li&gt;V2版本：新来的g和被唤醒的g竞争，可能直接拿到锁
&lt;ul&gt;
&lt;li&gt;g被唤醒不是直接拿到锁， 与新来的g竞争&lt;/li&gt;
&lt;li&gt;最多只有一个g被唤醒，被唤醒的g竞争失败，清空woken标记，再次睡眠&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V3版本：增加自旋逻辑，临界区较少时，自旋一会就能拿到锁，无需进入睡眠
&lt;ul&gt;
&lt;li&gt;自旋时会尝试设置woken标记， 通知Unlock无需唤醒其他等待者&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V4版本：解决饥饿，新增mutexStarving标记位，引入普通/饥饿模式
&lt;ul&gt;
&lt;li&gt;普通模式，同上述&lt;/li&gt;
&lt;li&gt;饥饿模式：
&lt;ul&gt;
&lt;li&gt;新来的g，不竞争锁， 在队尾等待&lt;/li&gt;
&lt;li&gt;被唤醒的g，无需再次竞争，直接得到锁&lt;/li&gt;
&lt;li&gt;Unlock，不设置woken标记， 直接唤醒&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;回到最开始的问题&#34;&gt;回到最开始的问题&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;为什么日志时间乱序？ mutex是先进先出的吗&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;线上go版本是1.16，是上面V4版本的实现，以测试代码为例&lt;/p&gt;
&lt;p&gt;在每分钟0s Write时阻塞，此时后续的g，会先进行自旋，然后得不到锁，进入睡眠&lt;/p&gt;
&lt;p&gt;到这里还都是按照先后顺序的，先进入，先打印，不应该出现时间乱序&lt;/p&gt;
&lt;p&gt;第一个g，阻塞3s后，Write，然后Unlock&lt;/p&gt;
&lt;p&gt;如果此时没有新来的g，交给sema第一个等待者，wake后没有竞争直接拿到锁，是不会进入饥饿模式的&lt;/p&gt;
&lt;p&gt;但是如果有新来的g，Unlock时处于普通模式，新来的g与被唤醒的g竞争&lt;/p&gt;
&lt;p&gt;1）如果新来的g（g1）竞争到，被唤醒的g0等待了3s，会cas进入饥饿模式。不过，这个cas不一定成功。g1 Unlock时刚好新来g2，仍然可以得到锁。g0会再次cas更新，直到CAS成功，一定能够加上mutexStarving标记&lt;/p&gt;
&lt;p&gt;如果在CAS中直接获取到锁，会加上starving标记&lt;/p&gt;
&lt;p&gt;如果在CAS中没有获取到锁，也会加上starving标记，并且加入到sema的队头，仍然符合先进先出的顺序&lt;/p&gt;
&lt;p&gt;2）如果被唤醒的g竞争到，不会进入饥饿模式，新来的g加到信号量队尾&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上述分析说明mutex整体上是先进先出的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;普通模式，新来的g与被唤醒的g竞争，新来的g优势大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;即使实际已经饥饿，等待时间过长，但是依次唤醒，依次都得到所有权，并不会转变到饥饿模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;但是一但没有获取到锁，就会加上starving标记，后续进入饥饿模式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;饥饿模式时，新来的g直接加入队尾&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;那么， 上述日志的时间乱序，另有原因&lt;/p&gt;
&lt;p&gt;标准库log方法&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (l *Logger) Output(calldepth int, s string) error {
	now := time.Now() // get this early.
	var file string
	var line int
	l.mu.Lock()
	defer l.mu.Unlock()
	if l.flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		// Release lock while getting caller info - it&amp;#39;s expensive.
		l.mu.Unlock()
		var ok bool
		_, file, line, ok = runtime.Caller(calldepth)
		if !ok {
			file = &amp;#34;???&amp;#34;
			line = 0
		}
		l.mu.Lock()
	}
	l.buf = l.buf[:0]
	l.formatHeader(&amp;amp;l.buf, now, file, line)
	l.buf = append(l.buf, s...)
	if len(s) == 0 || s[len(s)-1] != &amp;#39;\n&amp;#39; {
		l.buf = append(l.buf, &amp;#39;\n&amp;#39;)
	}
	_, err := l.out.Write(l.buf)
	return err
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;解释现象-个人理解&#34;&gt;解释现象 个人理解&lt;/h2&gt;
&lt;p&gt;时间是在Lock之前获取，为了获取代码行信息，会先Unlock，得到代码行数后，再Lock&lt;/p&gt;
&lt;p&gt;日志中时间乱序是因为 Unlock后，runtime.Caller耗时有差异，再次Lock的顺序与获取time.Now顺序不一致&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图片里第一个03s是因为新来的g先获取到了锁&lt;/li&gt;
&lt;li&gt;后续进入饥饿模式，时间是按顺序的&lt;/li&gt;
&lt;li&gt;饥饿模式退出后，有新来的g获取到了锁&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;去掉 Lshortfile|Llongfile， 再次运行，只会有零星的几个时间乱序，符合上述描述，还未实际进入饥饿模式时，新来的g也能够获取到锁&lt;/li&gt;
&lt;li&gt;将标准库内 runtime.Caller 注释掉，Lock， 仍然是Unlock， Lock, Unlock的顺序， 有阻塞3s，日志时间是有序的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;标准库log的优化&#34;&gt;标准库log的优化&lt;/h2&gt;
&lt;p&gt;go1.21 &lt;a href=&#34;https://github.com/golang/go/commit/c3b4c27fd31b51226274a0c038e9c10a65f11657&#34;&gt;代码&lt;/a&gt; 修改了log实现&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Performance:
	name           old time/op  new time/op  delta
	Concurrent-24  19.9µs ± 2%   8.3µs ± 1%  -58.37%  (p=0.000 n=10+10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不再是每个Log对象用一个buf，使用了[]byte池&lt;/p&gt;
&lt;p&gt;mutex只锁Write方法，runtime.Caller提前计算&lt;/p&gt;
&lt;p&gt;用新版本测试，不会再出现时间乱序的问题&lt;/p&gt;
&lt;h2 id=&#34;futex&#34;&gt;futex&lt;/h2&gt;
&lt;p&gt;linux用futex实现pthread_mutex_t&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/futex.2.html&#34;&gt;futex(2) — Linux manual page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;与go的实现有何不同&lt;/p&gt;
&lt;p&gt;后续分析&lt;/p&gt;
&lt;h1 id=&#34;参考文章&#34;&gt;参考文章&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://levelup.gitconnected.com/deep-understanding-of-golang-mutex-9964b02c56e9&#34;&gt;Deep Understanding of Golang Mutex&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Go异步日志实现</title>
      <link>/posts/golang-async-log/</link>
      <pubDate>Sat, 18 May 2024 12:47:51 +0800</pubDate>
      
      <guid>/posts/golang-async-log/</guid>
      <description>一、问题背景 线上某接口偶现pvlost，问题实例物理机所在磁盘都有大量磁盘IO，ioutil持续100%
二、分析 为什么pvlost？
header中透传deadline，判断time.Now() &amp;gt; deadline后，直接丢弃请求 框架在收到请求时立即打印一条请求日志 日志库实现类似go标准库的log，使用sync.Mutex保护buf，打印日志被串行化 磁盘IO打满时，log.Info(&amp;ldquo;msg&amp;rdquo;)在Write时可能会阻塞N秒，后续的请求也被阻塞，打印日志后进入 time.Now() &amp;gt; deadline 的判断时，已经超过了deadline，丢弃请求。对外表现为pvlost Write写文件会阻塞？
文件Write只需要写到内核的cache中，由操作系统负责flush，IO压力大时，cache不足，打印日志会阻塞 线上遇到的case，一般阻塞3秒以下 本次问题是物理机上部署的其他实例大量占用IO 写日志阻塞有以下解决方式
硬件层面 更换SSD：SSD有着更高的读写性能、更高的IOPS 独立部署：不与其他占用IO大的实例混部 服务层面 打印log不立即Write，将日志写到ctx上下文中（例如ctx提供Info、Error日志方法），回包后再Write，此时阻塞不影响请求 异步日志：日志写到进程缓冲区，异步Write 其实，Write行为可以看作是异步的，内核有page cache，并且会定期flush
但是内核cache不足时，也是会阻塞
因此，异步日志本质是在进程内增加cache，不依赖内核的cache
PS：起初并未怀疑是磁盘IO的问题，物理机有多磁盘，监控上需要选中对应磁盘才能看到相应监控
定位问题时最直接的表现是日志中时间乱序，故障时间前后几秒，日志中的时间是乱的，时间变化不合规律
正常的日志是下面这样，时间递增
2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:05.100 msg... 2024-01-01 12:34:05.200 msg... 故障时，时间从5s到6s到7s，又会回到5s
2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:06.100 msg... 2024-01-01 12:34:07.200 msg... 2024-01-01 12:34:05.000 msg... 2024-01-01 12:34:06.100 msg... 2024-01-01 12:34:05.000 msg... 有怀疑是时钟波动的问题，但是时钟波动概率还是很小，多次出现故障，并且时钟短时间频繁波动，排除
三、实现异步日志 几种解决方式中，异步日志可行性最高。
当前日志库实现整体逻辑类似标准库log，额外增加了日志滚动功能，增加Info、Error等日志等级
// 标准库log // A Logger represents an active logging object that generates lines of // output to an io.</description>
      <content>&lt;h1 id=&#34;一问题背景&#34;&gt;一、问题背景&lt;/h1&gt;
&lt;p&gt;线上某接口偶现pvlost，问题实例物理机所在磁盘都有大量&lt;strong&gt;磁盘IO&lt;/strong&gt;，ioutil持续100%&lt;/p&gt;
&lt;h1 id=&#34;二分析&#34;&gt;二、分析&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;为什么pvlost？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;header中透传deadline，判断time.Now() &amp;gt; deadline后，直接丢弃请求&lt;/li&gt;
&lt;li&gt;框架在收到请求时立即打印一条请求日志&lt;/li&gt;
&lt;li&gt;日志库实现类似go标准库的log，使用sync.Mutex保护buf，打印日志被&lt;strong&gt;串行化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;磁盘IO打满时，log.Info(&amp;ldquo;msg&amp;rdquo;)在Write时可能会&lt;strong&gt;阻塞N秒&lt;/strong&gt;，后续的请求也被阻塞，打印日志后进入 time.Now() &amp;gt; deadline 的判断时，已经超过了deadline，丢弃请求。对外表现为pvlost&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write写文件会阻塞？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文件Write只需要写到内核的cache中，由操作系统负责flush，IO压力大时，cache不足，打印日志会阻塞&lt;/li&gt;
&lt;li&gt;线上遇到的case，一般&lt;strong&gt;阻塞3秒以下&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;本次问题是物理机上部署的其他实例大量占用IO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写日志阻塞有以下解决方式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;硬件层面
&lt;ol&gt;
&lt;li&gt;更换SSD：SSD有着更高的读写性能、更高的IOPS&lt;/li&gt;
&lt;li&gt;独立部署：不与其他占用IO大的实例混部&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;服务层面
&lt;ol&gt;
&lt;li&gt;打印log不立即Write，将日志写到ctx上下文中（例如ctx提供Info、Error日志方法），回包后再Write，此时阻塞不影响请求&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;异步日志&lt;/strong&gt;：日志写到进程缓冲区，异步Write&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其实，Write行为可以看作是异步的，内核有page cache，并且会定期flush&lt;/p&gt;
&lt;p&gt;但是内核cache不足时，也是会阻塞&lt;/p&gt;
&lt;p&gt;因此，异步日志本质是在进程内增加cache，不依赖内核的cache&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;PS：起初并未怀疑是磁盘IO的问题，物理机有多磁盘，监控上需要选中对应磁盘才能看到相应监控&lt;/p&gt;
&lt;p&gt;定位问题时最直接的表现是日志中时间乱序，故障时间前后几秒，日志中的时间是乱的，时间变化不合规律&lt;/p&gt;
&lt;p&gt;正常的日志是下面这样，时间递增&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:05.100 msg...
2024-01-01 12:34:05.200 msg...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;故障时，时间从5s到6s到7s，又会回到5s&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:06.100 msg...
2024-01-01 12:34:07.200 msg...
2024-01-01 12:34:05.000 msg...
2024-01-01 12:34:06.100 msg...
2024-01-01 12:34:05.000 msg...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有怀疑是时钟波动的问题，但是时钟波动概率还是很小，多次出现故障，并且时钟短时间频繁波动，排除&lt;/p&gt;
&lt;h1 id=&#34;三实现异步日志&#34;&gt;三、实现异步日志&lt;/h1&gt;
&lt;p&gt;几种解决方式中，异步日志可行性最高。&lt;/p&gt;
&lt;p&gt;当前日志库实现整体逻辑类似标准库log，额外增加了&lt;strong&gt;日志滚动&lt;/strong&gt;功能，增加Info、Error等日志等级&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 标准库log
// A Logger represents an active logging object that generates lines of
// output to an io.Writer. Each logging operation makes a single call to
// the Writer&amp;#39;s Write method. A Logger can be used simultaneously from
// multiple goroutines; it guarantees to serialize access to the Writer.
type Logger struct {
	mu        sync.Mutex // ensures atomic writes; protects the following fields
	prefix    string     // prefix on each line to identify the logger (but see Lmsgprefix)
	flag      int        // properties
	out       io.Writer  // destination for output
	buf       []byte     // for accumulating text to write
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;二者实现上对比&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标准库log&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out为io.Writer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们的日志库&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out为*os.File，使用到Write写日志、Close方法关闭日志。打开新文件，滚动到新文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然只用到Write、Close，那么可以将out定义为io.WriteCloser&lt;/p&gt;
&lt;p&gt;提供fileWrapper方法， 将*os.File转为io.WriteCloser&lt;/p&gt;
&lt;p&gt;未开启异步log时，out = file，开启异步log时，将file包装为异步WriteCloser&lt;/p&gt;
&lt;h3 id=&#34;31-简单实现利用chan-byte&#34;&gt;3.1 简单实现：利用chan []byte&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
type AsyncWriteCloser struct {
	*os.File
	ch    chan []byte
	close chan error
}

func NewAsyncWriteCloser(f *os.File) *AsyncWriteCloser {
	res := &amp;amp;AsyncWriteCloser{
		File:  f,
		ch:    make(chan []byte, 1024),
		close: make(chan error),
	}
	go res.consume()
	return res
}

func (wc *AsyncWriteCloser) consume() {
	for b := range wc.ch {
		wc.File.Write(b)
	}
	wc.close &amp;lt;- wc.File.Close()
}

func (wc *AsyncWriteCloser) Write(b []byte) (int, error) {
	copy := append([]byte(nil), b...)
	wc.ch &amp;lt;- copy
	return len(b), nil
}

func (wc *AsyncWriteCloser) Close(b []byte) error {
	close(wc.ch)
	return &amp;lt;-wc.close
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意：Write参数b要进行&lt;strong&gt;深拷贝&lt;/strong&gt;。函数返回后，调用函数可以修改b的内容&lt;/p&gt;
&lt;p&gt;这个实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write次数不变，Write在chan未满时立即返回，阻塞只会在consume中&lt;/li&gt;
&lt;li&gt;多了一次内存拷贝， Write方法中的拷贝&lt;/li&gt;
&lt;li&gt;[]byte对象多&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-优化实现积攒数据每秒写一次&#34;&gt;3.2 优化实现：积攒数据，每秒写一次&lt;/h3&gt;
&lt;p&gt;先写到cur []byte, 写满cap后再写入chan，减少Write次数，避免产生大量[]byte对象&lt;/p&gt;
&lt;h4 id=&#34;write逻辑&#34;&gt;Write逻辑&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type AsyncWriteCloser2 struct {
	*os.File
	frozen chan []byte

	mu  sync.Mutex
	cur []byte

	closed  bool

	wg sync.WaitGroup
}

func (wc *AsyncWriteCloser2) Write(b []byte) (int, error) {
	wc.mu.Lock()
	defer wc.mu.Unlock()

	if wc.closed {
		return 0, errors.New(&amp;#34;write into closed file&amp;#34;)
	}

	l := len(b)
	if l+len(wc.cur) &amp;lt;= cap(wc.cur) {
		wc.cur = append(wc.cur, b...)
		return l, nil
	}

	if len(wc.cur) &amp;gt; 0 {
		// 阻塞send
		wc.frozen &amp;lt;- wc.cur
		wc.cur = nil // TODO 池化
	}

	wc.cur = append(wc.cur, b...)
	return l, nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外， 当日志量过少时，写满cap需要一定时间，日志更新慢，需要增加&lt;strong&gt;每秒写一次&lt;/strong&gt;的逻辑&lt;/p&gt;
&lt;h4 id=&#34;消费逻辑&#34;&gt;消费逻辑&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (wc *AsyncWriteCloser2) consume() {
	defer wc.wg.Done()

	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()

	var exit bool
	for !exit {
		select {
		case b, ok := &amp;lt;-wc.frozen:
			if !ok { // closed
				exit = true
				break
			}
			wc.File.Write(b)

		case &amp;lt;-ticker.C:
			if len(wc.frozen) &amp;gt; 0 {
				continue
			}

			wc.mu.Lock()
			// 此时 frozen可能满了
			if len(wc.cur) &amp;gt; 0 {
				select {
				case wc.frozen &amp;lt;- wc.cur: // 要写到frozen 顺序消费
					wc.cur = nil
				default:
					// 已经满了 or closed
				}
			}
			wc.mu.Unlock()
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;close等待数据写完&#34;&gt;Close等待数据写完&lt;/h4&gt;
&lt;p&gt;Close时，需要将已有数据写完，因此，在Close方法等待consume完成后调用文件Close&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (wc *AsyncWriteCloser2) waitLocked() {
	// cur还有数据， 写到frozen
	if len(wc.cur) &amp;gt; 0 {
		wc.frozen &amp;lt;- wc.cur
		wc.cur = nil
	}

	close(wc.frozen)
	wc.wg.Wait()
}

func (wc *AsyncWriteCloser2) Close(b []byte) error {
	wc.mu.Lock()
	defer wc.mu.Unlock()

	if wc.closed {
		return errors.New(&amp;#34;closed&amp;#34;)
	}
	wc.closed = true

	wc.waitLocked()

	return wc.File.Close()
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;乍一看没有问题，但是，这样可能会&lt;strong&gt;死锁&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;waitLocked时持有mutex，当前chan已满，此时写入cur阻塞，等待consume消费fronze&lt;/p&gt;
&lt;p&gt;如果consume的select触发ticker分支，ticker会加锁，保护cur，不会消费frozen，也就是waitLocked中无法写入chan&lt;/p&gt;
&lt;p&gt;二者互相等待，死锁&lt;/p&gt;
&lt;p&gt;问题发生在哪？cur写入chan阻塞？写cur移到File.Close前也会死锁&lt;/p&gt;
&lt;p&gt;问题在于sync.Mutex不可重入， Close等待consume退出，consume内会尝试获取Close持有的锁&lt;/p&gt;
&lt;p&gt;解决方式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;consume内使用TryLock，go1.18及以上可使用&lt;/li&gt;
&lt;li&gt;waitLocked写入chan前，先Unlock
&lt;ol&gt;
&lt;li&gt;这个场景可以解决问题，但是不建议&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;consume中select有两个分支，拆成&lt;strong&gt;2个goroutine&lt;/strong&gt;，一个负责消费frozen，一个负责ticker加锁将cur写入frozen，Close还是等待consume退出，consume不加锁，不会有问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;拆成2个goroutine，这种方法通用性很强&lt;/p&gt;
&lt;h3 id=&#34;33-避免重启丢日志&#34;&gt;3.3 避免重启丢日志&lt;/h3&gt;
&lt;p&gt;异步io.WriteCloser只要满足在Close时写完数据即可。&lt;/p&gt;
&lt;p&gt;这样，服务重启时，就不可避免丢数据&lt;/p&gt;
&lt;p&gt;一种选择提供Flush方法，收到信号Flush，但Flush后还会有日志，不能完全避免日志不丢失&lt;/p&gt;
&lt;p&gt;为避免日志丢失，可以提供一个Stop方法（并不只是实现io.WriteCloser了），停止异步写，转变为同步写文件&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;05-21补充&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上面说的关闭异步log的方法，是需要log库与自定义的异步io.WriteCloser相配合，二者有耦合&lt;/p&gt;
&lt;p&gt;思考了下，应该在log内实现Stop方法，加锁，调用异步io.WriteCloser的Flush方法，此时可以将log的out切换为原本的file。 解除了二者的耦合！&lt;/p&gt;
&lt;h3 id=&#34;34-还能做些什么&#34;&gt;3.4 还能做些什么&lt;/h3&gt;
&lt;p&gt;增加统计信息，例如Write最大耗时，chan最大长度等&lt;/p&gt;
&lt;h3 id=&#34;35-过程中的一些问题&#34;&gt;3.5 过程中的一些问题&lt;/h3&gt;
&lt;p&gt;起初尝试用context通知consume退出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;consume的select中可以增加ctx.Done，用来做退出通知
&lt;ul&gt;
&lt;li&gt;select的多个case同时触发时，会随机选择一个&lt;/li&gt;
&lt;li&gt;在退出后还要再消费完chan，代码偏复杂&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;close(chan) 或者 写入nil 标识关闭，这样退出for循环后一定已经消费完chan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;标准库log实现&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go1.18&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
// Output writes the output for a logging event. The string s contains
// the text to print after the prefix specified by the flags of the
// Logger. A newline is appended if the last character of s is not
// already a newline. Calldepth is used to recover the PC and is
// provided for generality, although at the moment on all pre-defined
// paths it will be 2.
func (l *Logger) Output(calldepth int, s string) error {
	now := time.Now() // get this early.
	var file string
	var line int
	l.mu.Lock()
	defer l.mu.Unlock()
	if l.flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		// Release lock while getting caller info - it&amp;#39;s expensive.
		l.mu.Unlock()
		var ok bool
		_, file, line, ok = runtime.Caller(calldepth)
		if !ok {
			file = &amp;#34;???&amp;#34;
			line = 0
		}
		l.mu.Lock()
	}
	l.buf = l.buf[:0]
	l.formatHeader(&amp;amp;l.buf, now, file, line)
	l.buf = append(l.buf, s...)
	if len(s) == 0 || s[len(s)-1] != &amp;#39;\n&amp;#39; {
		l.buf = append(l.buf, &amp;#39;\n&amp;#39;)
	}
	_, err := l.out.Write(l.buf)
	return err
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;mutex临界区较大&lt;/p&gt;
&lt;p&gt;注释写到 Release lock while getting caller info - it&amp;rsquo;s expensive.&lt;/p&gt;
&lt;p&gt;测试了下runtime.Caller的耗时&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func main() {
	for i := 0; i &amp;lt; 10; i++ {
		start := time.Now()
		runtime.Caller(2)
		fmt.Println(&amp;#34;cost&amp;#34;, time.Since(start))
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Caller第一次调用耗时15us，后续在500ns-1us左右。 开销确实大。&lt;/p&gt;
&lt;p&gt;同样功能C++可以用宏定义在编译期得到结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go1.21&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;mutex加锁范围变化，临界区变小&lt;/p&gt;
&lt;p&gt;增加了[]byte池，不再是每个Logger对象使用一个buf []byte&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (l *Logger) output(pc uintptr, calldepth int, appendOutput func([]byte) []byte) error {
	now := time.Now() // get this early.

	// Load prefix and flag once so that their value is consistent within
	// this call regardless of any concurrent changes to their value.
	prefix := l.Prefix()
	flag := l.Flags()

	var file string
	var line int
	if flag&amp;amp;(Lshortfile|Llongfile) != 0 {
		if pc == 0 {
			var ok bool
			_, file, line, ok = runtime.Caller(calldepth)
			if !ok {
				file = &amp;#34;???&amp;#34;
				line = 0
			}
		}
	}

	buf := getBuffer()
	defer putBuffer(buf)
	formatHeader(buf, now, prefix, flag, file, line)
	*buf = appendOutput(*buf)
	if len(*buf) == 0 || (*buf)[len(*buf)-1] != &amp;#39;\n&amp;#39; {
		*buf = append(*buf, &amp;#39;\n&amp;#39;)
	}

	l.outMu.Lock()
	defer l.outMu.Unlock()
	_, err := l.out.Write(*buf)
	return err
}

var bufferPool = sync.Pool{New: func() any { return new([]byte) }}

func getBuffer() *[]byte {
	p := bufferPool.Get().(*[]byte)
	*p = (*p)[:0]
	return p
}
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;四总结&#34;&gt;四、总结&lt;/h1&gt;
&lt;p&gt;使用异步io.WriteCloser，避免打印日志时发生阻塞，避免了接口请求失败&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;遗留问题：sync.Mutex&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;发生问题时日志可以看出，mutex的等待队列不是先进先出，有着一些随机性&lt;/p&gt;
&lt;p&gt;sync.Mutex有饥饿模式 ，饥饿模式下也不是先进先出吗？&lt;/p&gt;
&lt;p&gt;怎样复现日志乱序的现象？&lt;/p&gt;
&lt;p&gt;后续有时间再写篇博客分析&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
