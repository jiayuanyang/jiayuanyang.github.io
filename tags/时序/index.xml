<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>时序 on jiayuan&#39;s Blog</title>
    <link>/tags/%E6%97%B6%E5%BA%8F/</link>
    <description>Recent content in 时序 on jiayuan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 08:00:00 +0800</lastBuildDate><atom:link href="/tags/%E6%97%B6%E5%BA%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在线数据迁移</title>
      <link>/posts/online-data-migrate/</link>
      <pubDate>Tue, 11 Jun 2024 08:00:00 +0800</pubDate>
      
      <guid>/posts/online-data-migrate/</guid>
      <description>在线数据迁移 很多场景都有数据迁移的需求：
单表改分表 增加分表数，分32张表，扩到128张表 原来使用数据库A，后续迁移到数据库B 系统进行重构，新老数据库字段变化 如果允许停机迁移，数据的准确性一致性容易得到保证
但是大部分场景不允许停机迁移，系统中断几分钟，就会带来不少损失。
大部分应用选择不停机迁移，如何保证数据一致性是个问题
双写 打开双写，读老库 迁移存量数据 一致性校验 双写，灰度读新库 双写，全量读新库 下线双写、移除迁移代码 打开双写，目的是新修改数据新老数据库达到一致
迁移存量数据后，新老数据库完全一致
然后维持双写，灰度读新库持续观察，没有问题后读写新库
有问题可以随时切回读写老数据库
实际实施起来，比较复杂
如何双写 先写新库还是先写老库？ update时，如果新数据库没有数据，affect rows不一致，是否需要先从老数据库迁移 老数据库用事务更新多张表，新库是否需要使用事务 双写失败如何处理，接口返回成功还是失败 双insert，失败可能是超时导致，实际已插入，即使超时时增加反查，反查本身仍然可能超时 接口逻辑需多次update，前2个update更新成功，最后一个update新库失败，产生不一致 时序问题 delete与迁移的时序：存量迁移逻辑捞出数据A，准备迁移到新库，此时delete数据A，新老库双delete。数据A执行迁移，新库多数据 异步写新库的时序：如果异步写新库，多个更新的时序会错乱 云控开关时序：期望打开开关的一瞬间，所有实例都感知到变化。但是实际上还会有已经在处理中的请求。因此，处理存量数据时，一般会选择比打开双写早一点的时间，多处理一些数据 怎么保证数据一致 双写无法保证同时成功，有时间差 数据校验 记录update_time，做增量校验 update时，同时更新行的hash值，校验时可以select sum(hash) where xx（适用于结构不变的迁移） 切换期间也可能会引起不一致 一种可行的方式是切换前停止几秒的写请求 存储自身的数据迁移 双写方案在一致性上并不是很完美，可以先看看存储自身是如何处理的，能够提供参考
MySQL 全量同步 可以使用mysqldump导出，导出时记录binlog位置 增量同步 应用binlog 在线结构变更 MySQL早期版本DDL锁表，对应用影响较大，有pt-osc，gh-ost方案可以选择
pt-osc 使用触发器处理新增的修改 原表的insert，在新表replace into 原表的delete，在新表delete ignore 原本的update，在新表replace into 批量导入存量数据 使用insert ignore，避免已经被触发器插入 insert时需要锁，避免delete引起的时序问题 select读到数据A，请求delete A，触发器在新库空操作，然后insert到新库 实际上pt-osc使用insert ignore select xxx from old_table lock in share mode insert select 时持有锁，不会发生这种时序问题 gh-ost 应用binlog处理新增的修改 insert，转为replace into delete，转为delete update，不变 未导入时，update空操作，后续会将新数据导入 导入存量数据，与pt-osc类似 insert ignore select xxx from old_table lock in share mode Redis Cluster Redis对一致性要求并不高，方案可以作为参考</description>
      <content>&lt;h1 id=&#34;在线数据迁移&#34;&gt;在线数据迁移&lt;/h1&gt;
&lt;p&gt;很多场景都有数据迁移的需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单表改分表&lt;/li&gt;
&lt;li&gt;增加分表数，分32张表，扩到128张表&lt;/li&gt;
&lt;li&gt;原来使用数据库A，后续迁移到数据库B&lt;/li&gt;
&lt;li&gt;系统进行重构，新老数据库字段变化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果允许停机迁移，数据的准确性一致性容易得到保证&lt;/p&gt;
&lt;p&gt;但是大部分场景不允许停机迁移，系统中断几分钟，就会带来不少损失。&lt;/p&gt;
&lt;p&gt;大部分应用选择不停机迁移，如何保证数据一致性是个问题&lt;/p&gt;
&lt;h1 id=&#34;双写&#34;&gt;双写&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;打开双写，读老库&lt;/li&gt;
&lt;li&gt;迁移存量数据&lt;/li&gt;
&lt;li&gt;一致性校验&lt;/li&gt;
&lt;li&gt;双写，灰度读新库&lt;/li&gt;
&lt;li&gt;双写，全量读新库&lt;/li&gt;
&lt;li&gt;下线双写、移除迁移代码&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;打开双写，目的是新修改数据新老数据库达到一致&lt;/p&gt;
&lt;p&gt;迁移存量数据后，新老数据库完全一致&lt;/p&gt;
&lt;p&gt;然后维持双写，灰度读新库持续观察，没有问题后读写新库&lt;/p&gt;
&lt;p&gt;有问题可以随时切回读写老数据库&lt;/p&gt;
&lt;p&gt;实际实施起来，比较复杂&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何双写&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;先写新库还是先写老库？&lt;/li&gt;
&lt;li&gt;update时，如果新数据库没有数据，affect rows不一致，是否需要先从老数据库迁移&lt;/li&gt;
&lt;li&gt;老数据库用事务更新多张表，新库是否需要使用事务&lt;/li&gt;
&lt;li&gt;双写失败如何处理，接口返回成功还是失败
&lt;ul&gt;
&lt;li&gt;双insert，失败可能是超时导致，实际已插入，即使超时时增加反查，反查本身仍然可能超时&lt;/li&gt;
&lt;li&gt;接口逻辑需多次update，前2个update更新成功，最后一个update新库失败，产生不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;时序问题&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;delete与迁移的时序：存量迁移逻辑捞出数据A，准备迁移到新库，此时delete数据A，新老库双delete。数据A执行迁移，新库多数据&lt;/li&gt;
&lt;li&gt;异步写新库的时序：如果异步写新库，多个更新的时序会错乱&lt;/li&gt;
&lt;li&gt;云控开关时序：期望打开开关的一瞬间，所有实例都感知到变化。但是实际上还会有已经在处理中的请求。因此，处理存量数据时，一般会选择比打开双写早一点的时间，多处理一些数据&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;怎么保证数据一致&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;双写无法保证同时成功，有时间差&lt;/li&gt;
&lt;li&gt;数据校验
&lt;ul&gt;
&lt;li&gt;记录update_time，做增量校验&lt;/li&gt;
&lt;li&gt;update时，同时更新行的hash值，校验时可以select sum(hash) where xx（适用于结构不变的迁移）&lt;/li&gt;
&lt;li&gt;切换期间也可能会引起不一致
&lt;ul&gt;
&lt;li&gt;一种可行的方式是切换前停止几秒的写请求&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;存储自身的数据迁移&#34;&gt;存储自身的数据迁移&lt;/h1&gt;
&lt;p&gt;双写方案在一致性上并不是很完美，可以先看看存储自身是如何处理的，能够提供参考&lt;/p&gt;
&lt;h2 id=&#34;mysql&#34;&gt;MySQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;全量同步
&lt;ul&gt;
&lt;li&gt;可以使用mysqldump导出，导出时记录binlog位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;增量同步
&lt;ul&gt;
&lt;li&gt;应用binlog&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;在线结构变更&#34;&gt;在线结构变更&lt;/h3&gt;
&lt;p&gt;MySQL早期版本DDL锁表，对应用影响较大，有pt-osc，gh-ost方案可以选择&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pt-osc
&lt;ul&gt;
&lt;li&gt;使用触发器处理新增的修改
&lt;ul&gt;
&lt;li&gt;原表的insert，在新表replace into&lt;/li&gt;
&lt;li&gt;原表的delete，在新表delete ignore&lt;/li&gt;
&lt;li&gt;原本的update，在新表replace into&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;批量导入存量数据
&lt;ul&gt;
&lt;li&gt;使用insert ignore，避免已经被触发器插入&lt;/li&gt;
&lt;li&gt;insert时需要锁，避免delete引起的时序问题
&lt;ul&gt;
&lt;li&gt;select读到数据A，请求delete A，触发器在新库空操作，然后insert到新库&lt;/li&gt;
&lt;li&gt;实际上pt-osc使用insert ignore select  xxx from old_table lock in share mode
&lt;ul&gt;
&lt;li&gt;insert  select 时持有锁，不会发生这种时序问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gh-ost
&lt;ul&gt;
&lt;li&gt;应用binlog处理新增的修改
&lt;ul&gt;
&lt;li&gt;insert，转为replace into&lt;/li&gt;
&lt;li&gt;delete，转为delete&lt;/li&gt;
&lt;li&gt;update，不变
&lt;ul&gt;
&lt;li&gt;未导入时，update空操作，后续会将新数据导入&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;导入存量数据，与pt-osc类似
&lt;ul&gt;
&lt;li&gt;insert ignore select  xxx from old_table lock in share mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;redis-cluster&#34;&gt;Redis Cluster&lt;/h2&gt;
&lt;p&gt;Redis对一致性要求并不高，方案可以作为参考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;全量同步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master导出RDB文件，slave加载&lt;/li&gt;
&lt;li&gt;导出RDB期间的读写记录在缓冲区&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;增量同步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维持长连接，命令传播&lt;/li&gt;
&lt;li&gt;使用的是环形缓冲区，速度不匹配会丢数据导致不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;slot迁移&#34;&gt;slot迁移&lt;/h3&gt;
&lt;p&gt;新老节点配合&lt;/p&gt;
&lt;p&gt;从老节点迁移数据到新节点&lt;/p&gt;
&lt;p&gt;请求key时，如果该key已经从老节点迁移到新节点，老节点会返回ASK命令&lt;/p&gt;
&lt;p&gt;客户端请求新节点&lt;/p&gt;
&lt;p&gt;迁移完成后，再次请求原本在老节点的key，会返回MOVE命令，客户端可以更新路由&lt;/p&gt;
&lt;h1 id=&#34;数据一致性&#34;&gt;数据一致性&lt;/h1&gt;
&lt;p&gt;一般情况下，双写失败的概率很小，做了数据校验后进行切换，数据不一致概率很低，迁移后还可以再次校验&lt;/p&gt;
&lt;p&gt;但双写无法保证新老存储的事务性，总归是有不一致的概率&lt;/p&gt;
&lt;p&gt;下面讨论两种数据一致性高且比较通用的方案&lt;/p&gt;
&lt;h2 id=&#34;方案一全量同步增量迁移&#34;&gt;方案一：全量同步+增量迁移&lt;/h2&gt;
&lt;p&gt;导出快照，并且记录导出快照时位点，同步存量数据&lt;/p&gt;
&lt;p&gt;利用binlog追增量数据，主从差异降为0或者很小时，禁止主库写入，然后进行切换&lt;/p&gt;
&lt;p&gt;停写可以保证数据一致性，由于大部分都有分库分表，停写只会影响一部分用户&lt;/p&gt;
&lt;p&gt;注意在切换后，避免遗留/新增的请求仍然访问老库&lt;/p&gt;
&lt;h2 id=&#34;方案二细粒度迁移写操作与迁移互斥&#34;&gt;方案二：细粒度迁移，写操作与迁移互斥&lt;/h2&gt;
&lt;p&gt;方案一利用binlog有序，导入全量存量数后追增量数据&lt;/p&gt;
&lt;p&gt;还可以分uid迁移，可以理解成对uid的操作加分布式读写锁&lt;/p&gt;
&lt;p&gt;写操作与迁移操作竞争同一把分布式锁&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用户在线的读写请求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读时判断路由，读新库还是老库 (已经切到新库，读了老库不算问题)&lt;/li&gt;
&lt;li&gt;写时加锁，与用户其余写请求、离线迁移互斥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;worker离线的迁移请求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分uid迁移，控制速率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更具体一点的方案参考：&lt;/p&gt;
&lt;p&gt;新建一张迁移状态表，字段 id,uid,status,ctime,mtime,finish等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用户读请求，读迁移状态表，如果finish=1,读新库，否则读老库&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户写请求，先判断是否迁移完成，可以使用缓存等策略加速&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状态为待迁移时，更新状态占用锁，update set status = &amp;lsquo;迁移中&amp;rsquo; where status  = &amp;lsquo;待迁移&amp;rsquo; and uid = xxx，然后迁移数据到新库，迁移失败释放锁，下次迁移时删除新库数据；迁移成功后更新status = &amp;lsquo;迁移成功&amp;rsquo;，更新失败释放锁，返回处理失败&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与分布式锁类似，异常情况下可能会无法释放锁。可以增加一些逻辑，添加heartbeat字段，超时自动剥夺锁，worker定时扫描人工处理等等&lt;/p&gt;
&lt;p&gt;该方案将写与迁移互斥，保证数据一致性&lt;/p&gt;
&lt;p&gt;写持有锁时，其他写无法进行， 迁移无法进行&lt;/p&gt;
&lt;p&gt;迁移持有锁时，写无法进行&lt;/p&gt;
&lt;p&gt;实际操作时，可以进行迁移操作，迁移后更新status，但是并不读写新库&lt;/p&gt;
&lt;p&gt;观察服务与新老存储负载情况，再决定是否可采取该方案&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;当然，还是要看具体场景&lt;/p&gt;
&lt;p&gt;有些场景会有更简单的解决方案&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>缓存使用经验</title>
      <link>/posts/cache/</link>
      <pubDate>Mon, 27 May 2024 22:00:00 +0800</pubDate>
      
      <guid>/posts/cache/</guid>
      <description>一、Redis 1.1 大key、热key问题 大key 压缩：使用JSON序列化时，可以进行压缩，一般压缩率能在50%+ 拆分：假设使用string存浏览记录列表 记录过多时，数据过大，需要拆分 方式1：构造一个总key，history-summary:$uid，总key里保存分key信息 方式2：history:$uid直接存第一页信息，额外存总数量，其余全部页的key。数量少时，访问一次即可 下一页的key为$history_p1:$uid, hisotry_p2:$uid时，如果更新部分key失败，会有不一致问题 分页可以使用随机key，在第一页里标明，给前端page token，下一次可以用token作为key取数据 删除时，需要额外一个get操作， 先get到分页随机key，再删除 分页的如果用hashtag，可以使用redis事务保证同时成功，但是会集中在一个节点 热key 拆分：例如计数，拆成_0, _1，读取时合并 复制：只读的key，可以复制到_0, _1, _2，读取时随机读一个， 更新时一起更新 情况允许时，redis也可以读从库，但一般不会用 本地缓存：热key可以在本地进行缓存 实例较少时，本地热key发现即可 实例较多时，分布式统计 写热点：MQ削峰聚合 缓存穿透/雪崩 缓存空哨兵 随机过期时间 布隆过滤器提前拦截 1.2 缓存/DB数据一致性 常见的缓存更新方式
先更新DB，后更新缓存 更新缓存失败， 取到老数据 如果是更新DB后异步更新缓存，多次缓存写的顺序不确定 同步写，失败了还是老数据，还是要做重试，可能会有多个缓存更新顺序问题 更新缓存不建议放到DB事务中（见到过线上这么处理的） 开启事务，更新DB，更新缓存，更新缓存成功后提交事务，更新缓存失败，回滚事务 首先，更新缓存失败，可能是超时实际缓存已更新， 此时回滚DB造成数据不一致 更新缓存超时失败，再次查缓存查到了最新数据，此时commit，commit也可能失败，数据还是不一致 先更新缓存，后更新DB 排除 更新DB失败，缓存数据是错误的 先更新DB，后删除缓存 删除缓存失败， 会导致取到老数据，可以进行重试删除。或者什么都不做，等待缓存过期 即使删除成功，由于DB主从延迟，还是有可能读从库读到旧数据，写入缓存旧数据 缓存也有主从延迟的问题， 但一般缓存不读从库，不必考虑缓存的主从延迟 可以引入延迟删除，避免主从同步延迟 先删除缓存，后更新DB 删除缓存和更新DB期间，会读到旧数据， 更新旧数据到缓存 和上面类似 先更新缓存，后更新DB逻辑有问题
其余3种方式都大同小异，都没有100%完美
缓存操作不要放到DB事务 为避免主从延迟，可以采用延迟删除 高并发时，删除缓存会导致大量请求到DB，尽量采用更新缓存方式 更新缓存比删除缓存更复杂，有时要再拉取不同数据，组合起来 更新缓存有2个结果：新的数据 或者旧的数据 成功，新缓存 失败/超时，旧缓存或新缓存 (不考虑缓存过期) 而删除缓存的2个结果：旧的数据 或者无缓存， 这个结果更确定一些，重试无副作用， 而更新缓存的重试， 要考虑有没有更新的数据更新 成功，缓存被删除 失败/超时，旧缓存 或者 缓存被删除 更新缓存的时序会导致写入缓存顺序不一致，可以通过订阅binlog，串行更新 很多时候， 不必考虑太细， 我们认为时间差足够用应付上述的极端问题</description>
      <content>&lt;h1 id=&#34;一redis&#34;&gt;一、Redis&lt;/h1&gt;
&lt;h2 id=&#34;11-大key热key问题&#34;&gt;1.1 大key、热key问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;大key
&lt;ul&gt;
&lt;li&gt;压缩：使用JSON序列化时，可以进行压缩，一般压缩率能在50%+&lt;/li&gt;
&lt;li&gt;拆分：假设使用string存浏览记录列表
&lt;ul&gt;
&lt;li&gt;记录过多时，数据过大，需要拆分&lt;/li&gt;
&lt;li&gt;方式1：构造一个总key，history-summary:$uid，总key里保存分key信息&lt;/li&gt;
&lt;li&gt;方式2：history:$uid直接存第一页信息，额外存总数量，其余全部页的key。数量少时，访问一次即可
&lt;ul&gt;
&lt;li&gt;下一页的key为$history_p1:$uid, hisotry_p2:$uid时，如果更新部分key失败，会有不一致问题
&lt;ul&gt;
&lt;li&gt;分页可以使用随机key，在第一页里标明，给前端page token，下一次可以用token作为key取数据&lt;/li&gt;
&lt;li&gt;删除时，需要额外一个get操作， 先get到分页随机key，再删除&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分页的如果用hashtag，可以使用redis事务保证同时成功，但是会集中在一个节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;热key
&lt;ul&gt;
&lt;li&gt;拆分：例如计数，拆成_0, _1，读取时合并&lt;/li&gt;
&lt;li&gt;复制：只读的key，可以复制到_0, _1, _2，读取时随机读一个， 更新时一起更新
&lt;ul&gt;
&lt;li&gt;情况允许时，redis也可以读从库，但一般不会用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;本地缓存：热key可以在本地进行缓存
&lt;ul&gt;
&lt;li&gt;实例较少时，本地热key发现即可&lt;/li&gt;
&lt;li&gt;实例较多时，分布式统计&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;写热点：MQ削峰聚合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缓存穿透/雪崩
&lt;ul&gt;
&lt;li&gt;缓存空哨兵&lt;/li&gt;
&lt;li&gt;随机过期时间&lt;/li&gt;
&lt;li&gt;布隆过滤器提前拦截&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;12-缓存db数据一致性&#34;&gt;1.2 缓存/DB数据一致性&lt;/h2&gt;
&lt;p&gt;常见的缓存更新方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先更新DB，后更新缓存
&lt;ul&gt;
&lt;li&gt;更新缓存失败， 取到老数据
&lt;ul&gt;
&lt;li&gt;如果是更新DB后异步更新缓存，多次缓存写的顺序不确定&lt;/li&gt;
&lt;li&gt;同步写，失败了还是老数据，还是要做重试，可能会有多个缓存更新顺序问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;更新缓存不建议放到DB事务中（见到过线上这么处理的）
&lt;ul&gt;
&lt;li&gt;开启事务，更新DB，更新缓存，更新缓存成功后提交事务，更新缓存失败，回滚事务
&lt;ul&gt;
&lt;li&gt;首先，更新缓存失败，可能是超时实际缓存已更新， 此时回滚DB造成数据不一致&lt;/li&gt;
&lt;li&gt;更新缓存超时失败，再次查缓存查到了最新数据，此时commit，commit也可能失败，数据还是不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;del&gt;先更新缓存，后更新DB&lt;/del&gt; 排除
&lt;ul&gt;
&lt;li&gt;更新DB失败，缓存数据是错误的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先更新DB，后删除缓存
&lt;ul&gt;
&lt;li&gt;删除缓存失败， 会导致取到老数据，可以进行重试删除。或者什么都不做，等待缓存过期
&lt;ul&gt;
&lt;li&gt;即使删除成功，由于&lt;strong&gt;DB主从延迟&lt;/strong&gt;，还是有可能读从库读到旧数据，写入缓存旧数据
&lt;ul&gt;
&lt;li&gt;缓存也有主从延迟的问题， 但一般缓存不读从库，不必考虑缓存的主从延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可以引入延迟删除，避免主从同步延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先删除缓存，后更新DB
&lt;ul&gt;
&lt;li&gt;删除缓存和更新DB期间，会读到旧数据， 更新旧数据到缓存
&lt;ul&gt;
&lt;li&gt;和上面类似&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先更新缓存，后更新DB逻辑有问题&lt;/p&gt;
&lt;p&gt;其余3种方式都大同小异，都没有100%完美&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存操作不要放到DB事务&lt;/li&gt;
&lt;li&gt;为避免主从延迟，可以采用延迟删除&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高并发&lt;/strong&gt;时，删除缓存会导致大量请求到DB，尽量采用&lt;strong&gt;更新缓存&lt;/strong&gt;方式
&lt;ul&gt;
&lt;li&gt;更新缓存比删除缓存更复杂，有时要再拉取不同数据，组合起来&lt;/li&gt;
&lt;li&gt;更新缓存有2个结果：新的数据 或者旧的数据
&lt;ul&gt;
&lt;li&gt;成功，新缓存&lt;/li&gt;
&lt;li&gt;失败/超时，旧缓存或新缓存 (不考虑缓存过期)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;而删除缓存的2个结果：旧的数据 或者无缓存， 这个结果更确定一些，重试无副作用， 而更新缓存的重试， 要考虑有没有&lt;strong&gt;更新的数据更新&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;成功，缓存被删除&lt;/li&gt;
&lt;li&gt;失败/超时，旧缓存 或者 缓存被删除&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;更新缓存的时序会导致写入缓存顺序不一致，可以通过订阅binlog，串行更新&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很多时候， 不必考虑太细， 我们认为&lt;strong&gt;时间差&lt;/strong&gt;足够用应付上述的极端问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先更新DB，后删除缓存
&lt;ul&gt;
&lt;li&gt;有主从延迟问题， 但主从延迟一般很小，再次查询的时间差足够同步&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先删除缓存，后更新DB
&lt;ul&gt;
&lt;li&gt;删除缓存 与 更新DB的时间差内，可能将旧数据写入缓存，认为这个时间差很小，忽略&lt;/li&gt;
&lt;li&gt;再有，即使期间没有读操作，也仍有主从延迟问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;p&gt;数据库与缓存是两个系统，总会有不一致&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;时间差&lt;/strong&gt;与&lt;strong&gt;时序&lt;/strong&gt;会有引发各种问题&lt;/p&gt;
&lt;h3 id=&#34;一种更新的方式尽力避免主从延迟避免不一致&#34;&gt;一种更新的方式，尽力避免主从延迟，避免不一致&lt;/h3&gt;
&lt;p&gt;例如uid的账号基本信息， 空哨兵可以存empty特殊字符，也可以存{uid:-1}， 用-1标记为空&lt;/p&gt;
&lt;p&gt;参考这种方式&lt;/p&gt;
&lt;p&gt;更新DB后， 写入缓存{uid: -2} （-2标识最近有更新，读取时需要再次读从库，或者主库）  过期时间 略大于主从延迟&lt;/p&gt;
&lt;p&gt;应用读缓存，读到-2时，时效要求强的，可以读主库  (读了主库也不更新缓存)， 时效要求低的，读从库&lt;/p&gt;
&lt;p&gt;等到-2过期后，缓存无数据，再次读从库，构造缓存&lt;/p&gt;
&lt;p&gt;并发更新时，会一直向缓存写入-2，等到-2过期，认为最近没有更新，可以读从库构建数据&lt;/p&gt;
&lt;p&gt;这个方式假设主从复制在一个固定时间内完成&lt;/p&gt;
&lt;h3 id=&#34;另外一种更新方式&#34;&gt;另外一种更新方式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;更新DB后，更新缓存&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新DB后，更新缓存期间，缓存可能过期&lt;/p&gt;
&lt;p&gt;此时另外一个读请求可能读DB读到旧数据， 再更新DB请求，更新缓存之后， 写入缓存&lt;/p&gt;
&lt;p&gt;更新DB后更新缓存，用SET写入&lt;/p&gt;
&lt;p&gt;读请求构造缓存时，用SETNX写入&lt;/p&gt;
&lt;p&gt;这样避免，读请求的老数据覆盖掉更新后的新数据&lt;/p&gt;
&lt;h2 id=&#34;13-zset使用&#34;&gt;1.3 zset使用&lt;/h2&gt;
&lt;p&gt;用户点赞列表，zset score存时间戳，member存被点赞对象id&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先读缓存，有数据直接使用&lt;/li&gt;
&lt;li&gt;再读DB，写入zset&lt;/li&gt;
&lt;li&gt;写操作时， 需要先判断zset是否存在， 不存在则不更新zset
&lt;ul&gt;
&lt;li&gt;zset存在，则zadd&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单实现就是上述方案，但是上述有时序问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先读缓存，zset不存在&lt;/li&gt;
&lt;li&gt;此时读DB，读了100条数据，还未写入zset&lt;/li&gt;
&lt;li&gt;新来的写请求，新点赞记录，判断zset不存在，未写入zset
&lt;ul&gt;
&lt;li&gt;如何判断zset存在？exist判断，ttl判断，可能恰好只剩下1s过期，不完美&lt;/li&gt;
&lt;li&gt;可以用expire 延迟过期时间， expire $key 60 更新成功返回1，key不存在返回0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;此时100条数据写入zset， zset丢失了最新一条数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;读zset，不存在时，再读DB之前，应该向zset写入一个假数据， 例如score时间戳无限大/无限小， member=-1&lt;/p&gt;
&lt;p&gt;这样，写请求就可以将最新的数据写入&lt;/p&gt;
&lt;p&gt;读缓存时，要过滤掉member=-1&lt;/p&gt;
&lt;h2 id=&#34;14-并发更新缓存的一些通用解法&#34;&gt;1.4 并发更新缓存的一些通用解法&lt;/h2&gt;
&lt;h3 id=&#34;缓存提供版本号set能力--cas更新&#34;&gt;缓存提供版本号set能力  cas更新&lt;/h3&gt;
&lt;p&gt;SET时传入版本号，版本号大于key的版本号时，才能写入&lt;/p&gt;
&lt;h3 id=&#34;监听binlog更新&#34;&gt;监听binlog更新&lt;/h3&gt;
&lt;p&gt;binlog有序，串行操作&lt;/p&gt;
&lt;p&gt;合并：同一个key， update 1, 2, 3 可以合并后只处理最后一个写&lt;/p&gt;
&lt;h3 id=&#34;redis-的watch--multi&#34;&gt;redis 的watch  multi&lt;/h3&gt;
&lt;p&gt;太麻烦&lt;/p&gt;
&lt;h1 id=&#34;二本地缓存&#34;&gt;二、本地缓存&lt;/h1&gt;
&lt;p&gt;redis的性能很高，单节点能够达到10W QPS&lt;/p&gt;
&lt;p&gt;但是有时候还是有瓶颈，可以再本地再做一层缓存&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定期更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;异步更新&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;哪些key要本地缓存？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;预先知道的，可以云控下发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;未知的，热key发现&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地记录
&lt;ul&gt;
&lt;li&gt;可以用lfu+滑动窗口， 可以参考redis的lfu实现&lt;/li&gt;
&lt;li&gt;采样， 1/10 1/20，每10个/20个key 统计一次  100W qps，降到10w qps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读合并&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;golang的singleflight， 多个key合并为一次访问
&lt;ul&gt;
&lt;li&gt;第一个请求的ctx过短，失败后，被合并的请求也立即失败
&lt;ul&gt;
&lt;li&gt;可以加一些策略进行重试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;singleflight也有一些问题：例如单个连接异常，被合并的请求都会被影响&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
